{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Planet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DDiekmann/Applied-Verification-Lab-Neural-Networks/blob/main/Tutorials/Planet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Planet\n",
        "[Github Repository](https://github.com/progirep/planet)"
      ],
      "metadata": {
        "id": "8Ht-gLaou9-K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZedEeKnXjHc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Clone the repo\n",
        "!git clone https://github.com/progirep/planet.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# we need a script to convert the neural network description in to the \"prototxt\" format (produced by Caffe) into a json file\n",
        "!cd tools\n",
        "!wget https://gist.github.com/progirep/fd7d2dc120862faa984a70f503611013/raw/260e1e76cebd0ea58bf1a03b64c3f1e0002fc677/csv_to_hdf5_supervised_classification.py \n",
        "\n",
        "# we need a second script to generate a database in \"HDF5\" format from comma-separated value files\n",
        "!wget https://raw.githubusercontent.com/vadimkantorov/caffemodel2json/3a8fd443bf1596dad5f517aecdef08a81bf73bfe/caffemodel2json.py"
      ],
      "metadata": {
        "id": "H3Qz6yJ2Ymkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# install packages in order to build PLANET\n",
        "!sudo apt-get install libglpk-dev\n",
        "!sudo apt-get install qt5-qmake\n",
        "!sudo apt-get install valgrind\n",
        "!sudo apt-get install libltdl-dev\n",
        "!sudo apt-get install protobuf-compiler"
      ],
      "metadata": {
        "id": "FiERp9lPZpRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# compile the source code\n",
        "%cd planet/src\n",
        "%ls\n",
        "!qmake Tool.pro\n",
        "!make"
      ],
      "metadata": {
        "id": "azIQnanwahJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# install Caffe and Yices\n",
        "# change root path of #CAFFE and #YICES\n",
        "!sudo apt install caffe-cuda\n",
        "!sudo add-apt-repository ppa:sri-csl/formal-methods -qq\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install yices2"
      ],
      "metadata": {
        "id": "g8PelEMlajl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# envs probably useless\n",
        "%env YICES=/usr\n",
        "%env CAFFE=/usr\n",
        "%cd /content/planet/casestudies/MNIST/"
      ],
      "metadata": {
        "id": "pwJgqPLOdJ59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "export YICES=/usr\n",
        "export CAFFE=/sr\n",
        "set -e\n",
        "caffe train --solver=lenet_solver.prototxt $@ "
      ],
      "metadata": {
        "id": "k15Egul8gDel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Current Problem:\n",
        "https://github.com/BVLC/caffe/issues/2780\n",
        "> Check failed: mdb_status == 0\n",
        "\n"
      ],
      "metadata": {
        "id": "FQPp7O7BnyMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Current Problems\n",
        "* csv_to_hdf5_supervised_classificatoin.py doesn't work, because it's Python2\n",
        "* Error with caffe train - not sure why -https://github.com/BVLC/caffe/issues/2780\n",
        "\n",
        "#TODO's\n",
        "* Try to do a local jupyter notebook instead of colab\n",
        "* Continue the \"Running the Collision Avoidance Benchmarks\" Section\n",
        "\n"
      ],
      "metadata": {
        "id": "eS904UOo-JAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Caffe\n",
        "\n",
        "Following [this tutorial](https://colab.research.google.com/github/Huxwell/caffe-colab/blob/main/caffe_details.ipynb). Caution: this takes 5 minutes."
      ],
      "metadata": {
        "id": "684Qyid6YTHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/BVLC/caffe.git"
      ],
      "metadata": {
        "id": "PzUPIFFZYkiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture \n",
        "!sudo apt-get install libgflags2.2 \n",
        "!sudo apt-get install libgflags-dev\n",
        "!sudo apt-get install libgoogle-glog-dev\n",
        "!sudo apt-get install libhdf5-100\n",
        "!sudo apt-get install libhdf5-serial-dev\n",
        "!sudo apt-get install libhdf5-dev\n",
        "!sudo apt-get install libhdf5-cpp-100\n",
        "!sudo apt-get install libprotobuf-dev protobuf-compiler"
      ],
      "metadata": {
        "id": "J2juacyoYrKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find /usr -iname \"*hdf5.so\"\n",
        "# got: /usr/lib/x86_64-linux-gnu/hdf5/serial\n",
        "!find /usr -iname \"*hdf5_hl.so\""
      ],
      "metadata": {
        "id": "BBaRB7xuZACp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /usr/lib/x86_64-linux-gnu/libhdf5_serial.so /usr/lib/x86_64-linux-gnu/libhdf5.so\n",
        "!ln -s /usr/lib/x86_64-linux-gnu/libhdf5_serial_hl.so /usr/lib/x86_64-linux-gnu/libhdf5_hl.so"
      ],
      "metadata": {
        "id": "MaRpc2_JZEdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!find /usr -iname \"*hdf5.h*\" # got:\n",
        "# /usr/include/hdf5/serial/hdf5.h \n",
        "# /usr/include/opencv2/flann/hdf5.h\n",
        "# Let's try the first one.\n",
        "%env CPATH=\"/usr/include/hdf5/serial/\"\n",
        "#fatal error: hdf5.h: No such file or directory"
      ],
      "metadata": {
        "id": "yocwisfyZLaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo apt-get install libleveldb-dev\n",
        "!sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev\n",
        "!sudo apt-get install libsnappy-dev"
      ],
      "metadata": {
        "id": "oelKebsrZM_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build caffe from source files."
      ],
      "metadata": {
        "id": "3JXTpkNWZV1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $CPATH"
      ],
      "metadata": {
        "id": "oXGdHIdKZYMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd caffe\n",
        "\n",
        "!ls\n",
        "!make clean\n",
        "!cp Makefile.config.example Makefile.config"
      ],
      "metadata": {
        "id": "XUc1y_V3ZaiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/-gencode arch=compute_20/#-gencode arch=compute_20/' Makefile.config #old cuda versions won't compile \n",
        "!sed -i 's/\\/usr\\/local\\/include/\\/usr\\/local\\/include \\/usr\\/include\\/hdf5\\/serial\\//'  Makefile.config #one of the 4 things needed to fix hdf5 issues\n",
        "!sed -i 's/# OPENCV_VERSION := 3/OPENCV_VERSION := 3/' Makefile.config #We actually use opencv 4.1.2, but it's similar enough to opencv 3.\n",
        "!sed -i 's/code=compute_61/code=compute_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75/' Makefile.config #support for new GPUs"
      ],
      "metadata": {
        "id": "jadFlxc6ZkDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!make all -j 4 # -j would use all availiable cores, but RAM related errors occur"
      ],
      "metadata": {
        "id": "zXC_OPCEZmlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -iname \"*caffe*\""
      ],
      "metadata": {
        "id": "mZrkU74_8fgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Caffe model on MNIST"
      ],
      "metadata": {
        "id": "sjhA_H6Qi1Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloads mnist dataset\n",
        "\n",
        "%cd /content/caffe/\n",
        "\n",
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "!cp -rv MNIST/raw/* data/mnist/"
      ],
      "metadata": {
        "id": "Mim2_gdsnB4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates mnist_test_lmdb and mnist_train_lmdb\n",
        "\n",
        "!/content/caffe/examples/mnist/create_mnist.sh"
      ],
      "metadata": {
        "id": "Wmq4yNR1nFcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy lmdbs to planet folder\n",
        "\n",
        "%cp -a /content/caffe/examples/mnist/mnist_test_lmdb /content/planet/casestudies/MNIST/\n",
        "%cp -a /content/caffe/examples/mnist/mnist_train_lmdb /content/planet/casestudies/MNIST/"
      ],
      "metadata": {
        "id": "L4LUHluFAoFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/planet/casestudies/MNIST/\n",
        "\n",
        "# change iterations from 200k to 50k\n",
        "!sed -i 's/200000/50000/g' /content/planet/casestudies/MNIST/lenet_solver.prototxt\n",
        "!sed -i 's/200000/50000/g' /content/planet/casestudies/MNIST/lenet_solver.prototxt\n",
        "\n",
        "# remove snapshots\n",
        "!sed -i 's/snapshot:/#snapshot:/g' /content/planet/casestudies/MNIST/lenet_solver.prototxt\n",
        "!sed -i 's/snapshot_prefix:/#snapshot_prefix:/g' /content/planet/casestudies/MNIST/lenet_solver.prototxt\n",
        "\n",
        "# train the model\n",
        "!/content/caffe/build/tools/caffe train --solver=/content/planet/casestudies/MNIST/lenet_solver.prototxt $@"
      ],
      "metadata": {
        "id": "HQno6KNxnHM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "done: training of the model\n",
        "\n",
        "result: lenet_iter_50000.caffemodel and lenet_iter_50000.solverstate\n",
        "\n",
        "next step: convert caffemodel file to .rlv so PLANET can verify it\n"
      ],
      "metadata": {
        "id": "EiA9fgUwuJMf"
      }
    }
  ]
}