{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Planet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DDiekmann/Applied-Verification-Lab-Neural-Networks/blob/main/Tutorials/Planet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial for Neural Network Verification using Planet\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*As an example we try to verify the adversarial robustness of a classification Network trained on the MNIST dataset. The model is trained using [Caffe](https://caffe.berkeleyvision.org/) and the verification is done with [Planet](https://arxiv.org/abs/1705.01320).*\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LHNCbwmKwJ-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Planet\n",
        "[Github Repository](https://github.com/progirep/planet)"
      ],
      "metadata": {
        "id": "8Ht-gLaou9-K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZedEeKnXjHc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Clone the repo\n",
        "!git clone https://github.com/progirep/planet.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# we need a script to convert the neural network description in to the \"prototxt\" format (produced by Caffe) into a json file\n",
        "!cd tools\n",
        "!wget https://gist.github.com/progirep/fd7d2dc120862faa984a70f503611013/raw/260e1e76cebd0ea58bf1a03b64c3f1e0002fc677/csv_to_hdf5_supervised_classification.py \n",
        "\n",
        "# we need a second script to generate a database in \"HDF5\" format from comma-separated value files\n",
        "!wget https://raw.githubusercontent.com/vadimkantorov/caffemodel2json/3a8fd443bf1596dad5f517aecdef08a81bf73bfe/caffemodel2json.py"
      ],
      "metadata": {
        "id": "H3Qz6yJ2Ymkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# install packages in order to build PLANET\n",
        "!sudo apt-get install libglpk-dev\n",
        "!sudo apt-get install qt5-qmake\n",
        "!sudo apt-get install valgrind\n",
        "!sudo apt-get install libltdl-dev\n",
        "!sudo apt-get install protobuf-compiler"
      ],
      "metadata": {
        "id": "FiERp9lPZpRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# compile the source code\n",
        "%cd planet/src\n",
        "%ls\n",
        "!qmake Tool.pro\n",
        "!make"
      ],
      "metadata": {
        "id": "azIQnanwahJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Caffe - Currently not working in Python\n",
        "\n",
        "Following [this tutorial](https://colab.research.google.com/github/Huxwell/caffe-colab/blob/main/caffe_details.ipynb). Caution: this takes 5 minutes."
      ],
      "metadata": {
        "id": "684Qyid6YTHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caffe currently doesnt work in Python, but you can train your model on cli.\n",
        "\n",
        "%%capture\n",
        "\n",
        "# install Caffe and Yices\n",
        "# change root path of #CAFFE and #YICES\n",
        "!sudo apt install caffe-cuda\n",
        "!sudo add-apt-repository ppa:sri-csl/formal-methods -qq\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install yices2"
      ],
      "metadata": {
        "id": "g8PelEMlajl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/BVLC/caffe.git"
      ],
      "metadata": {
        "id": "PzUPIFFZYkiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture \n",
        "!sudo apt-get install libgflags2.2 \n",
        "!sudo apt-get install libgflags-dev\n",
        "!sudo apt-get install libgoogle-glog-dev\n",
        "!sudo apt-get install libhdf5-100\n",
        "!sudo apt-get install libhdf5-serial-dev\n",
        "!sudo apt-get install libhdf5-dev\n",
        "!sudo apt-get install libhdf5-cpp-100\n",
        "!sudo apt-get install libprotobuf-dev protobuf-compiler"
      ],
      "metadata": {
        "id": "J2juacyoYrKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find /usr -iname \"*hdf5.so\"\n",
        "# got: /usr/lib/x86_64-linux-gnu/hdf5/serial\n",
        "!find /usr -iname \"*hdf5_hl.so\""
      ],
      "metadata": {
        "id": "BBaRB7xuZACp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /usr/lib/x86_64-linux-gnu/libhdf5_serial.so /usr/lib/x86_64-linux-gnu/libhdf5.so\n",
        "!ln -s /usr/lib/x86_64-linux-gnu/libhdf5_serial_hl.so /usr/lib/x86_64-linux-gnu/libhdf5_hl.so"
      ],
      "metadata": {
        "id": "MaRpc2_JZEdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!find /usr -iname \"*hdf5.h*\" # got:\n",
        "# /usr/include/hdf5/serial/hdf5.h \n",
        "# /usr/include/opencv2/flann/hdf5.h\n",
        "# Let's try the first one.\n",
        "%env CPATH=\"/usr/include/hdf5/serial/\"\n",
        "#fatal error: hdf5.h: No such file or directory"
      ],
      "metadata": {
        "id": "yocwisfyZLaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo apt-get install libleveldb-dev\n",
        "!sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev\n",
        "!sudo apt-get install libsnappy-dev"
      ],
      "metadata": {
        "id": "oelKebsrZM_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build caffe from source files."
      ],
      "metadata": {
        "id": "3JXTpkNWZV1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $CPATH"
      ],
      "metadata": {
        "id": "oXGdHIdKZYMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd caffe\n",
        "\n",
        "!ls\n",
        "!make clean\n",
        "!cp Makefile.config.example Makefile.config"
      ],
      "metadata": {
        "id": "XUc1y_V3ZaiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/-gencode arch=compute_20/#-gencode arch=compute_20/' Makefile.config #old cuda versions won't compile \n",
        "!sed -i 's/\\/usr\\/local\\/include/\\/usr\\/local\\/include \\/usr\\/include\\/hdf5\\/serial\\//'  Makefile.config #one of the 4 things needed to fix hdf5 issues\n",
        "!sed -i 's/# OPENCV_VERSION := 3/OPENCV_VERSION := 3/' Makefile.config #We actually use opencv 4.1.2, but it's similar enough to opencv 3.\n",
        "!sed -i 's/code=compute_61/code=compute_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75/' Makefile.config #support for new GPUs"
      ],
      "metadata": {
        "id": "jadFlxc6ZkDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!make all -j 4 # -j would use all availiable cores, but RAM related errors occur"
      ],
      "metadata": {
        "id": "zXC_OPCEZmlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -iname \"*caffe*\""
      ],
      "metadata": {
        "id": "mZrkU74_8fgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Caffe model on MNIST"
      ],
      "metadata": {
        "id": "sjhA_H6Qi1Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloads mnist dataset\n",
        "\n",
        "%cd /content/caffe/\n",
        "\n",
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "!cp -rv MNIST/raw/* data/mnist/"
      ],
      "metadata": {
        "id": "Mim2_gdsnB4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates mnist_test_lmdb and mnist_train_lmdb\n",
        "\n",
        "!/content/caffe/examples/mnist/create_mnist.sh"
      ],
      "metadata": {
        "id": "Wmq4yNR1nFcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy lmdbs to planet folder\n",
        "\n",
        "%cp -a /content/caffe/examples/mnist/mnist_test_lmdb /content/planet/casestudies/MNIST/\n",
        "%cp -a /content/caffe/examples/mnist/mnist_train_lmdb /content/planet/casestudies/MNIST/"
      ],
      "metadata": {
        "id": "L4LUHluFAoFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define our neural network"
      ],
      "metadata": {
        "id": "QRMEodfNuJ2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/lenet_train_test.prototext\n",
        "name: \"LeNet\"\n",
        "layer {\n",
        "  name: \"mnist\"\n",
        "  type: \"Data\"\n",
        "  top: \"data\"\n",
        "  top: \"label\"\n",
        "  include {\n",
        "    phase: TRAIN\n",
        "  }\n",
        "  transform_param {\n",
        "    scale: 0.00390625\n",
        "  }\n",
        "  data_param {\n",
        "    source: \"mnist_train_lmdb\"\n",
        "    batch_size: 64\n",
        "    backend: LMDB\n",
        "  }\n",
        "}\n",
        "layer {\n",
        "  name: \"mnist\"\n",
        "  type: \"Data\"\n",
        "  top: \"data\"\n",
        "  top: \"label\"\n",
        "  include {\n",
        "    phase: TEST\n",
        "  }\n",
        "  transform_param {\n",
        "    scale: 0.00390625\n",
        "  }\n",
        "  data_param {\n",
        "    source: \"mnist_test_lmdb\"\n",
        "    batch_size: 100\n",
        "    backend: LMDB\n",
        "  }\n",
        "}\n",
        "\n",
        "layer {\n",
        "    name: \"reshapeA\"\n",
        "    type: \"Reshape\"\n",
        "    bottom: \"data\"\n",
        "    top: \"reshapeA\"\n",
        "    reshape_param {\n",
        "      shape {\n",
        "        dim: -1  # copy the dimension from below\n",
        "        dim: 1  # copy the dimension from below\n",
        "        dim: 28  # copy the dimension from below\n",
        "        dim: 28 # infer it from the other dimensions\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\n",
        "layer {\n",
        "  name: \"conv1\"\n",
        "  type: \"Convolution\"\n",
        "  bottom: \"reshapeA\"\n",
        "  top: \"conv1\"\n",
        "  param {\n",
        "    lr_mult: 1\n",
        "  }\n",
        "  param {\n",
        "    lr_mult: 2\n",
        "  }\n",
        "  convolution_param {\n",
        "    num_output: 3\n",
        "    kernel_size: 4\n",
        "    stride: 2\n",
        "    weight_filler {\n",
        "      type: \"xavier\"\n",
        "    }\n",
        "    bias_filler {\n",
        "      type: \"constant\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "layer {\n",
        "  name: \"pool1\"\n",
        "  type: \"Pooling\"\n",
        "  bottom: \"conv1\"\n",
        "  top: \"pool1\"\n",
        "  pooling_param {\n",
        "    pool: MAX\n",
        "    kernel_size: 4\n",
        "    stride: 3\n",
        "  }\n",
        "}\n",
        "\n",
        "layer {\n",
        "    name: \"reshapeB\"\n",
        "    type: \"Reshape\"\n",
        "    bottom: \"pool1\"\n",
        "    top: \"reshapeB\"\n",
        "    reshape_param {\n",
        "      shape {\n",
        "        dim: -1  # copy the dimension from below\n",
        "        dim: 48 # infer it from the other dimensions\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\n",
        "layer {\n",
        "  name: \"ip1\"\n",
        "  type: \"InnerProduct\"\n",
        "  bottom: \"reshapeB\"\n",
        "  top: \"ip1\"\n",
        "  param {\n",
        "    lr_mult: 1\n",
        "  }\n",
        "  param {\n",
        "    lr_mult: 2\n",
        "  }\n",
        "  inner_product_param {\n",
        "    num_output: 8\n",
        "    weight_filler {\n",
        "      type: \"xavier\"\n",
        "    }\n",
        "    bias_filler {\n",
        "      type: \"constant\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "layer {\n",
        "  name: \"relu1\"\n",
        "  type: \"ReLU\"\n",
        "  bottom: \"ip1\"\n",
        "  top: \"relu1\"\n",
        "}\n",
        "layer {\n",
        "  name: \"ip2\"\n",
        "  type: \"InnerProduct\"\n",
        "  bottom: \"relu1\"\n",
        "  top: \"ip2\"\n",
        "  param {\n",
        "    lr_mult: 1\n",
        "  }\n",
        "  param {\n",
        "    lr_mult: 2\n",
        "  }\n",
        "  inner_product_param {\n",
        "    num_output: 10\n",
        "    weight_filler {\n",
        "      type: \"xavier\"\n",
        "    }\n",
        "    bias_filler {\n",
        "      type: \"constant\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "layer {\n",
        "  name: \"accuracy\"\n",
        "  type: \"Accuracy\"\n",
        "  bottom: \"ip2\"\n",
        "  bottom: \"label\"\n",
        "  top: \"accuracy\"\n",
        "}\n",
        "layer {\n",
        "  name: \"loss\"\n",
        "  type: \"SoftmaxWithLoss\"\n",
        "  bottom: \"ip2\"\n",
        "  bottom: \"label\"\n",
        "  top: \"loss\"\n",
        "}"
      ],
      "metadata": {
        "id": "P_6ze7ZxuNcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define our training "
      ],
      "metadata": {
        "id": "cFfk8Cr9uf4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/lenet_solver.prototxt\n",
        "# The train/test net protocol buffer definition\n",
        "net: \"lenet_train_test.prototxt\"\n",
        "# test_iter specifies how many forward passes the test should carry out.\n",
        "# In the case of MNIST, we have test batch size 100 and 100 test iterations,\n",
        "# covering the full 10,000 testing images.\n",
        "test_iter: 100\n",
        "# Carry out testing every 500 training iterations.\n",
        "test_interval: 1000\n",
        "# The base learning rate, momentum and the weight decay of the network.\n",
        "base_lr: 0.01\n",
        "momentum: 0.9\n",
        "weight_decay: 0.0005\n",
        "# The learning rate policy\n",
        "lr_policy: \"inv\"\n",
        "gamma: 0.0001\n",
        "power: 0.75\n",
        "# Display every 100 iterations\n",
        "display: 1000\n",
        "# The maximum number of iterations\n",
        "max_iter: 20000\n",
        "# solver mode: CPU or GPU\n",
        "solver_mode: CPU"
      ],
      "metadata": {
        "id": "xvK34bjquj0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train our model using Caffe"
      ],
      "metadata": {
        "id": "-qSefwU1vFba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "!/content/caffe/build/tools/caffe train --solver=/content/lenet_solver.prototxt $@"
      ],
      "metadata": {
        "id": "HQno6KNxnHM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training results in a .caffemodel file, which now describes our trained model. To verify it with PLANET, we have to convert it to the right input format:\n",
        "\n",
        "*.caffemodel -> .json -> .rlv*"
      ],
      "metadata": {
        "id": "EiA9fgUwuJMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Caffe model to Planet input file"
      ],
      "metadata": {
        "id": "H46WPknAXiRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/caffe/\n",
        "!make pycaffe\n",
        "!make distribute"
      ],
      "metadata": {
        "id": "Xc0-CX_tcxXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env CAFFE_ROOT=/content/caffe\n",
        "%env PYTHONPATH=/content/caffe/python:$PYTHONPATH"
      ],
      "metadata": {
        "id": "iAu_kL44eDcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import requests\n",
        "import argparse\n",
        "import tempfile\n",
        "import subprocess\n",
        "from google.protobuf.descriptor import FieldDescriptor as FD\n",
        "\n",
        "# inspired by https://github.com/dpp-name/protobuf-json/blob/master/protobuf_json.py\n",
        "def pb2json(pb, print_arrays):\n",
        "\t_ftype2js = {\n",
        "\t\tFD.TYPE_DOUBLE: float,\n",
        "\t\tFD.TYPE_FLOAT: float,\n",
        "\t\tFD.TYPE_INT64: long,\n",
        "\t\tFD.TYPE_UINT64: long,\n",
        "\t\tFD.TYPE_INT32: int,\n",
        "\t\tFD.TYPE_FIXED64: float,\n",
        "\t\tFD.TYPE_FIXED32: float,\n",
        "\t\tFD.TYPE_BOOL: bool,\n",
        "\t\tFD.TYPE_STRING: unicode,\n",
        "\t\tFD.TYPE_BYTES: lambda x: x.encode('string_escape'),\n",
        "\t\tFD.TYPE_UINT32: int,\n",
        "\t\tFD.TYPE_ENUM: int,\n",
        "\t\tFD.TYPE_SFIXED32: float,\n",
        "\t\tFD.TYPE_SFIXED64: float,\n",
        "\t\tFD.TYPE_SINT32: int,\n",
        "\t\tFD.TYPE_SINT64: long,\n",
        "\t\tFD.TYPE_MESSAGE: lambda x: pb2json(x, print_arrays = print_arrays),\n",
        "\t\t'unknown' : lambda x: 'Unknown field type: %s' % x\n",
        "\t}\n",
        "\tjs = {}\n",
        "\tfor field, value in pb.ListFields():\n",
        "\t\tftype = _ftype2js[field.type] if field.type in _ftype2js else _ftype2js['unknown']\n",
        "\t\tif field.label == FD.LABEL_REPEATED:\n",
        "\t\t\tjs_value = map(ftype, value)\n",
        "\t\t\tif not print_arrays and (field.name == 'data' and len(js_value) > 8):\n",
        "\t\t\t\thead_n = 5\n",
        "\t\t\t\tjs_value = js_value[:head_n] + ['(%d elements more)' % (len(js_value) - head_n)]\n",
        "\t\telse:\n",
        "\t\t\tjs_value = ftype(value)\n",
        "\t\tjs[field.name] = js_value\n",
        "\treturn js\n",
        "\n",
        "from caffe import *\n",
        "\n",
        "caffemodel_file = '/content/planet/casestudies/MNIST/lenet_solver_iter_50000.caffemodel'\n",
        "\n",
        "deserialized = caffe_pb2.NetParameter()\n",
        "deserialized.ParseFromString(open(caffemodel_file, 'rb').read())\n",
        "\n",
        "json.dump(pb2json(deserialized, args.data), sys.stdout, indent = 2)\n"
      ],
      "metadata": {
        "id": "ZwgmxWD4Xk-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caffe to JSON converter in Python3\n",
        "### TODO caffe doesnt work in python currently\n",
        "\n"
      ],
      "metadata": {
        "id": "QW1BFNkwCuA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import argparse\n",
        "import tempfile\n",
        "import subprocess\n",
        "from google.protobuf.descriptor import FieldDescriptor as FD\n",
        "\n",
        "# inspired by https://github.com/dpp-name/protobuf-json/blob/master/protobuf_json.py\n",
        "def pb2json(pb, print_arrays):\n",
        "\t_ftype2js = {\n",
        "        FD.TYPE_DOUBLE: float,\n",
        "\t\tFD.TYPE_FLOAT: float,\n",
        "\t\tFD.TYPE_INT64: int,\n",
        "\t\tFD.TYPE_UINT64: int,\n",
        "\t\tFD.TYPE_INT32: int,\n",
        "\t\tFD.TYPE_FIXED64: float,\n",
        "\t\tFD.TYPE_FIXED32: float,\n",
        "\t\tFD.TYPE_BOOL: bool,\n",
        "\t\tFD.TYPE_STRING: str,\n",
        "\t\tFD.TYPE_BYTES: lambda x: x.encode('string_escape'),\n",
        "\t\tFD.TYPE_UINT32: int,\n",
        "\t\tFD.TYPE_ENUM: int,\n",
        "\t\tFD.TYPE_SFIXED32: float,\n",
        "\t\tFD.TYPE_SFIXED64: float,\n",
        "\t\tFD.TYPE_SINT32: int,\n",
        "\t\tFD.TYPE_SINT64: int,\n",
        "\t\tFD.TYPE_MESSAGE: lambda x: pb2json(x, print_arrays = print_arrays),\n",
        "\t\t'unknown' : lambda x: 'Unknown field type: %s' % x\n",
        "\t}\n",
        "\tjs = {}\n",
        "\tfor field, value in pb.ListFields():\n",
        "\t\tftype = _ftype2js[field.type] if field.type in _ftype2js else _ftype2js['unknown']\n",
        "\t\tif field.label == FD.LABEL_REPEATED:\n",
        "\t\t\tjs_value = list(map(ftype, value))\n",
        "\t\t\tif not print_arrays and (field.name == 'data' and len(js_value) > 8):\n",
        "\t\t\t\thead_n = 5\n",
        "\t\t\t\tjs_value = js_value[:head_n] + ['(%d elements more)' % (len(js_value) - head_n)]\n",
        "\t\telse:\n",
        "\t\t\tjs_value = ftype(value)\n",
        "\t\tjs[field.name] = js_value\n",
        "\treturn js\n",
        "\n",
        "from caffe.proto import caffe_pb2\n",
        "\n",
        "caffe_file = \"lenet_solver_iter_50000.caffemodel\"\n",
        "\n",
        "deserialized = caffe_pb2.NetParameter()\n",
        "deserialized.ParseFromString(open(caffe_file, 'rb').read())\n",
        "\n",
        "# print(deserialized)\n",
        "# json dump to console\n",
        "json.dump(pb2json(deserialized, \"store_true\"), sys.stdout, indent = 2)\n",
        "\n",
        "# json dump to file\n",
        "with open(\"caffemodel_mnist.json\", \"w\") as f:\n",
        "    json.dump(pb2json(deserialized, \"store_true\"), f, indent = 2)"
      ],
      "metadata": {
        "id": "FMEP4fLMCtlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JSON to RLV converter in Python3"
      ],
      "metadata": {
        "id": "aZ1j-mNVCU2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture cap --no-stderr\n",
        "with open('output.rlv', 'w') as f:\n",
        "    f.write(cap.stdout)\n",
        "\n",
        "import os, sys\n",
        "import json\n",
        "# import operator\n",
        "from functools import reduce\n",
        "from pprint import pprint\n",
        "\n",
        "# Read input file\n",
        "if len(sys.argv) < 2:\n",
        "    print(sys.stderr, \"Error: Expected JSON input file name!\")\n",
        "\n",
        "\"\"\"\n",
        "with open(sys.argv[1]) as data_file:    \n",
        "    data = json.load(data_file)\n",
        "\"\"\"\n",
        "\n",
        "with open(\"caffemodel_mnist.json\") as data_file:    \n",
        "    data = json.load(data_file)\n",
        "\n",
        "data = data[\"layer\"]\n",
        "\n",
        "# Neuron lookup table: layer--->Neuron names\n",
        "neurons = {}\n",
        "\n",
        "\n",
        "# Function for recursively processing layers\n",
        "def recurseProcessLayer(dataLineName,dataLineWidth):\n",
        "    '''Returns the neuron names. DataLineWidth may be unknown.'''\n",
        "\n",
        "    # DAG Caching\n",
        "    if dataLineName in neurons:\n",
        "        return neurons[dataLineName]\n",
        "        \n",
        "    # Search for the producer of this layer\n",
        "    for layer in data:\n",
        "        if dataLineName in layer[\"top\"]:\n",
        "\n",
        "            # This is the layer to be processed\n",
        "            # ---> Proceed according to type.\n",
        "            if layer[\"type\"]==\"Split\":\n",
        "                assert len(layer[\"bottom\"])==1\n",
        "                return recurseProcessLayer(layer[\"bottom\"][0],dataLineWidth)\n",
        "            elif layer[\"type\"]==\"InnerProduct\":\n",
        "                \n",
        "                # Get output dimension\n",
        "                outputLineWidth = layer[\"inner_product_param\"][\"num_output\"]\n",
        "                if dataLineWidth!=None:\n",
        "                    assert outputLineWidth==dataLineWidth\n",
        "                \n",
        "                # Now get input dimension and blob data\n",
        "                inputLineWidth = None\n",
        "                biasWeights = None\n",
        "                inputWeights = None\n",
        "                for blob in layer[\"blobs\"]:\n",
        "                    if len(blob[\"shape\"][\"dim\"])==1:\n",
        "                        biasWeights = blob[\"data\"]\n",
        "                    elif len(blob[\"shape\"][\"dim\"])==2:\n",
        "                        inputLineWidth = blob[\"shape\"][\"dim\"][1]\n",
        "                        inputWeights = blob[\"data\"]\n",
        "                        assert blob[\"shape\"][\"dim\"][0]==outputLineWidth\n",
        "                    else:\n",
        "                        assert False\n",
        "                assert inputLineWidth != None\n",
        "                \n",
        "                # Now get input\n",
        "                assert len(layer[\"bottom\"])==1\n",
        "                inputNeurons = recurseProcessLayer(layer[\"bottom\"][0],inputLineWidth)\n",
        "                \n",
        "                # Produce outputs\n",
        "                outputNeurons = []\n",
        "                for i in range(0,outputLineWidth):\n",
        "                    outputNeurons.append(dataLineName+\"X\"+str(i))\n",
        "                    sys.stdout.write(\"Linear \"+dataLineName+\"X\"+str(i)+\" \"+str(biasWeights[i]))\n",
        "                    for j in range(0,len(inputNeurons)):\n",
        "                        sys.stdout.write(\" \"+str(inputWeights[i*inputLineWidth+j])+\" \"+str(inputNeurons[j]))\n",
        "                    sys.stdout.write(\"\\n\")\n",
        "                \n",
        "                neurons[dataLineName] = outputNeurons\n",
        "                return outputNeurons\n",
        "                \n",
        "                \n",
        "            elif layer[\"type\"]==\"ReLU\":\n",
        "            \n",
        "                # RELU: Get Input\n",
        "                assert len(layer[\"bottom\"])==1\n",
        "                inputNeurons = recurseProcessLayer(layer[\"bottom\"][0],dataLineWidth)\n",
        "                \n",
        "                # Produce outputs\n",
        "                outputNeurons = []\n",
        "                for i in range(0,len(inputNeurons)):\n",
        "                    outputNeurons.append(dataLineName+\"X\"+str(i))\n",
        "                    sys.stdout.write(\"ReLU \"+dataLineName+\"X\"+str(i)+\" 0.0 1.0 \"+str(inputNeurons[i])+\"\\n\")\n",
        "                \n",
        "                neurons[dataLineName] = outputNeurons\n",
        "                return outputNeurons\n",
        "                \n",
        "                \n",
        "                \n",
        "            elif layer[\"type\"]==\"Convolution\":\n",
        "                \n",
        "                # Convolution\n",
        "                \n",
        "                # ---> Load weights and biasses\n",
        "                dimWeights = None\n",
        "                weights = None\n",
        "                biasses = None\n",
        "                for blob in layer[\"blobs\"]:\n",
        "                    size = blob[\"shape\"][\"dim\"]\n",
        "                    params = blob[\"data\"]\n",
        "                    if len(size)==1:\n",
        "                        biasses = params\n",
        "                    else:\n",
        "                        dimWeights = size\n",
        "                        weights = params\n",
        "                        \n",
        "                # ---> Other parameters\n",
        "                #--------> Read Stride\n",
        "                if \"stride\" in layer[\"convolution_param\"]:\n",
        "                    stride = layer[\"convolution_param\"][\"stride\"]\n",
        "                    assert len(stride)==1\n",
        "                    stride = stride + stride\n",
        "                else:\n",
        "                    if \"stride_w\" in layer[\"convolution_param\"]:\n",
        "                        if \"stride_h\" in layer[\"convolution_param\"]:\n",
        "                            stride = [layer[\"convolution_param\"][\"stride_w\"],layer[\"convolution_param\"][\"stride_h\"]]\n",
        "                        else:\n",
        "                            stride = [layer[\"convolution_param\"][\"stride_w\"],1]\n",
        "                    else:\n",
        "                        if \"stride_h\" in layer[\"convolution_param\"]:\n",
        "                            stride = [1,layer[\"convolution_param\"][\"stride_h\"]]\n",
        "                        else:\n",
        "                            stride = [1,1]\n",
        "                \n",
        "                #--------> Read Kernel Size\n",
        "                if \"kernel_size\" in layer[\"convolution_param\"]:\n",
        "                    kernel_size = layer[\"convolution_param\"][\"kernel_size\"]\n",
        "                    assert len(kernel_size)==1\n",
        "                    kernel_size = kernel_size + kernel_size\n",
        "                else:\n",
        "                    kernel_size = [layer[\"convolution_param\"][\"kernel_w\"],layer[\"convolution_param\"][\"kernel_h\"]]\n",
        "\n",
        "                #--------> Read PAD\n",
        "                if \"pad\" in layer[\"convolution_param\"]:\n",
        "                    padding = layer[\"convolution_param\"][\"pad\"]\n",
        "                    assert len(padding)==1\n",
        "                    padding = padding + padding\n",
        "                else:\n",
        "                    if \"pad_w\" in layer[\"convolution_param\"]:\n",
        "                        if \"pad_h\" in layer[\"convolution_param\"]:\n",
        "                            padding = [layer[\"convolution_param\"][\"pad_w\"],layer[\"convolution_param\"][\"pad_h\"]]\n",
        "                        else:\n",
        "                            padding = [layer[\"convolution_param\"][\"pad_w\"],0]\n",
        "                    else:\n",
        "                        if \"pad_h\" in layer[\"convolution_param\"]:\n",
        "                            padding = [0,layer[\"convolution_param\"][\"pad_h\"]]\n",
        "                        else:\n",
        "                            padding = [0,0]\n",
        "\n",
        "                num_output = layer[\"convolution_param\"][\"num_output\"]\n",
        "                num_input_channels = dimWeights[1]\n",
        "                \n",
        "                # Rest is unimplemented for the time being.\n",
        "                # assert dimWeights[0]==1 #---> This is for the *outgoing* num_outputs\n",
        "                # assert dimWeights[1]==1 #----> This is for the incoming colors or features\n",
        "                \n",
        "                # Check for some unsupported features\n",
        "                if \"bias_term\" in layer[\"convolution_param\"]:\n",
        "                    print(sys.stderr, \"Error: Only the default 'bias_term' value is supported for convolution layers\")\n",
        "                    sys.exit(1)\n",
        "\n",
        "                if \"group\" in layer[\"convolution_param\"]:\n",
        "                    print(sys.stderr, \"Error: Only the default 'group' value is supported for convolution layers\")\n",
        "                    sys.exit(1)\n",
        "                \n",
        "                # ---> Read input\n",
        "                inputNeurons = recurseProcessLayer(layer[\"bottom\"][0],None)\n",
        "                \n",
        "                # ---> Unflatten weights\n",
        "                def unflatten(neurons,remainingDimensions):\n",
        "                    if len(remainingDimensions)==1:\n",
        "                        return (neurons[0:remainingDimensions[0]],neurons[remainingDimensions[0]:])\n",
        "                    else:\n",
        "                        res = []\n",
        "                        for a in range(0,remainingDimensions[0]):\n",
        "                            (d,neurons) = unflatten(neurons,remainingDimensions[1:])\n",
        "                            res.append(d)\n",
        "                        return (res,neurons)\n",
        "                \n",
        "                (unflattenedWeights,rest) = unflatten(weights,dimWeights)\n",
        "                assert rest==[]\n",
        "                \n",
        "                # Compute convolution\n",
        "                resultingNeurons = []\n",
        "                for i in range(0,num_output):\n",
        "                    ysize = len(inputNeurons[0])\n",
        "                    xsize = len(inputNeurons[0][0])\n",
        "                    thisBlock = []\n",
        "                    for y in range(-1*padding[1],ysize-kernel_size[1]+1+padding[1],stride[1]):\n",
        "                        thisLine = []                    \n",
        "                        for x in range(-1*padding[0],xsize-kernel_size[0]+1+padding[0],stride[0]):\n",
        "                            thisLine.append(dataLineName+\"X\"+str(i)+\"X\"+str(x)+\"X\"+str(y))\n",
        "                            localInputs = []\n",
        "                            for c in range(0,num_input_channels):\n",
        "                                for b in range(0,kernel_size[1]):\n",
        "                                    for a in range(0,kernel_size[0]):\n",
        "                                        if y+b>=0 and y+b<len(inputNeurons[c]):\n",
        "                                            if x+a>=0 and x+a<len(inputNeurons[c][y+b]):\n",
        "                                                localInputs.append(str(unflattenedWeights[i][c][b][a])+\" \"+inputNeurons[c][y+b][x+a]) \n",
        "                            sys.stdout.write(\"Linear \"+thisLine[-1]+\" \"+str(biasses[i])+\" \"+\" \".join(localInputs)+\"\\n\")\n",
        "                        thisBlock.append(thisLine)\n",
        "                    resultingNeurons.append(thisBlock)\n",
        "                    \n",
        "                return resultingNeurons\n",
        "\n",
        "            elif layer[\"type\"]==\"HDF5Data\":\n",
        "            \n",
        "                # Input layer\n",
        "                assert dataLineWidth!=None\n",
        "                \n",
        "                outputNeurons = []\n",
        "                for i in range(0,dataLineWidth):\n",
        "                    outputNeurons.append(\"inX\"+str(i))\n",
        "                    sys.stdout.write(\"Input inX\"+str(i)+\"\\n\")\n",
        "\n",
        "                neurons[dataLineName] = outputNeurons\n",
        "                return outputNeurons\n",
        "                \n",
        "            elif layer[\"type\"]==\"Data\":\n",
        "            \n",
        "                # Input layer\n",
        "                assert dataLineWidth!=None\n",
        "                \n",
        "                # We assume normalization by a factor of 1/256.0 here.\n",
        "                assert layer['transform_param']['scale'] == 0.00390625\n",
        "\n",
        "                outputNeurons = []\n",
        "                for i in range(0,dataLineWidth):\n",
        "                    outputNeurons.append(\"inX\"+str(i))\n",
        "                    sys.stdout.write(\"Input inX\"+str(i)+\"\\n\")\n",
        "\n",
        "                neurons[dataLineName] = outputNeurons\n",
        "                return outputNeurons\n",
        "                \n",
        "            elif layer[\"type\"]==\"Reshape\":\n",
        "            \n",
        "                # Reshape layer\n",
        "                outputDimension = layer['reshape_param']['shape']['dim']\n",
        "                # print layer\n",
        "                assert outputDimension[0]==-1 # The first dimension is always the sample points\n",
        "                \n",
        "                nofInputs = 1\n",
        "                for a in outputDimension[1:]:\n",
        "                    nofInputs *= a\n",
        "                inputNeurons = recurseProcessLayer(layer[\"bottom\"][0],nofInputs)\n",
        "\n",
        "                \n",
        "                # Ok, first flatten input Neurons\n",
        "                def flat(neurons):\n",
        "                    if type(neurons)==str or type(neurons)==str:\n",
        "                        return [neurons]\n",
        "                    else:\n",
        "                        l = []\n",
        "                        for a in neurons:\n",
        "                            l.extend(flat(a))\n",
        "                        return l\n",
        "                \n",
        "                # Ok, first flatten input Neurons\n",
        "                #def flat(neurons):\n",
        "                #    if type(neurons)==str or type(neurons)==unicode or type(neurons)==int:\n",
        "                #        return [neurons]\n",
        "                #    else:\n",
        "                #        l = []\n",
        "                #        for a in neurons:\n",
        "                #            l.append(flat(a))\n",
        "                #        return [a for j in zip(*l) for a in j ]\n",
        "\n",
        "                flattenedNeurons = flat(inputNeurons)\n",
        "                \n",
        "                # Ok, now reshape\n",
        "                def unflatten(neurons,remainingDimensions):\n",
        "                    if len(remainingDimensions)==1:\n",
        "                        return (neurons[0:remainingDimensions[0]],neurons[remainingDimensions[0]:])\n",
        "                    else:\n",
        "                        res = []\n",
        "                        for a in range(0,remainingDimensions[0]):\n",
        "                            (d,neurons) = unflatten(neurons,remainingDimensions[1:])\n",
        "                            res.append(d)\n",
        "                        return (res,neurons)\n",
        "                \n",
        "                # Ok, now reshape\n",
        "                #def unflatten(neurons,selection,dimensions):\n",
        "                #    if len(selection)==len(dimensions):\n",
        "                #        index = 0\n",
        "                #        factor = 1\n",
        "                #        for i in range(0,len(selection)):\n",
        "                #            index += factor*selection[i]\n",
        "                #            factor *= dimensions[i]\n",
        "                #        return neurons[index]\n",
        "                #    else:\n",
        "                #        res = []\n",
        "                #        for a in range(0,dimensions[len(selection)]):\n",
        "                #            res.append(unflatten(neurons,selection+[a],dimensions))\n",
        "                #        return res\n",
        "\n",
        "                \n",
        "                (unflattenedNeurons,rest) = unflatten(flattenedNeurons,outputDimension[1:])\n",
        "                assert rest==[]\n",
        "                # print outputDimension\n",
        "                # print inputNeurons\n",
        "                # print flattenedNeurons\n",
        "                # print \"-----UF:->\",unflattenedNeurons\n",
        "                assert len(flattenedNeurons)==reduce(operator.mul, outputDimension[1:], 1)\n",
        "                              \n",
        "                return unflattenedNeurons\n",
        "\n",
        "            elif layer[\"type\"]==\"Pooling\":\n",
        "            \n",
        "                # Reshape layer\n",
        "                inputNeurons = recurseProcessLayer(layer[\"bottom\"][0],None)\n",
        "                \n",
        "                assert layer[\"pooling_param\"][\"pool\"]==0 # Must be a MAXPOOL (for the time being)\n",
        "                \n",
        "                # ---> Other parameters\n",
        "                #--------> Read Stride\n",
        "                if \"stride\" in layer[\"pooling_param\"]:\n",
        "                    # Why is the \"stride\" here an int, but for the Convolution layer is a list?\n",
        "                    stride = layer[\"pooling_param\"][\"stride\"]\n",
        "                    if isinstance(stride,list):\n",
        "                        assert len(stride)==1\n",
        "                        stride = stride + stride\n",
        "                    else:\n",
        "                        stride = [stride,stride]\n",
        "                else:\n",
        "                    if \"stride_w\" in layer[\"pooling_param\"]:\n",
        "                        if \"stride_h\" in layer[\"pooling_param\"]:\n",
        "                            stride = [layer[\"pooling_param\"][\"stride_w\"],layer[\"pooling_param\"][\"stride_h\"]]\n",
        "                        else:\n",
        "                            stride = [layer[\"pooling_param\"][\"stride_w\"],1]\n",
        "                    else:\n",
        "                        if \"stride_h\" in layer[\"pooling_param\"]:\n",
        "                            stride = [1,layer[\"pooling_param\"][\"stride_h\"]]\n",
        "                        else:\n",
        "                            stride = [1,1]\n",
        "                \n",
        "                #--------> Read Kernel Size\n",
        "                if \"kernel_size\" in layer[\"pooling_param\"]:\n",
        "                    # Why is the \"kernel_size\" here an int, but for the Convolution layer is a list?\n",
        "                    kernel_size = layer[\"pooling_param\"][\"kernel_size\"]\n",
        "                    if isinstance(kernel_size,list):\n",
        "                        assert len(kernel_size)==1\n",
        "                        kernel_size = kernel_size + kernel_size\n",
        "                    else:\n",
        "                        kernel_size = [kernel_size,kernel_size]\n",
        "                else:\n",
        "                    kernel_size = [layer[\"pooling_param\"][\"kernel_w\"],layer[\"pooling_param\"][\"kernel_h\"]]\n",
        "\n",
        "                #--------> Read PAD\n",
        "                if \"pad\" in layer[\"pooling_param\"]:\n",
        "                    padding = layer[\"pooling_param\"][\"pad\"]\n",
        "                    assert len(padding)==1\n",
        "                    padding = padding + padding\n",
        "                else:\n",
        "                    if \"pad_w\" in layer[\"pooling_param\"]:\n",
        "                        if \"pad_h\" in layer[\"pooling_param\"]:\n",
        "                            padding = [layer[\"pooling_param\"][\"pad_w\"],layer[\"pooling_param\"][\"pad_h\"]]\n",
        "                        else:\n",
        "                            padding = [layer[\"pooling_param\"][\"pad_w\"],0]\n",
        "                    else:\n",
        "                        if \"pad_h\" in layer[\"pooling_param\"]:\n",
        "                            padding = [0,layer[\"pooling_param\"][\"pad_h\"]]\n",
        "                        else:\n",
        "                            padding = [0,0]\n",
        "\n",
        "                \n",
        "                # Here, we assume that the \"inputNeurons\" array is three-dimensional:\n",
        "                # - color channel\n",
        "                # - X channel\n",
        "                # - Y channel\n",
        "                resultingNeurons = []\n",
        "                for i,channel in enumerate(inputNeurons):\n",
        "                    ysize = len(channel)\n",
        "                    xsize = len(channel[0])\n",
        "                    thisBlock = []\n",
        "                    for y in range(-1*padding[1],ysize-kernel_size[1]+1+padding[1],stride[1]):\n",
        "                        thisLine = []                    \n",
        "                        for x in range(-1*padding[0],xsize-kernel_size[0]+1+padding[0],stride[0]):\n",
        "                            thisLine.append(dataLineName+\"X\"+str(i)+\"X\"+str(x)+\"X\"+str(y))\n",
        "                            localInputs = []\n",
        "                            for b in range(0,kernel_size[1]):\n",
        "                                for a in range(0,kernel_size[0]):\n",
        "                                    if y+b>=0 and y+b<len(channel):\n",
        "                                        if x+a>=0 and x+a<len(channel[y+b]):\n",
        "                                            localInputs.append(channel[y+b][x+a]) \n",
        "                            sys.stdout.write(\"MaxPool \"+thisLine[-1]+\" \"+\" \".join(localInputs)+\"\\n\")\n",
        "                        thisBlock.append(thisLine)\n",
        "                    resultingNeurons.append(thisBlock)\n",
        "                return resultingNeurons                    \n",
        "                    \n",
        "            else:\n",
        "                raise RuntimeError(\"Unsupported Layer Type: \"+layer[\"type\"])\n",
        "            \n",
        "    raise \"Error: Data Line \"+dataLineName+\" not found.\"\n",
        "\n",
        "\n",
        "# Process the Accuracy layer\n",
        "foundAccurracyLayer = False\n",
        "for layer in data:\n",
        "    if layer['type'] == 'Accuracy':\n",
        "        foundAccurracyLayer = True\n",
        "        outputs = recurseProcessLayer(layer[\"bottom\"][0],None)\n",
        "        for i in range(0,len(outputs)):\n",
        "            sys.stdout.write(\"Linear outX\"+str(i)+\" 0.0 1.0 \"+outputs[i]+\"\\n\")\n",
        "                \n",
        "if not foundAccurracyLayer:\n",
        "    print(sys.stderr, \"Warning: No 'Accuracy' layer found, hence nothing was translated.\")"
      ],
      "metadata": {
        "id": "VhZoZYycCQSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run planet with your RLV file"
      ],
      "metadata": {
        "id": "i1cmxBBICojK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add contraints on Input Variables for Planet\n",
        "%cd /content/\n",
        "\n",
        "with open(\"output.rlv\", \"ab\") as f:\n",
        "  for i in range(28*28):\n",
        "    linebreak = bytes(\"\\n\", \"utf-8\")\n",
        "    assert_lowerbound = bytes(\"Assert <= 0.0 1.0 inX\" + str(i), \"utf-8\")\n",
        "    assert_upperbound = bytes(\"Assert >= 1.0 1.0 inX\" + str(i), \"utf-8\")\n",
        "\n",
        "    f.write(linebreak)\n",
        "    f.write(assert_lowerbound)\n",
        "    f.write(linebreak)\n",
        "    f.write(assert_upperbound)"
      ],
      "metadata": {
        "id": "5V2ezS_Dwy9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verify Robustness with Planet"
      ],
      "metadata": {
        "id": "prLQbKiridL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-mnist"
      ],
      "metadata": {
        "id": "l-Lee6NzpFFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mnist import MNIST\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "data = MNIST(\"/content/caffe/data/mnist/\")\n",
        "\n",
        "imgs, labels = data.load_training()\n",
        "\n",
        "img = np.asarray(imgs[0]).reshape(28, 28)\n",
        "\n",
        "plt.title(\"Label: {}\".format(labels[0]))\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "cB0NIuWMiaz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.05\n",
        "\n",
        "def addBounds(i, l_bound, u_bound):\n",
        "  with open(\"/content/output.rlv\", \"ab\") as f:\n",
        "    linebreak = bytes(\"\\n\", \"utf-8\")\n",
        "    assert_lowerbound = bytes(\"Assert <= {} 1.0 inX{}\".format(l_bound, i), \"utf-8\")\n",
        "    assert_upperbound = bytes(\"Assert >= {} 1.0 inX{}\".format(u_bound, i), \"utf-8\")\n",
        "\n",
        "    f.write(linebreak)\n",
        "    f.write(assert_lowerbound)\n",
        "    f.write(linebreak)\n",
        "    f.write(assert_upperbound)\n",
        "\n",
        "counter = 0\n",
        "for h in range(img.shape[0]):\n",
        "  for w in range(img.shape[1]):\n",
        "    lower_bound = img[h][w] - epsilon\n",
        "    upper_bound = img[h][w] + epsilon\n",
        "    addBounds(counter, lower_bound, upper_bound)\n",
        "    counter += 1\n"
      ],
      "metadata": {
        "id": "2fteFaDxrWRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removes assert conditions and empty lines\n",
        "\n",
        "!sed -i \"/Assert/d\" /content/output.rlv\n",
        "!sed -i \"/^$/d\" /content/output.rlv "
      ],
      "metadata": {
        "id": "nVLvoMLsvtqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run planet\n",
        "#!/content/planet/src/planet /content/caffemodel_mnist.rlv\n",
        "!/content/planet/src/planet /content/output.rlv"
      ],
      "metadata": {
        "id": "X1jmu4Pgw--g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Current TODO:\n",
        "The Output Variables are not constrained, we probably need to do that next!"
      ],
      "metadata": {
        "id": "erMO_H9_yauq"
      }
    }
  ]
}