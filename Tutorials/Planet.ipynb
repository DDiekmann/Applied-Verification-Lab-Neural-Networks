{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Planet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DDiekmann/Applied-Verification-Lab-Neural-Networks/blob/main/Tutorials/Planet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Planet\n",
        "[Github Repository](https://github.com/progirep/planet)"
      ],
      "metadata": {
        "id": "8Ht-gLaou9-K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MZedEeKnXjHc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Clone the repo\n",
        "!git clone https://github.com/progirep/planet.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# we need a script to convert the neural network description in to the \"prototxt\" format (produced by Caffe) into a json file\n",
        "!cd tools\n",
        "!wget https://gist.github.com/progirep/fd7d2dc120862faa984a70f503611013/raw/260e1e76cebd0ea58bf1a03b64c3f1e0002fc677/csv_to_hdf5_supervised_classification.py \n",
        "\n",
        "# we need a second script to generate a database in \"HDF5\" format from comma-separated value files\n",
        "!wget https://raw.githubusercontent.com/vadimkantorov/caffemodel2json/3a8fd443bf1596dad5f517aecdef08a81bf73bfe/caffemodel2json.py"
      ],
      "metadata": {
        "id": "H3Qz6yJ2Ymkf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# install packages in order to build PLANET\n",
        "!sudo apt-get install libglpk-dev\n",
        "!sudo apt-get install qt5-qmake\n",
        "!sudo apt-get install valgrind\n",
        "!sudo apt-get install libltdl-dev\n",
        "!sudo apt-get install protobuf-compiler"
      ],
      "metadata": {
        "id": "FiERp9lPZpRR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# compile the source code\n",
        "%cd planet/src\n",
        "%ls\n",
        "!qmake Tool.pro\n",
        "!make"
      ],
      "metadata": {
        "id": "azIQnanwahJA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Caffe - Currently not working\n",
        "\n",
        "Following [this tutorial](https://colab.research.google.com/github/Huxwell/caffe-colab/blob/main/caffe_details.ipynb). Caution: this takes 5 minutes."
      ],
      "metadata": {
        "id": "684Qyid6YTHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caffe currently doesnt work!!!!!!\n",
        "\n",
        "%%capture\n",
        "\n",
        "# install Caffe and Yices\n",
        "# change root path of #CAFFE and #YICES\n",
        "!sudo apt install caffe-cuda\n",
        "!sudo add-apt-repository ppa:sri-csl/formal-methods -qq\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install yices2"
      ],
      "metadata": {
        "id": "g8PelEMlajl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/BVLC/caffe.git"
      ],
      "metadata": {
        "id": "PzUPIFFZYkiq",
        "outputId": "e9e2ef17-0593-4dde-9bf7-fc0c80d34630",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'caffe'...\n",
            "remote: Enumerating objects: 65274, done.\u001b[K\n",
            "remote: Total 65274 (delta 0), reused 0 (delta 0), pack-reused 65274\u001b[K\n",
            "Receiving objects: 100% (65274/65274), 74.14 MiB | 14.31 MiB/s, done.\n",
            "Resolving deltas: 100% (41245/41245), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture \n",
        "!sudo apt-get install libgflags2.2 \n",
        "!sudo apt-get install libgflags-dev\n",
        "!sudo apt-get install libgoogle-glog-dev\n",
        "!sudo apt-get install libhdf5-100\n",
        "!sudo apt-get install libhdf5-serial-dev\n",
        "!sudo apt-get install libhdf5-dev\n",
        "!sudo apt-get install libhdf5-cpp-100\n",
        "!sudo apt-get install libprotobuf-dev protobuf-compiler"
      ],
      "metadata": {
        "id": "J2juacyoYrKV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find /usr -iname \"*hdf5.so\"\n",
        "# got: /usr/lib/x86_64-linux-gnu/hdf5/serial\n",
        "!find /usr -iname \"*hdf5_hl.so\""
      ],
      "metadata": {
        "id": "BBaRB7xuZACp",
        "outputId": "410663aa-ac3e-41f2-8cb1-e6da832129dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5.so\n",
            "/usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5_hl.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /usr/lib/x86_64-linux-gnu/libhdf5_serial.so /usr/lib/x86_64-linux-gnu/libhdf5.so\n",
        "!ln -s /usr/lib/x86_64-linux-gnu/libhdf5_serial_hl.so /usr/lib/x86_64-linux-gnu/libhdf5_hl.so"
      ],
      "metadata": {
        "id": "MaRpc2_JZEdC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!find /usr -iname \"*hdf5.h*\" # got:\n",
        "# /usr/include/hdf5/serial/hdf5.h \n",
        "# /usr/include/opencv2/flann/hdf5.h\n",
        "# Let's try the first one.\n",
        "%env CPATH=\"/usr/include/hdf5/serial/\"\n",
        "#fatal error: hdf5.h: No such file or directory"
      ],
      "metadata": {
        "id": "yocwisfyZLaO",
        "outputId": "59afeb15-ff9b-4a4f-fb7c-be7d0787bc9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CPATH=\"/usr/include/hdf5/serial/\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo apt-get install libleveldb-dev\n",
        "!sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev\n",
        "!sudo apt-get install libsnappy-dev"
      ],
      "metadata": {
        "id": "oelKebsrZM_r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build caffe from source files."
      ],
      "metadata": {
        "id": "3JXTpkNWZV1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $CPATH"
      ],
      "metadata": {
        "id": "oXGdHIdKZYMq",
        "outputId": "a819406d-8033-495d-a4ef-a01f753faa9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"/usr/include/hdf5/serial/\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd caffe\n",
        "\n",
        "!ls\n",
        "!make clean\n",
        "!cp Makefile.config.example Makefile.config"
      ],
      "metadata": {
        "id": "XUc1y_V3ZaiF",
        "outputId": "74def0e2-a8c7-45d9-e1be-c28523dc908f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/caffe\n",
            "caffe.cloc\t data\t   INSTALL.md\t\t    models     tools\n",
            "cmake\t\t docker    LICENSE\t\t    python\n",
            "CMakeLists.txt\t docs\t   Makefile\t\t    README.md\n",
            "CONTRIBUTING.md  examples  Makefile.config.example  scripts\n",
            "CONTRIBUTORS.md  include   matlab\t\t    src\n",
            "Makefile:6: *** Makefile.config not found. See Makefile.config.example..  Stop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/-gencode arch=compute_20/#-gencode arch=compute_20/' Makefile.config #old cuda versions won't compile \n",
        "!sed -i 's/\\/usr\\/local\\/include/\\/usr\\/local\\/include \\/usr\\/include\\/hdf5\\/serial\\//'  Makefile.config #one of the 4 things needed to fix hdf5 issues\n",
        "!sed -i 's/# OPENCV_VERSION := 3/OPENCV_VERSION := 3/' Makefile.config #We actually use opencv 4.1.2, but it's similar enough to opencv 3.\n",
        "!sed -i 's/code=compute_61/code=compute_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75/' Makefile.config #support for new GPUs"
      ],
      "metadata": {
        "id": "jadFlxc6ZkDQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!make all -j 4 # -j would use all availiable cores, but RAM related errors occur"
      ],
      "metadata": {
        "id": "zXC_OPCEZmlX",
        "outputId": "1a4add39-2dcd-4c1c-fdf7-e3b2f0d413fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROTOC src/caffe/proto/caffe.proto\n",
            "NVCC src/caffe/layers/log_layer.cu\n",
            "NVCC src/caffe/util/math_functions.cu\n",
            "NVCC src/caffe/util/im2col.cu\n",
            "NVCC src/caffe/layers/power_layer.cu\n",
            "In file included from \u001b[01m\u001b[Ksrc/caffe/util/math_functions.cu:1:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/math_functions.h:54:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[Ksrc/caffe/util/math_functions.cu:1:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/math_functions.h:54:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "NVCC src/caffe/layers/concat_layer.cu\n",
            "NVCC src/caffe/layers/recurrent_layer.cu\n",
            "In file included from src/caffe/util/math_functions.cu:1:0:\n",
            "/usr/local/cuda/include/math_functions.h:54:2: warning: #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [-Wcpp]\n",
            " #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\n",
            "  ^~~~~~~\n",
            "In file included from src/caffe/util/math_functions.cu:1:0:\n",
            "/usr/local/cuda/include/math_functions.h:54:2: warning: #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [-Wcpp]\n",
            " #warning \"math_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\n",
            "  ^~~~~~~\n",
            "NVCC src/caffe/layers/cudnn_pooling_layer.cu\n",
            "NVCC src/caffe/layers/dropout_layer.cu\n",
            "NVCC src/caffe/layers/hdf5_output_layer.cu\n",
            "NVCC src/caffe/layers/cudnn_lrn_layer.cu\n",
            "NVCC src/caffe/layers/lrn_layer.cu\n",
            "NVCC src/caffe/layers/cudnn_softmax_layer.cu\n",
            "NVCC src/caffe/layers/elu_layer.cu\n",
            "NVCC src/caffe/layers/embed_layer.cu\n",
            "NVCC src/caffe/layers/sigmoid_cross_entropy_loss_layer.cu\n",
            "NVCC src/caffe/layers/clip_layer.cu\n",
            "NVCC src/caffe/layers/reduction_layer.cu\n",
            "NVCC src/caffe/layers/threshold_layer.cu\n",
            "NVCC src/caffe/layers/slice_layer.cu\n",
            "NVCC src/caffe/layers/tile_layer.cu\n",
            "NVCC src/caffe/layers/cudnn_sigmoid_layer.cu\n",
            "NVCC src/caffe/layers/split_layer.cu\n",
            "NVCC src/caffe/layers/lstm_unit_layer.cu\n",
            "NVCC src/caffe/layers/crop_layer.cu\n",
            "NVCC src/caffe/layers/batch_reindex_layer.cu\n",
            "NVCC src/caffe/layers/accuracy_layer.cu\n",
            "NVCC src/caffe/layers/batch_norm_layer.cu\n",
            "NVCC src/caffe/layers/deconv_layer.cu\n",
            "NVCC src/caffe/layers/cudnn_conv_layer.cu\n",
            "NVCC src/caffe/layers/absval_layer.cu\n",
            "NVCC src/caffe/layers/scale_layer.cu\n",
            "NVCC src/caffe/layers/relu_layer.cu\n",
            "NVCC src/caffe/layers/base_data_layer.cu\n",
            "NVCC src/caffe/layers/mvn_layer.cu\n",
            "NVCC src/caffe/layers/inner_product_layer.cu\n",
            "NVCC src/caffe/layers/swish_layer.cu\n",
            "NVCC src/caffe/layers/contrastive_loss_layer.cu\n",
            "NVCC src/caffe/layers/bnll_layer.cu\n",
            "NVCC src/caffe/layers/im2col_layer.cu\n",
            "NVCC src/caffe/layers/cudnn_tanh_layer.cu\n",
            "NVCC src/caffe/layers/pooling_layer.cu\n",
            "NVCC src/caffe/layers/filter_layer.cu\n",
            "NVCC src/caffe/layers/cudnn_deconv_layer.cu\n",
            "NVCC src/caffe/layers/exp_layer.cu\n",
            "NVCC src/caffe/layers/sigmoid_layer.cu\n",
            "NVCC src/caffe/layers/conv_layer.cu\n",
            "NVCC src/caffe/layers/silence_layer.cu\n",
            "NVCC src/caffe/layers/cudnn_lcn_layer.cu\n",
            "NVCC src/caffe/layers/cudnn_relu_layer.cu\n",
            "NVCC src/caffe/layers/prelu_layer.cu\n",
            "NVCC src/caffe/layers/softmax_loss_layer.cu\n",
            "NVCC src/caffe/layers/softmax_layer.cu\n",
            "NVCC src/caffe/layers/hdf5_data_layer.cu\n",
            "NVCC src/caffe/layers/tanh_layer.cu\n",
            "NVCC src/caffe/layers/eltwise_layer.cu\n",
            "NVCC src/caffe/layers/euclidean_loss_layer.cu\n",
            "NVCC src/caffe/layers/bias_layer.cu\n",
            "NVCC src/caffe/solvers/sgd_solver.cu\n",
            "NVCC src/caffe/solvers/adam_solver.cu\n",
            "NVCC src/caffe/solvers/rmsprop_solver.cu\n",
            "NVCC src/caffe/solvers/nesterov_solver.cu\n",
            "NVCC src/caffe/solvers/adadelta_solver.cu\n",
            "NVCC src/caffe/solvers/adagrad_solver.cu\n",
            "CXX tools/upgrade_net_proto_text.cpp\n",
            "CXX tools/convert_imageset.cpp\n",
            "CXX tools/upgrade_solver_proto_text.cpp\n",
            "CXX tools/upgrade_net_proto_binary.cpp\n",
            "CXX tools/compute_image_mean.cpp\n",
            "CXX tools/extract_features.cpp\n",
            "CXX tools/caffe.cpp\n",
            "CXX examples/siamese/convert_mnist_siamese_data.cpp\n",
            "CXX examples/cifar10/convert_cifar_data.cpp\n",
            "CXX examples/mnist/convert_mnist_data.cpp\n",
            "CXX examples/cpp_classification/classification.cpp\n",
            "CXX .build_release/src/caffe/proto/caffe.pb.cc\n",
            "CXX src/caffe/parallel.cpp\n",
            "CXX src/caffe/util/db_leveldb.cpp\n",
            "CXX src/caffe/util/hdf5.cpp\n",
            "CXX src/caffe/util/blocking_queue.cpp\n",
            "CXX src/caffe/util/db_lmdb.cpp\n",
            "CXX src/caffe/util/db.cpp\n",
            "CXX src/caffe/util/math_functions.cpp\n",
            "CXX src/caffe/util/signal_handler.cpp\n",
            "CXX src/caffe/util/io.cpp\n",
            "CXX src/caffe/util/im2col.cpp\n",
            "CXX src/caffe/util/cudnn.cpp\n",
            "CXX src/caffe/util/upgrade_proto.cpp\n",
            "CXX src/caffe/util/insert_splits.cpp\n",
            "CXX src/caffe/util/benchmark.cpp\n",
            "CXX src/caffe/layer.cpp\n",
            "CXX src/caffe/internal_thread.cpp\n",
            "CXX src/caffe/solver.cpp\n",
            "CXX src/caffe/syncedmem.cpp\n",
            "CXX src/caffe/data_transformer.cpp\n",
            "CXX src/caffe/layers/log_layer.cpp\n",
            "CXX src/caffe/layers/scale_layer.cpp\n",
            "CXX src/caffe/layers/exp_layer.cpp\n",
            "CXX src/caffe/layers/hdf5_output_layer.cpp\n",
            "CXX src/caffe/layers/embed_layer.cpp\n",
            "CXX src/caffe/layers/flatten_layer.cpp\n",
            "CXX src/caffe/layers/base_data_layer.cpp\n",
            "CXX src/caffe/layers/slice_layer.cpp\n",
            "CXX src/caffe/layers/window_data_layer.cpp\n",
            "CXX src/caffe/layers/euclidean_loss_layer.cpp\n",
            "CXX src/caffe/layers/conv_layer.cpp\n",
            "CXX src/caffe/layers/cudnn_sigmoid_layer.cpp\n",
            "CXX src/caffe/layers/cudnn_lcn_layer.cpp\n",
            "CXX src/caffe/layers/split_layer.cpp\n",
            "CXX src/caffe/layers/softmax_layer.cpp\n",
            "CXX src/caffe/layers/hdf5_data_layer.cpp\n",
            "CXX src/caffe/layers/cudnn_pooling_layer.cpp\n",
            "CXX src/caffe/layers/mvn_layer.cpp\n",
            "CXX src/caffe/layers/data_layer.cpp\n",
            "CXX src/caffe/layers/deconv_layer.cpp\n",
            "CXX src/caffe/layers/absval_layer.cpp\n",
            "CXX src/caffe/layers/concat_layer.cpp\n",
            "CXX src/caffe/layers/silence_layer.cpp\n",
            "CXX src/caffe/layers/cudnn_conv_layer.cpp\n",
            "CXX src/caffe/layers/lstm_layer.cpp\n",
            "CXX src/caffe/layers/cudnn_softmax_layer.cpp\n",
            "CXX src/caffe/layers/threshold_layer.cpp\n",
            "CXX src/caffe/layers/pooling_layer.cpp\n",
            "CXX src/caffe/layers/crop_layer.cpp\n",
            "CXX src/caffe/layers/prelu_layer.cpp\n",
            "CXX src/caffe/layers/im2col_layer.cpp\n",
            "CXX src/caffe/layers/reshape_layer.cpp\n",
            "CXX src/caffe/layers/lstm_unit_layer.cpp\n",
            "CXX src/caffe/layers/spp_layer.cpp\n",
            "CXX src/caffe/layers/filter_layer.cpp\n",
            "CXX src/caffe/layers/dummy_data_layer.cpp\n",
            "CXX src/caffe/layers/recurrent_layer.cpp\n",
            "CXX src/caffe/layers/eltwise_layer.cpp\n",
            "CXX src/caffe/layers/clip_layer.cpp\n",
            "CXX src/caffe/layers/input_layer.cpp\n",
            "CXX src/caffe/layers/cudnn_deconv_layer.cpp\n",
            "CXX src/caffe/layers/memory_data_layer.cpp\n",
            "CXX src/caffe/layers/bnll_layer.cpp\n",
            "CXX src/caffe/layers/sigmoid_layer.cpp\n",
            "CXX src/caffe/layers/relu_layer.cpp\n",
            "CXX src/caffe/layers/parameter_layer.cpp\n",
            "CXX src/caffe/layers/tanh_layer.cpp\n",
            "CXX src/caffe/layers/cudnn_relu_layer.cpp\n",
            "CXX src/caffe/layers/lrn_layer.cpp\n",
            "CXX src/caffe/layers/batch_reindex_layer.cpp\n",
            "CXX src/caffe/layers/loss_layer.cpp\n",
            "CXX src/caffe/layers/inner_product_layer.cpp\n",
            "CXX src/caffe/layers/cudnn_lrn_layer.cpp\n",
            "CXX src/caffe/layers/reduction_layer.cpp\n",
            "CXX src/caffe/layers/rnn_layer.cpp\n",
            "CXX src/caffe/layers/softmax_loss_layer.cpp\n",
            "CXX src/caffe/layers/dropout_layer.cpp\n",
            "CXX src/caffe/layers/bias_layer.cpp\n",
            "CXX src/caffe/layers/base_conv_layer.cpp\n",
            "CXX src/caffe/layers/multinomial_logistic_loss_layer.cpp\n",
            "CXX src/caffe/layers/elu_layer.cpp\n",
            "CXX src/caffe/layers/power_layer.cpp\n",
            "CXX src/caffe/layers/swish_layer.cpp\n",
            "CXX src/caffe/layers/infogain_loss_layer.cpp\n",
            "CXX src/caffe/layers/hinge_loss_layer.cpp\n",
            "CXX src/caffe/layers/contrastive_loss_layer.cpp\n",
            "CXX src/caffe/layers/neuron_layer.cpp\n",
            "CXX src/caffe/layers/cudnn_tanh_layer.cpp\n",
            "CXX src/caffe/layers/argmax_layer.cpp\n",
            "CXX src/caffe/layers/image_data_layer.cpp\n",
            "CXX src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp\n",
            "CXX src/caffe/layers/batch_norm_layer.cpp\n",
            "CXX src/caffe/layers/accuracy_layer.cpp\n",
            "CXX src/caffe/layers/tile_layer.cpp\n",
            "CXX src/caffe/blob.cpp\n",
            "CXX src/caffe/solvers/nesterov_solver.cpp\n",
            "CXX src/caffe/solvers/sgd_solver.cpp\n",
            "CXX src/caffe/solvers/rmsprop_solver.cpp\n",
            "CXX src/caffe/solvers/adagrad_solver.cpp\n",
            "CXX src/caffe/solvers/adam_solver.cpp\n",
            "CXX src/caffe/solvers/adadelta_solver.cpp\n",
            "CXX src/caffe/common.cpp\n",
            "CXX src/caffe/net.cpp\n",
            "CXX src/caffe/layer_factory.cpp\n",
            "AR -o .build_release/lib/libcaffe.a\n",
            "LD -o .build_release/lib/libcaffe.so.1.0.0\n",
            "CXX/LD -o .build_release/tools/convert_imageset.bin\n",
            "CXX/LD -o .build_release/tools/upgrade_net_proto_binary.bin\n",
            "CXX/LD -o .build_release/tools/upgrade_solver_proto_text.bin\n",
            "CXX/LD -o .build_release/tools/upgrade_net_proto_text.bin\n",
            "CXX/LD -o .build_release/tools/compute_image_mean.bin\n",
            "CXX/LD -o .build_release/tools/extract_features.bin\n",
            "CXX/LD -o .build_release/tools/caffe.bin\n",
            "CXX/LD -o .build_release/examples/siamese/convert_mnist_siamese_data.bin\n",
            "CXX/LD -o .build_release/examples/cifar10/convert_cifar_data.bin\n",
            "CXX/LD -o .build_release/examples/mnist/convert_mnist_data.bin\n",
            "CXX/LD -o .build_release/examples/cpp_classification/classification.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -iname \"*caffe*\""
      ],
      "metadata": {
        "id": "mZrkU74_8fgA",
        "outputId": "12a31373-eb1a-4517-eef9-845293f7249c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/onnx/symbolic_caffe2.py\n",
            "/usr/local/lib/python3.7/dist-packages/torch/onnx/__pycache__/symbolic_caffe2.cpython-37.pyc\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/_caffe2_graph.py\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/__pycache__/_caffe2_graph.cpython-37.pyc\n",
            "/usr/local/lib/python3.7/dist-packages/torch/share/cmake/Caffe2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/share/cmake/Caffe2/Caffe2ConfigVersion.cmake\n",
            "/usr/local/lib/python3.7/dist-packages/torch/share/cmake/Caffe2/Caffe2Targets.cmake\n",
            "/usr/local/lib/python3.7/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake\n",
            "/usr/local/lib/python3.7/dist-packages/torch/share/cmake/Caffe2/Caffe2Targets-release.cmake\n",
            "/usr/local/lib/python3.7/dist-packages/torch/lib/libcaffe2_nvrtc.so\n",
            "/usr/local/lib/python3.7/dist-packages/torch/include/caffe2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/include/caffe2/core/export_caffe2_op_to_c10.h\n",
            "/usr/local/lib/python3.7/dist-packages/torch/include/caffe2/core/export_c10_op_to_caffe2.h\n",
            "/usr/local/lib/python3.7/dist-packages/torch/include/caffe2/proto/caffe2.pb.h\n",
            "/usr/local/lib/python3.7/dist-packages/torch/include/caffe2/proto/caffe2_pb.h\n",
            "/usr/local/lib/python3.7/dist-packages/caffe2\n",
            "/usr/local/lib/python3.7/dist-packages/caffe2/contrib/playground/resnetdemo/caffe2_resnet50_default_param_update.py\n",
            "/usr/local/lib/python3.7/dist-packages/caffe2/contrib/playground/resnetdemo/__pycache__/caffe2_resnet50_default_param_update.cpython-37.pyc\n",
            "/usr/local/lib/python3.7/dist-packages/caffe2/contrib/playground/resnetdemo/__pycache__/caffe2_resnet50_default_forward.cpython-37.pyc\n",
            "/usr/local/lib/python3.7/dist-packages/caffe2/contrib/playground/resnetdemo/caffe2_resnet50_default_forward.py\n",
            "/usr/local/lib/python3.7/dist-packages/caffe2/proto/__pycache__/caffe2_pb2.cpython-37.pyc\n",
            "/usr/local/lib/python3.7/dist-packages/caffe2/proto/caffe2_pb2.py\n",
            "/usr/local/lib/python3.7/dist-packages/caffe2/python/__pycache__/caffe_translator_test.cpython-37.pyc\n",
            "/usr/local/lib/python3.7/dist-packages/caffe2/python/__pycache__/caffe_translator.cpython-37.pyc\n",
            "/usr/local/lib/python3.7/dist-packages/caffe2/python/caffe_translator_test.py\n",
            "/usr/local/lib/python3.7/dist-packages/caffe2/python/caffe_translator.py\n",
            "/usr/local/bin/convert-onnx-to-caffe2\n",
            "/usr/local/bin/convert-caffe2-to-onnx\n",
            "find: ‘/proc/28/task/28/net’: Invalid argument\n",
            "find: ‘/proc/28/net’: Invalid argument\n",
            "/content/caffe\n",
            "/content/caffe/tools/caffe.cpp\n",
            "/content/caffe/cmake/Templates/caffe_config.h.in\n",
            "/content/caffe/cmake/Templates/CaffeConfig.cmake.in\n",
            "/content/caffe/cmake/Templates/CaffeConfigVersion.cmake.in\n",
            "/content/caffe/src/caffe\n",
            "/content/caffe/src/caffe/proto/caffe.proto\n",
            "/content/caffe/src/caffe/test/test_caffe_main.cpp\n",
            "/content/caffe/.build_release/tools/caffe.o\n",
            "/content/caffe/.build_release/tools/caffe\n",
            "/content/caffe/.build_release/tools/caffe.o.warnings.txt\n",
            "/content/caffe/.build_release/tools/caffe.bin\n",
            "/content/caffe/.build_release/tools/caffe.d\n",
            "/content/caffe/.build_release/lib/libcaffe.so.1.0.0\n",
            "/content/caffe/.build_release/lib/libcaffe.a\n",
            "/content/caffe/.build_release/lib/libcaffe.so\n",
            "/content/caffe/.build_release/src/caffe\n",
            "/content/caffe/.build_release/src/caffe/proto/caffe.pb.o.warnings.txt\n",
            "/content/caffe/.build_release/src/caffe/proto/caffe.pb.cc\n",
            "/content/caffe/.build_release/src/caffe/proto/caffe.pb.o\n",
            "/content/caffe/.build_release/src/caffe/proto/caffe.pb.h\n",
            "/content/caffe/.build_release/src/caffe/proto/caffe.pb.d\n",
            "/content/caffe/.build_release/matlab/+caffe\n",
            "/content/caffe/.build_release/cuda/src/caffe\n",
            "/content/caffe/.build_release/cuda/matlab/+caffe\n",
            "/content/caffe/.build_release/cuda/python/caffe\n",
            "/content/caffe/.build_release/python/caffe\n",
            "/content/caffe/matlab/+caffe\n",
            "/content/caffe/matlab/+caffe/private/caffe_.cpp\n",
            "/content/caffe/scripts/split_caffe_proto.py\n",
            "/content/caffe/scripts/caffe\n",
            "/content/caffe/caffe.cloc\n",
            "/content/caffe/examples/net_surgery/bvlc_caffenet_full_conv.prototxt\n",
            "/content/caffe/examples/imagenet/train_caffenet.sh\n",
            "/content/caffe/examples/pycaffe\n",
            "/content/caffe/examples/pycaffe/caffenet.py\n",
            "/content/caffe/docs/images/caffeine-icon.png\n",
            "/content/caffe/include/caffe\n",
            "/content/caffe/include/caffe/caffe.hpp\n",
            "/content/caffe/include/caffe/test/test_caffe_main.hpp\n",
            "/content/caffe/python/caffe\n",
            "/content/caffe/python/caffe/pycaffe.py\n",
            "/content/caffe/python/caffe/_caffe.cpp\n",
            "/content/caffe/models/bvlc_reference_caffenet\n",
            "/content/caffemodel_mnist.json\n",
            "/content/planet/casestudies/vehicleCollision/snapshots/_iter_300000.caffemodel\n",
            "/content/planet/casestudies/vehicleCollision/caffe_solver_with_pooling.prototxt\n",
            "/content/planet/casestudies/vehicleCollision/caffe_net_with_pooling.prototxt\n",
            "/content/planet/casestudies/MNIST/snapshots/_iter_200000.caffemodel\n",
            "/content/caffemodel2json.py\n",
            "/tensorflow-1.15.2/python3.7/lucid/modelzoo/caffe_models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Caffe model on MNIST"
      ],
      "metadata": {
        "id": "sjhA_H6Qi1Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloads mnist dataset\n",
        "\n",
        "%cd /content/caffe/\n",
        "\n",
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "!cp -rv MNIST/raw/* data/mnist/"
      ],
      "metadata": {
        "id": "Mim2_gdsnB4N",
        "outputId": "d97a613f-7928-4696-f918-3de7eec04378",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/caffe\n",
            "--2022-06-22 08:42:58--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
            "--2022-06-22 08:42:58--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘MNIST.tar.gz’\n",
            "\n",
            "MNIST.tar.gz            [               <=>  ]  33.20M  8.63MB/s    in 4.4s    \n",
            "\n",
            "2022-06-22 08:43:03 (7.52 MB/s) - ‘MNIST.tar.gz’ saved [34813078]\n",
            "\n",
            "MNIST/\n",
            "MNIST/raw/\n",
            "MNIST/raw/train-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "MNIST/raw/train-images-idx3-ubyte\n",
            "MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte\n",
            "MNIST/raw/train-images-idx3-ubyte.gz\n",
            "MNIST/processed/\n",
            "MNIST/processed/training.pt\n",
            "MNIST/processed/test.pt\n",
            "'MNIST/raw/t10k-images-idx3-ubyte' -> 'data/mnist/t10k-images-idx3-ubyte'\n",
            "'MNIST/raw/t10k-images-idx3-ubyte.gz' -> 'data/mnist/t10k-images-idx3-ubyte.gz'\n",
            "'MNIST/raw/t10k-labels-idx1-ubyte' -> 'data/mnist/t10k-labels-idx1-ubyte'\n",
            "'MNIST/raw/t10k-labels-idx1-ubyte.gz' -> 'data/mnist/t10k-labels-idx1-ubyte.gz'\n",
            "'MNIST/raw/train-images-idx3-ubyte' -> 'data/mnist/train-images-idx3-ubyte'\n",
            "'MNIST/raw/train-images-idx3-ubyte.gz' -> 'data/mnist/train-images-idx3-ubyte.gz'\n",
            "'MNIST/raw/train-labels-idx1-ubyte' -> 'data/mnist/train-labels-idx1-ubyte'\n",
            "'MNIST/raw/train-labels-idx1-ubyte.gz' -> 'data/mnist/train-labels-idx1-ubyte.gz'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creates mnist_test_lmdb and mnist_train_lmdb\n",
        "\n",
        "!/content/caffe/examples/mnist/create_mnist.sh"
      ],
      "metadata": {
        "id": "Wmq4yNR1nFcL",
        "outputId": "c9c56df2-420f-477e-9bf7-17d41d9dac52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating lmdb...\n",
            "I0622 08:43:07.853860  4789 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb\n",
            "I0622 08:43:07.854115  4789 convert_mnist_data.cpp:88] A total of 60000 items.\n",
            "I0622 08:43:07.854122  4789 convert_mnist_data.cpp:89] Rows: 28 Cols: 28\n",
            "I0622 08:43:08.573644  4789 convert_mnist_data.cpp:108] Processed 60000 files.\n",
            "I0622 08:43:09.721743  4793 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb\n",
            "I0622 08:43:09.722028  4793 convert_mnist_data.cpp:88] A total of 10000 items.\n",
            "I0622 08:43:09.722034  4793 convert_mnist_data.cpp:89] Rows: 28 Cols: 28\n",
            "I0622 08:43:09.839326  4793 convert_mnist_data.cpp:108] Processed 10000 files.\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copy lmdbs to planet folder\n",
        "\n",
        "%cp -a /content/caffe/examples/mnist/mnist_test_lmdb /content/planet/casestudies/MNIST/\n",
        "%cp -a /content/caffe/examples/mnist/mnist_train_lmdb /content/planet/casestudies/MNIST/"
      ],
      "metadata": {
        "id": "L4LUHluFAoFy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/planet/casestudies/MNIST/\n",
        "\n",
        "# change iterations from 200k to 50k\n",
        "!sed -i 's/200000/50000/g' /content/planet/casestudies/MNIST/lenet_solver.prototxt\n",
        "\n",
        "# remove snapshots\n",
        "!sed -i 's/snapshot:/#snapshot:/g' /content/planet/casestudies/MNIST/lenet_solver.prototxt\n",
        "!sed -i 's/snapshot_prefix:/#snapshot_prefix:/g' /content/planet/casestudies/MNIST/lenet_solver.prototxt\n",
        "\n",
        "# train the model\n",
        "!/content/caffe/build/tools/caffe train --solver=/content/planet/casestudies/MNIST/lenet_solver.prototxt $@"
      ],
      "metadata": {
        "id": "HQno6KNxnHM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "done: training of the model\n",
        "\n",
        "result: lenet_iter_50000.caffemodel and lenet_iter_50000.solverstate\n",
        "\n",
        "next step: convert caffemodel file to .rlv so PLANET can verify it\n"
      ],
      "metadata": {
        "id": "EiA9fgUwuJMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Caffe to JSON"
      ],
      "metadata": {
        "id": "H46WPknAXiRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/caffe/\n",
        "!make pycaffe\n",
        "!make distribute"
      ],
      "metadata": {
        "id": "Xc0-CX_tcxXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env CAFFE_ROOT=/content/caffe\n",
        "%env PYTHONPATH=/content/caffe/python:$PYTHONPATH"
      ],
      "metadata": {
        "id": "iAu_kL44eDcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import requests\n",
        "import argparse\n",
        "import tempfile\n",
        "import subprocess\n",
        "from google.protobuf.descriptor import FieldDescriptor as FD\n",
        "\n",
        "# inspired by https://github.com/dpp-name/protobuf-json/blob/master/protobuf_json.py\n",
        "def pb2json(pb, print_arrays):\n",
        "\t_ftype2js = {\n",
        "\t\tFD.TYPE_DOUBLE: float,\n",
        "\t\tFD.TYPE_FLOAT: float,\n",
        "\t\tFD.TYPE_INT64: long,\n",
        "\t\tFD.TYPE_UINT64: long,\n",
        "\t\tFD.TYPE_INT32: int,\n",
        "\t\tFD.TYPE_FIXED64: float,\n",
        "\t\tFD.TYPE_FIXED32: float,\n",
        "\t\tFD.TYPE_BOOL: bool,\n",
        "\t\tFD.TYPE_STRING: unicode,\n",
        "\t\tFD.TYPE_BYTES: lambda x: x.encode('string_escape'),\n",
        "\t\tFD.TYPE_UINT32: int,\n",
        "\t\tFD.TYPE_ENUM: int,\n",
        "\t\tFD.TYPE_SFIXED32: float,\n",
        "\t\tFD.TYPE_SFIXED64: float,\n",
        "\t\tFD.TYPE_SINT32: int,\n",
        "\t\tFD.TYPE_SINT64: long,\n",
        "\t\tFD.TYPE_MESSAGE: lambda x: pb2json(x, print_arrays = print_arrays),\n",
        "\t\t'unknown' : lambda x: 'Unknown field type: %s' % x\n",
        "\t}\n",
        "\tjs = {}\n",
        "\tfor field, value in pb.ListFields():\n",
        "\t\tftype = _ftype2js[field.type] if field.type in _ftype2js else _ftype2js['unknown']\n",
        "\t\tif field.label == FD.LABEL_REPEATED:\n",
        "\t\t\tjs_value = map(ftype, value)\n",
        "\t\t\tif not print_arrays and (field.name == 'data' and len(js_value) > 8):\n",
        "\t\t\t\thead_n = 5\n",
        "\t\t\t\tjs_value = js_value[:head_n] + ['(%d elements more)' % (len(js_value) - head_n)]\n",
        "\t\telse:\n",
        "\t\t\tjs_value = ftype(value)\n",
        "\t\tjs[field.name] = js_value\n",
        "\treturn js\n",
        "\n",
        "from caffe import *\n",
        "\n",
        "caffemodel_file = '/content/planet/casestudies/MNIST/lenet_solver_iter_50000.caffemodel'\n",
        "\n",
        "deserialized = caffe_pb2.NetParameter()\n",
        "deserialized.ParseFromString(open(caffemodel_file, 'rb').read())\n",
        "\n",
        "json.dump(pb2json(deserialized, args.data), sys.stdout, indent = 2)\n"
      ],
      "metadata": {
        "id": "ZwgmxWD4Xk-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caffe to JSON converter in Python3\n",
        "### TODO caffe doesnt work in python currently\n",
        "\n"
      ],
      "metadata": {
        "id": "QW1BFNkwCuA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import argparse\n",
        "import tempfile\n",
        "import subprocess\n",
        "from google.protobuf.descriptor import FieldDescriptor as FD\n",
        "\n",
        "# inspired by https://github.com/dpp-name/protobuf-json/blob/master/protobuf_json.py\n",
        "def pb2json(pb, print_arrays):\n",
        "\t_ftype2js = {\n",
        "        FD.TYPE_DOUBLE: float,\n",
        "\t\tFD.TYPE_FLOAT: float,\n",
        "\t\tFD.TYPE_INT64: int,\n",
        "\t\tFD.TYPE_UINT64: int,\n",
        "\t\tFD.TYPE_INT32: int,\n",
        "\t\tFD.TYPE_FIXED64: float,\n",
        "\t\tFD.TYPE_FIXED32: float,\n",
        "\t\tFD.TYPE_BOOL: bool,\n",
        "\t\tFD.TYPE_STRING: str,\n",
        "\t\tFD.TYPE_BYTES: lambda x: x.encode('string_escape'),\n",
        "\t\tFD.TYPE_UINT32: int,\n",
        "\t\tFD.TYPE_ENUM: int,\n",
        "\t\tFD.TYPE_SFIXED32: float,\n",
        "\t\tFD.TYPE_SFIXED64: float,\n",
        "\t\tFD.TYPE_SINT32: int,\n",
        "\t\tFD.TYPE_SINT64: int,\n",
        "\t\tFD.TYPE_MESSAGE: lambda x: pb2json(x, print_arrays = print_arrays),\n",
        "\t\t'unknown' : lambda x: 'Unknown field type: %s' % x\n",
        "\t}\n",
        "\tjs = {}\n",
        "\tfor field, value in pb.ListFields():\n",
        "\t\tftype = _ftype2js[field.type] if field.type in _ftype2js else _ftype2js['unknown']\n",
        "\t\tif field.label == FD.LABEL_REPEATED:\n",
        "\t\t\tjs_value = list(map(ftype, value))\n",
        "\t\t\tif not print_arrays and (field.name == 'data' and len(js_value) > 8):\n",
        "\t\t\t\thead_n = 5\n",
        "\t\t\t\tjs_value = js_value[:head_n] + ['(%d elements more)' % (len(js_value) - head_n)]\n",
        "\t\telse:\n",
        "\t\t\tjs_value = ftype(value)\n",
        "\t\tjs[field.name] = js_value\n",
        "\treturn js\n",
        "\n",
        "from caffe.proto import caffe_pb2\n",
        "\n",
        "caffe_file = \"lenet_solver_iter_50000.caffemodel\"\n",
        "\n",
        "deserialized = caffe_pb2.NetParameter()\n",
        "deserialized.ParseFromString(open(caffe_file, 'rb').read())\n",
        "\n",
        "# print(deserialized)\n",
        "# json dump to console\n",
        "json.dump(pb2json(deserialized, \"store_true\"), sys.stdout, indent = 2)\n",
        "\n",
        "# json dump to file\n",
        "with open(\"caffemodel_mnist.json\", \"w\") as f:\n",
        "    json.dump(pb2json(deserialized, \"store_true\"), f, indent = 2)"
      ],
      "metadata": {
        "id": "FMEP4fLMCtlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JSON to RLV converter in Python3"
      ],
      "metadata": {
        "id": "aZ1j-mNVCU2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture cap --no-stderr\n",
        "with open('output.rlv', 'w') as f:\n",
        "    f.write(cap.stdout)\n",
        "\n",
        "import os, sys\n",
        "import json\n",
        "# import operator\n",
        "from functools import reduce\n",
        "from pprint import pprint\n",
        "\n",
        "# Read input file\n",
        "if len(sys.argv) < 2:\n",
        "    print(sys.stderr, \"Error: Expected JSON input file name!\")\n",
        "\n",
        "\"\"\"\n",
        "with open(sys.argv[1]) as data_file:    \n",
        "    data = json.load(data_file)\n",
        "\"\"\"\n",
        "\n",
        "with open(\"caffemodel_mnist.json\") as data_file:    \n",
        "    data = json.load(data_file)\n",
        "\n",
        "data = data[\"layer\"]\n",
        "\n",
        "# Neuron lookup table: layer--->Neuron names\n",
        "neurons = {}\n",
        "\n",
        "\n",
        "# Function for recursively processing layers\n",
        "def recurseProcessLayer(dataLineName,dataLineWidth):\n",
        "    '''Returns the neuron names. DataLineWidth may be unknown.'''\n",
        "\n",
        "    # DAG Caching\n",
        "    if dataLineName in neurons:\n",
        "        return neurons[dataLineName]\n",
        "        \n",
        "    # Search for the producer of this layer\n",
        "    for layer in data:\n",
        "        if dataLineName in layer[\"top\"]:\n",
        "\n",
        "            # This is the layer to be processed\n",
        "            # ---> Proceed according to type.\n",
        "            if layer[\"type\"]==\"Split\":\n",
        "                assert len(layer[\"bottom\"])==1\n",
        "                return recurseProcessLayer(layer[\"bottom\"][0],dataLineWidth)\n",
        "            elif layer[\"type\"]==\"InnerProduct\":\n",
        "                \n",
        "                # Get output dimension\n",
        "                outputLineWidth = layer[\"inner_product_param\"][\"num_output\"]\n",
        "                if dataLineWidth!=None:\n",
        "                    assert outputLineWidth==dataLineWidth\n",
        "                \n",
        "                # Now get input dimension and blob data\n",
        "                inputLineWidth = None\n",
        "                biasWeights = None\n",
        "                inputWeights = None\n",
        "                for blob in layer[\"blobs\"]:\n",
        "                    if len(blob[\"shape\"][\"dim\"])==1:\n",
        "                        biasWeights = blob[\"data\"]\n",
        "                    elif len(blob[\"shape\"][\"dim\"])==2:\n",
        "                        inputLineWidth = blob[\"shape\"][\"dim\"][1]\n",
        "                        inputWeights = blob[\"data\"]\n",
        "                        assert blob[\"shape\"][\"dim\"][0]==outputLineWidth\n",
        "                    else:\n",
        "                        assert False\n",
        "                assert inputLineWidth != None\n",
        "                \n",
        "                # Now get input\n",
        "                assert len(layer[\"bottom\"])==1\n",
        "                inputNeurons = recurseProcessLayer(layer[\"bottom\"][0],inputLineWidth)\n",
        "                \n",
        "                # Produce outputs\n",
        "                outputNeurons = []\n",
        "                for i in range(0,outputLineWidth):\n",
        "                    outputNeurons.append(dataLineName+\"X\"+str(i))\n",
        "                    sys.stdout.write(\"Linear \"+dataLineName+\"X\"+str(i)+\" \"+str(biasWeights[i]))\n",
        "                    for j in range(0,len(inputNeurons)):\n",
        "                        sys.stdout.write(\" \"+str(inputWeights[i*inputLineWidth+j])+\" \"+str(inputNeurons[j]))\n",
        "                    sys.stdout.write(\"\\n\")\n",
        "                \n",
        "                neurons[dataLineName] = outputNeurons\n",
        "                return outputNeurons\n",
        "                \n",
        "                \n",
        "            elif layer[\"type\"]==\"ReLU\":\n",
        "            \n",
        "                # RELU: Get Input\n",
        "                assert len(layer[\"bottom\"])==1\n",
        "                inputNeurons = recurseProcessLayer(layer[\"bottom\"][0],dataLineWidth)\n",
        "                \n",
        "                # Produce outputs\n",
        "                outputNeurons = []\n",
        "                for i in range(0,len(inputNeurons)):\n",
        "                    outputNeurons.append(dataLineName+\"X\"+str(i))\n",
        "                    sys.stdout.write(\"ReLU \"+dataLineName+\"X\"+str(i)+\" 0.0 1.0 \"+str(inputNeurons[i])+\"\\n\")\n",
        "                \n",
        "                neurons[dataLineName] = outputNeurons\n",
        "                return outputNeurons\n",
        "                \n",
        "                \n",
        "                \n",
        "            elif layer[\"type\"]==\"Convolution\":\n",
        "                \n",
        "                # Convolution\n",
        "                \n",
        "                # ---> Load weights and biasses\n",
        "                dimWeights = None\n",
        "                weights = None\n",
        "                biasses = None\n",
        "                for blob in layer[\"blobs\"]:\n",
        "                    size = blob[\"shape\"][\"dim\"]\n",
        "                    params = blob[\"data\"]\n",
        "                    if len(size)==1:\n",
        "                        biasses = params\n",
        "                    else:\n",
        "                        dimWeights = size\n",
        "                        weights = params\n",
        "                        \n",
        "                # ---> Other parameters\n",
        "                #--------> Read Stride\n",
        "                if \"stride\" in layer[\"convolution_param\"]:\n",
        "                    stride = layer[\"convolution_param\"][\"stride\"]\n",
        "                    assert len(stride)==1\n",
        "                    stride = stride + stride\n",
        "                else:\n",
        "                    if \"stride_w\" in layer[\"convolution_param\"]:\n",
        "                        if \"stride_h\" in layer[\"convolution_param\"]:\n",
        "                            stride = [layer[\"convolution_param\"][\"stride_w\"],layer[\"convolution_param\"][\"stride_h\"]]\n",
        "                        else:\n",
        "                            stride = [layer[\"convolution_param\"][\"stride_w\"],1]\n",
        "                    else:\n",
        "                        if \"stride_h\" in layer[\"convolution_param\"]:\n",
        "                            stride = [1,layer[\"convolution_param\"][\"stride_h\"]]\n",
        "                        else:\n",
        "                            stride = [1,1]\n",
        "                \n",
        "                #--------> Read Kernel Size\n",
        "                if \"kernel_size\" in layer[\"convolution_param\"]:\n",
        "                    kernel_size = layer[\"convolution_param\"][\"kernel_size\"]\n",
        "                    assert len(kernel_size)==1\n",
        "                    kernel_size = kernel_size + kernel_size\n",
        "                else:\n",
        "                    kernel_size = [layer[\"convolution_param\"][\"kernel_w\"],layer[\"convolution_param\"][\"kernel_h\"]]\n",
        "\n",
        "                #--------> Read PAD\n",
        "                if \"pad\" in layer[\"convolution_param\"]:\n",
        "                    padding = layer[\"convolution_param\"][\"pad\"]\n",
        "                    assert len(padding)==1\n",
        "                    padding = padding + padding\n",
        "                else:\n",
        "                    if \"pad_w\" in layer[\"convolution_param\"]:\n",
        "                        if \"pad_h\" in layer[\"convolution_param\"]:\n",
        "                            padding = [layer[\"convolution_param\"][\"pad_w\"],layer[\"convolution_param\"][\"pad_h\"]]\n",
        "                        else:\n",
        "                            padding = [layer[\"convolution_param\"][\"pad_w\"],0]\n",
        "                    else:\n",
        "                        if \"pad_h\" in layer[\"convolution_param\"]:\n",
        "                            padding = [0,layer[\"convolution_param\"][\"pad_h\"]]\n",
        "                        else:\n",
        "                            padding = [0,0]\n",
        "\n",
        "                num_output = layer[\"convolution_param\"][\"num_output\"]\n",
        "                num_input_channels = dimWeights[1]\n",
        "                \n",
        "                # Rest is unimplemented for the time being.\n",
        "                # assert dimWeights[0]==1 #---> This is for the *outgoing* num_outputs\n",
        "                # assert dimWeights[1]==1 #----> This is for the incoming colors or features\n",
        "                \n",
        "                # Check for some unsupported features\n",
        "                if \"bias_term\" in layer[\"convolution_param\"]:\n",
        "                    print(sys.stderr, \"Error: Only the default 'bias_term' value is supported for convolution layers\")\n",
        "                    sys.exit(1)\n",
        "\n",
        "                if \"group\" in layer[\"convolution_param\"]:\n",
        "                    print(sys.stderr, \"Error: Only the default 'group' value is supported for convolution layers\")\n",
        "                    sys.exit(1)\n",
        "                \n",
        "                # ---> Read input\n",
        "                inputNeurons = recurseProcessLayer(layer[\"bottom\"][0],None)\n",
        "                \n",
        "                # ---> Unflatten weights\n",
        "                def unflatten(neurons,remainingDimensions):\n",
        "                    if len(remainingDimensions)==1:\n",
        "                        return (neurons[0:remainingDimensions[0]],neurons[remainingDimensions[0]:])\n",
        "                    else:\n",
        "                        res = []\n",
        "                        for a in range(0,remainingDimensions[0]):\n",
        "                            (d,neurons) = unflatten(neurons,remainingDimensions[1:])\n",
        "                            res.append(d)\n",
        "                        return (res,neurons)\n",
        "                \n",
        "                (unflattenedWeights,rest) = unflatten(weights,dimWeights)\n",
        "                assert rest==[]\n",
        "                \n",
        "                # Compute convolution\n",
        "                resultingNeurons = []\n",
        "                for i in range(0,num_output):\n",
        "                    ysize = len(inputNeurons[0])\n",
        "                    xsize = len(inputNeurons[0][0])\n",
        "                    thisBlock = []\n",
        "                    for y in range(-1*padding[1],ysize-kernel_size[1]+1+padding[1],stride[1]):\n",
        "                        thisLine = []                    \n",
        "                        for x in range(-1*padding[0],xsize-kernel_size[0]+1+padding[0],stride[0]):\n",
        "                            thisLine.append(dataLineName+\"X\"+str(i)+\"X\"+str(x)+\"X\"+str(y))\n",
        "                            localInputs = []\n",
        "                            for c in range(0,num_input_channels):\n",
        "                                for b in range(0,kernel_size[1]):\n",
        "                                    for a in range(0,kernel_size[0]):\n",
        "                                        if y+b>=0 and y+b<len(inputNeurons[c]):\n",
        "                                            if x+a>=0 and x+a<len(inputNeurons[c][y+b]):\n",
        "                                                localInputs.append(str(unflattenedWeights[i][c][b][a])+\" \"+inputNeurons[c][y+b][x+a]) \n",
        "                            sys.stdout.write(\"Linear \"+thisLine[-1]+\" \"+str(biasses[i])+\" \"+\" \".join(localInputs)+\"\\n\")\n",
        "                        thisBlock.append(thisLine)\n",
        "                    resultingNeurons.append(thisBlock)\n",
        "                    \n",
        "                return resultingNeurons\n",
        "\n",
        "            elif layer[\"type\"]==\"HDF5Data\":\n",
        "            \n",
        "                # Input layer\n",
        "                assert dataLineWidth!=None\n",
        "                \n",
        "                outputNeurons = []\n",
        "                for i in range(0,dataLineWidth):\n",
        "                    outputNeurons.append(\"inX\"+str(i))\n",
        "                    sys.stdout.write(\"Input inX\"+str(i)+\"\\n\")\n",
        "\n",
        "                neurons[dataLineName] = outputNeurons\n",
        "                return outputNeurons\n",
        "                \n",
        "            elif layer[\"type\"]==\"Data\":\n",
        "            \n",
        "                # Input layer\n",
        "                assert dataLineWidth!=None\n",
        "                \n",
        "                # We assume normalization by a factor of 1/256.0 here.\n",
        "                assert layer['transform_param']['scale'] == 0.00390625\n",
        "\n",
        "                outputNeurons = []\n",
        "                for i in range(0,dataLineWidth):\n",
        "                    outputNeurons.append(\"inX\"+str(i))\n",
        "                    sys.stdout.write(\"Input inX\"+str(i)+\"\\n\")\n",
        "\n",
        "                neurons[dataLineName] = outputNeurons\n",
        "                return outputNeurons\n",
        "                \n",
        "            elif layer[\"type\"]==\"Reshape\":\n",
        "            \n",
        "                # Reshape layer\n",
        "                outputDimension = layer['reshape_param']['shape']['dim']\n",
        "                # print layer\n",
        "                assert outputDimension[0]==-1 # The first dimension is always the sample points\n",
        "                \n",
        "                nofInputs = 1\n",
        "                for a in outputDimension[1:]:\n",
        "                    nofInputs *= a\n",
        "                inputNeurons = recurseProcessLayer(layer[\"bottom\"][0],nofInputs)\n",
        "\n",
        "                \n",
        "                # Ok, first flatten input Neurons\n",
        "                def flat(neurons):\n",
        "                    if type(neurons)==str or type(neurons)==str:\n",
        "                        return [neurons]\n",
        "                    else:\n",
        "                        l = []\n",
        "                        for a in neurons:\n",
        "                            l.extend(flat(a))\n",
        "                        return l\n",
        "                \n",
        "                # Ok, first flatten input Neurons\n",
        "                #def flat(neurons):\n",
        "                #    if type(neurons)==str or type(neurons)==unicode or type(neurons)==int:\n",
        "                #        return [neurons]\n",
        "                #    else:\n",
        "                #        l = []\n",
        "                #        for a in neurons:\n",
        "                #            l.append(flat(a))\n",
        "                #        return [a for j in zip(*l) for a in j ]\n",
        "\n",
        "                flattenedNeurons = flat(inputNeurons)\n",
        "                \n",
        "                # Ok, now reshape\n",
        "                def unflatten(neurons,remainingDimensions):\n",
        "                    if len(remainingDimensions)==1:\n",
        "                        return (neurons[0:remainingDimensions[0]],neurons[remainingDimensions[0]:])\n",
        "                    else:\n",
        "                        res = []\n",
        "                        for a in range(0,remainingDimensions[0]):\n",
        "                            (d,neurons) = unflatten(neurons,remainingDimensions[1:])\n",
        "                            res.append(d)\n",
        "                        return (res,neurons)\n",
        "                \n",
        "                # Ok, now reshape\n",
        "                #def unflatten(neurons,selection,dimensions):\n",
        "                #    if len(selection)==len(dimensions):\n",
        "                #        index = 0\n",
        "                #        factor = 1\n",
        "                #        for i in range(0,len(selection)):\n",
        "                #            index += factor*selection[i]\n",
        "                #            factor *= dimensions[i]\n",
        "                #        return neurons[index]\n",
        "                #    else:\n",
        "                #        res = []\n",
        "                #        for a in range(0,dimensions[len(selection)]):\n",
        "                #            res.append(unflatten(neurons,selection+[a],dimensions))\n",
        "                #        return res\n",
        "\n",
        "                \n",
        "                (unflattenedNeurons,rest) = unflatten(flattenedNeurons,outputDimension[1:])\n",
        "                assert rest==[]\n",
        "                # print outputDimension\n",
        "                # print inputNeurons\n",
        "                # print flattenedNeurons\n",
        "                # print \"-----UF:->\",unflattenedNeurons\n",
        "                assert len(flattenedNeurons)==reduce(operator.mul, outputDimension[1:], 1)\n",
        "                              \n",
        "                return unflattenedNeurons\n",
        "\n",
        "            elif layer[\"type\"]==\"Pooling\":\n",
        "            \n",
        "                # Reshape layer\n",
        "                inputNeurons = recurseProcessLayer(layer[\"bottom\"][0],None)\n",
        "                \n",
        "                assert layer[\"pooling_param\"][\"pool\"]==0 # Must be a MAXPOOL (for the time being)\n",
        "                \n",
        "                # ---> Other parameters\n",
        "                #--------> Read Stride\n",
        "                if \"stride\" in layer[\"pooling_param\"]:\n",
        "                    # Why is the \"stride\" here an int, but for the Convolution layer is a list?\n",
        "                    stride = layer[\"pooling_param\"][\"stride\"]\n",
        "                    if isinstance(stride,list):\n",
        "                        assert len(stride)==1\n",
        "                        stride = stride + stride\n",
        "                    else:\n",
        "                        stride = [stride,stride]\n",
        "                else:\n",
        "                    if \"stride_w\" in layer[\"pooling_param\"]:\n",
        "                        if \"stride_h\" in layer[\"pooling_param\"]:\n",
        "                            stride = [layer[\"pooling_param\"][\"stride_w\"],layer[\"pooling_param\"][\"stride_h\"]]\n",
        "                        else:\n",
        "                            stride = [layer[\"pooling_param\"][\"stride_w\"],1]\n",
        "                    else:\n",
        "                        if \"stride_h\" in layer[\"pooling_param\"]:\n",
        "                            stride = [1,layer[\"pooling_param\"][\"stride_h\"]]\n",
        "                        else:\n",
        "                            stride = [1,1]\n",
        "                \n",
        "                #--------> Read Kernel Size\n",
        "                if \"kernel_size\" in layer[\"pooling_param\"]:\n",
        "                    # Why is the \"kernel_size\" here an int, but for the Convolution layer is a list?\n",
        "                    kernel_size = layer[\"pooling_param\"][\"kernel_size\"]\n",
        "                    if isinstance(kernel_size,list):\n",
        "                        assert len(kernel_size)==1\n",
        "                        kernel_size = kernel_size + kernel_size\n",
        "                    else:\n",
        "                        kernel_size = [kernel_size,kernel_size]\n",
        "                else:\n",
        "                    kernel_size = [layer[\"pooling_param\"][\"kernel_w\"],layer[\"pooling_param\"][\"kernel_h\"]]\n",
        "\n",
        "                #--------> Read PAD\n",
        "                if \"pad\" in layer[\"pooling_param\"]:\n",
        "                    padding = layer[\"pooling_param\"][\"pad\"]\n",
        "                    assert len(padding)==1\n",
        "                    padding = padding + padding\n",
        "                else:\n",
        "                    if \"pad_w\" in layer[\"pooling_param\"]:\n",
        "                        if \"pad_h\" in layer[\"pooling_param\"]:\n",
        "                            padding = [layer[\"pooling_param\"][\"pad_w\"],layer[\"pooling_param\"][\"pad_h\"]]\n",
        "                        else:\n",
        "                            padding = [layer[\"pooling_param\"][\"pad_w\"],0]\n",
        "                    else:\n",
        "                        if \"pad_h\" in layer[\"pooling_param\"]:\n",
        "                            padding = [0,layer[\"pooling_param\"][\"pad_h\"]]\n",
        "                        else:\n",
        "                            padding = [0,0]\n",
        "\n",
        "                \n",
        "                # Here, we assume that the \"inputNeurons\" array is three-dimensional:\n",
        "                # - color channel\n",
        "                # - X channel\n",
        "                # - Y channel\n",
        "                resultingNeurons = []\n",
        "                for i,channel in enumerate(inputNeurons):\n",
        "                    ysize = len(channel)\n",
        "                    xsize = len(channel[0])\n",
        "                    thisBlock = []\n",
        "                    for y in range(-1*padding[1],ysize-kernel_size[1]+1+padding[1],stride[1]):\n",
        "                        thisLine = []                    \n",
        "                        for x in range(-1*padding[0],xsize-kernel_size[0]+1+padding[0],stride[0]):\n",
        "                            thisLine.append(dataLineName+\"X\"+str(i)+\"X\"+str(x)+\"X\"+str(y))\n",
        "                            localInputs = []\n",
        "                            for b in range(0,kernel_size[1]):\n",
        "                                for a in range(0,kernel_size[0]):\n",
        "                                    if y+b>=0 and y+b<len(channel):\n",
        "                                        if x+a>=0 and x+a<len(channel[y+b]):\n",
        "                                            localInputs.append(channel[y+b][x+a]) \n",
        "                            sys.stdout.write(\"MaxPool \"+thisLine[-1]+\" \"+\" \".join(localInputs)+\"\\n\")\n",
        "                        thisBlock.append(thisLine)\n",
        "                    resultingNeurons.append(thisBlock)\n",
        "                return resultingNeurons                    \n",
        "                    \n",
        "            else:\n",
        "                raise RuntimeError(\"Unsupported Layer Type: \"+layer[\"type\"])\n",
        "            \n",
        "    raise \"Error: Data Line \"+dataLineName+\" not found.\"\n",
        "\n",
        "\n",
        "# Process the Accuracy layer\n",
        "foundAccurracyLayer = False\n",
        "for layer in data:\n",
        "    if layer['type'] == 'Accuracy':\n",
        "        foundAccurracyLayer = True\n",
        "        outputs = recurseProcessLayer(layer[\"bottom\"][0],None)\n",
        "        for i in range(0,len(outputs)):\n",
        "            sys.stdout.write(\"Linear outX\"+str(i)+\" 0.0 1.0 \"+outputs[i]+\"\\n\")\n",
        "                \n",
        "if not foundAccurracyLayer:\n",
        "    print(sys.stderr, \"Warning: No 'Accuracy' layer found, hence nothing was translated.\")"
      ],
      "metadata": {
        "id": "VhZoZYycCQSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run planet with your RLV file"
      ],
      "metadata": {
        "id": "i1cmxBBICojK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add contraints on Input Variables for Planet\n",
        "%cd /content/\n",
        "\n",
        "with open(\"output.rlv\", \"ab\") as f:\n",
        "  for i in range(28*28):\n",
        "    linebreak = bytes(\"\\n\", \"utf-8\")\n",
        "    assert_lowerbound = bytes(\"Assert <= 0.0 1.0 inX\" + str(i), \"utf-8\")\n",
        "    assert_upperbound = bytes(\"Assert >= 1.0 1.0 inX\" + str(i), \"utf-8\")\n",
        "\n",
        "    f.write(linebreak)\n",
        "    f.write(assert_lowerbound)\n",
        "    f.write(linebreak)\n",
        "    f.write(assert_upperbound)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V2ezS_Dwy9V",
        "outputId": "0a764895-cced-48e7-f89b-a0c8fa9a9182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verify Robustness with Planet"
      ],
      "metadata": {
        "id": "prLQbKiridL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-mnist"
      ],
      "metadata": {
        "id": "l-Lee6NzpFFq",
        "outputId": "1c81fb88-77f1-43e3-b1cb-8cb6281c8898",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-mnist\n",
            "  Downloading python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: python-mnist\n",
            "Successfully installed python-mnist-0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mnist import MNIST\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "data = MNIST(\"/content/caffe/data/mnist/\")\n",
        "\n",
        "imgs, labels = data.load_training()\n",
        "\n",
        "img = np.asarray(imgs[0]).reshape(28, 28)\n",
        "\n",
        "plt.title(\"Label: {}\".format(labels[0]))\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "cB0NIuWMiaz6",
        "outputId": "f8f59b5a-fc74-4a9b-b09b-6fabebb56f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f90c5e128d0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.05\n",
        "\n",
        "def addBounds(i, l_bound, u_bound):\n",
        "  with open(\"/content/output.rlv\", \"ab\") as f:\n",
        "    linebreak = bytes(\"\\n\", \"utf-8\")\n",
        "    assert_lowerbound = bytes(\"Assert <= {} 1.0 inX{}\".format(l_bound, i), \"utf-8\")\n",
        "    assert_upperbound = bytes(\"Assert >= {} 1.0 inX{}\".format(u_bound, i), \"utf-8\")\n",
        "\n",
        "    f.write(linebreak)\n",
        "    f.write(assert_lowerbound)\n",
        "    f.write(linebreak)\n",
        "    f.write(assert_upperbound)\n",
        "\n",
        "counter = 0\n",
        "for h in range(img.shape[0]):\n",
        "  for w in range(img.shape[1]):\n",
        "    lower_bound = img[h][w] - epsilon\n",
        "    upper_bound = img[h][w] + epsilon\n",
        "    addBounds(counter, lower_bound, upper_bound)\n",
        "    counter += 1\n"
      ],
      "metadata": {
        "id": "2fteFaDxrWRR"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i \"/Assert/d\" /content/output.rlv\n",
        "!sed -i \"/^$/d\" /content/output.rlv "
      ],
      "metadata": {
        "id": "nVLvoMLsvtqv"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/content/planet/src/planet /content/caffemodel_mnist.rlv\n",
        "!/content/planet/src/planet /content/output.rlv"
      ],
      "metadata": {
        "id": "X1jmu4Pgw--g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Current TODO:\n",
        "The Output Variables are not constrained, we probably need to do that next!"
      ],
      "metadata": {
        "id": "erMO_H9_yauq"
      }
    }
  ]
}