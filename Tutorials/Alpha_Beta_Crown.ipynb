{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alpha-Beta-Crown.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DDiekmann/Applied-Verification-Lab-Neural-Networks/blob/main/Tutorials/Alpha_Beta_Crown.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial for α,β-CROWN Robustness Verification \n",
        "\n"
      ],
      "metadata": {
        "id": "aYgyJk_R5281"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*This tutorial shows the robustness verification of a neural network trained on \n",
        "the MNIST dataset with use of α,β-CROWN.*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**This is a interactive tutorial. So feel free to experiment and change stuff. In every spot we think it would be intersting to change some stuff around we put this little icon.**\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Edit_icon_%28the_Noun_Project_30184%29.svg/1024px-Edit_icon_%28the_Noun_Project_30184%29.svg.png\" alt=\"drawing\" width=\"50\"/>\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "rz2u1ieA8NBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Why do we need to verify neural networks? \n",
        "Nowadays, neural networks are used in safety-critical areas, among others. As a result, there is a need to formally prove some of the network's behaviours (especially under malicious inputs). \\\\\n",
        "One of those behaviours is the so-called **Robustness** of the network. Robustness means that small perturbations to the inputs should not lead to changes in the output of the neural network. \\\\\n",
        "![img](https://openai.com/content/images/2017/02/adversarial_img_1.png) \\\\\n",
        "The illustration shows, for example, image recognition for an animal. On the left side, the neural network recognises a panda with a probability of 57.7%. By adding noise, a gibbon is recognised in the following with a probability of 99.3%. In this example, this wrong decision is probably not safety-critical, but other use cases (for example in the field of autonomous driving) are conceivable where such a wrong decision could have serious consequences.\n"
      ],
      "metadata": {
        "id": "0MWlEEUHV5YG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Basics on α,β-CROWN"
      ],
      "metadata": {
        "id": "BA4rvbBQSZjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic idea behind α,β-CROWN is to use efficient bound propagation for verification tasks based on *Automatic Linear Relaxation based Perturbation Analysis for Neural Networks* (LiRPA). The [code](https://github.com/KaidiXu/auto_LiRPA) of LiRPA can be found on github. \\\\\n",
        "In the picture below one can see the concept of LiRPA. The input to a neural network or any general computations consists of data or model weights under perturbations. That refers to the image with the panda above. We want to allow pictures as input that are slightly different, but then then the network should predict the same classification output. \n",
        "![img](https://camo.githubusercontent.com/86e55db4461fc54896a582f0c275c1d4f02f2ffd9159d3c6e9d4fcfa9744d854/687474703a2f2f7777772e6875616e2d7a68616e672e636f6d2f696d616765732f75706c6f61642f6c697270612f6175746f5f4c695250415f6261636b67726f756e645f332e706e67) \\\\\n",
        "The output of LiRPA are a set of output bounds, which are the guaranteed score ranges for different classification outputs of the neural network. "
      ],
      "metadata": {
        "id": "V088bP8TdQCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complete verification** In a complete verification setting the verifier should say \"yes\" or \"no\" to a given property of a network. \\\\\n",
        "**α,β-CROWN**\n",
        "α,β-CROWN is an open-source neural network verifier based on an efficient bound propagation algorithm and branch and bound. The code can be found on [their website](https://github.com/huanzhang12/alpha-beta-CROWN). \\\\\n",
        "**CROWN** is a general framework to certify robustness of\n",
        "neural networks with general activation functions for given input data points. The algorithm can be used for certifying NNs using linear or quadratic\n",
        "upper and lower bounds for general activation functions that are not necessarily piece-wise linear.[See their paper for more information](https://arxiv.org/pdf/1811.00866.pdf). \\\\\n",
        "**β-CROWN** is a \"new bound propagation based method that can fully encode neuron splits via optimizable parameters\n",
        "β constructed from either primal or dual space\". [See the paper of β-CROWN for more information](https://arxiv.org/pdf/2103.06624.pdf). \\\\\n",
        "**α-CROWN** is for incomplete verification with optimized CROWN bound. [See the paper of α-CROWN for more information](https://arxiv.org/pdf/2011.13824.pdf). \\\\\n"
      ],
      "metadata": {
        "id": "sOpfUe1C8xBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial we will first go through the installation of α,β-CROWN. Then we will see some of the features and configuration options of α,β-CROWN."
      ],
      "metadata": {
        "id": "LfF1cvIX70Kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "n0Quth1B5Wbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This installation is based on another [tutorial](https://colab.research.google.com/drive/1mJTOmq2qHxMycHUzBepBN47QWcxda3ov#scrollTo=Y0toepwVIFTG). "
      ],
      "metadata": {
        "id": "Y8neFi6n5da9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will set up our miniconda environment. "
      ],
      "metadata": {
        "id": "A2WnHH7D8QdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "%env PYTHONPATH=\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "metadata": {
        "id": "JV2OjHuS32CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python in version 3.7 is installed into the environment. "
      ],
      "metadata": {
        "id": "mu9YAbJS8WHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "conda install --channel defaults conda python=3.7 --yes\n",
        "conda update --channel defaults --all --yes"
      ],
      "metadata": {
        "id": "2r20b2Wg36Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import sys\n",
        "sys.path\n",
        "!ls /usr/local/lib/python3.7/dist-packages\n",
        "_ = (sys.path\n",
        "        .append(\"/usr/local/lib/python3.7/site-packages\"))"
      ],
      "metadata": {
        "id": "iVia-nO53826"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use the library, we have to clone the corresponding git-repository."
      ],
      "metadata": {
        "id": "clVssA098eAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Uninstall existing Pytorch on Colab, which might be incompatible or buggy.\n",
        "# Note that the alpha beta crown library is tested on Pytorch 1.8.2 LTS, and other versions might be incompatible.(according to reference tutorial)\n",
        "# !pip uninstall --yes torch torchvision torchaudio torchtext\n",
        "!git clone https://github.com/huanzhang12/alpha-beta-CROWN.git"
      ],
      "metadata": {
        "id": "R3m60hqD3_sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The environment is created. "
      ],
      "metadata": {
        "id": "3mZvUGSX8rAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "# Remove the old environment, if necessary.\n",
        "conda env remove --name alpha-beta-crown\n",
        "conda env create -f alpha-beta-CROWN/complete_verifier/environment.yml  # install all dependents into the alpha-beta-crown environment"
      ],
      "metadata": {
        "id": "UThlPEBk4Ckw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd alpha-beta-CROWN/complete_verifier/"
      ],
      "metadata": {
        "id": "ssVpobxp4GQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As explained on [the website of the α,β-project](https://github.com/huanzhang12/alpha-beta-CROWN), it is nessasary to create a configuration file in order to load the data. "
      ],
      "metadata": {
        "id": "pgl_rkFY8uvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure verification"
      ],
      "metadata": {
        "id": "LK3JuZPt4-mC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to verifiy a neural network. Therefore, we create a file called my_example_config.yaml which configures parameters for verification. \n",
        "The model is defined as followed: \n",
        "\n",
        "\n",
        "```\n",
        "def mnist_6_100():\n",
        "    model = nn.Sequential(\n",
        "        Flatten(),\n",
        "        nn.Linear(784,100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100,100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100,100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100,100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100,100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100, 10)\n",
        "    )\n",
        "    return model\n",
        "```\n",
        "\n",
        "It contains six linear layers and uses the ReLU activation function."
      ],
      "metadata": {
        "id": "Z8U-YzCS9yAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start with a very basic configuration file, which means that most parameters are set to default.\n",
        "The network is trained to classify input from the MNIST dataset. The MNIST dataset consists of images of handwritten digits as shown below. \\\\\n",
        "![img](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/320px-MnistExamples.png) \\\\\n",
        "If we now have one image from the dataset (for example a 5 shown on the left in the picture below), we want the network to classify a slightly changed image (the 7 on the right) to also be classified as a 5 (and not, as in this case, as a 3). \\\\\n",
        "![img](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRf-yaAECy1V4PR9Po2o8cV7BOqug_-SODrtg&usqp=CAU) "
      ],
      "metadata": {
        "id": "fekbamk9Dm82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exp_configs/my_example_config.yaml\n",
        "general:\n",
        "  # device to run verifier\n",
        "  device: cpu \n",
        "  # Complete verification verifier. \n",
        "  # \"bab\": branch and bound with beta-CROWN; \n",
        "  # \"mip\": mixed integer programming (MIP) formulation; \n",
        "  # \"bab-refine\": branch and bound with intermediate layer bounds computed by MIP.\n",
        "  complete_verifier: bab\n",
        "model:\n",
        "  # name of the model (provided by library, see above)\n",
        "  name: mnist_6_100\n",
        "  # Load pretrained model from this specified path.\n",
        "  path: models/eran/mnist_6_100_nat.pth\n",
        "data:\n",
        "  # Dataset name. Dataset must be defined in utils.py.\n",
        "  dataset: MNIST_ERAN_UN\n",
        "  # Std vector used in data preprocessing.\n",
        "  std: [1.0]\n",
        "  # Mean vector used in data preprocessing.\n",
        "  mean: [0.0]\n",
        "specification:\n",
        "  # Set perturbation size (Lp norm). \n",
        "  # If not set, a default value may be used based on dataset loader.\n",
        "  epsilon: 0.026\n",
        "solver:\n",
        "  alpha-crown:\n",
        "    # Number of iterations for alpha-CROWN incomplete verifier.\n",
        "    iteration: 10\n",
        "attack:\n",
        "  # Early stop PGD when an adversarial example is found.\n",
        "  pgd_early_stop: true  "
      ],
      "metadata": {
        "id": "s1E0j7dbVvGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verification of network with α,β-CROWN"
      ],
      "metadata": {
        "id": "ivgGYPZUMShY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we only have to run our verification. "
      ],
      "metadata": {
        "id": "y-uwErjZ-T4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we activate our environment.\n",
        "Then we call the robustness_verifier on our configured yaml-file. The robustness_verifier is a class provided by the α,β-CROWN-library for Lp norm robustness verification and is often used to certify the robustness of a neural network. \n",
        "By setting start to 0 and end to 3, we indicate that only images 0 to 3 from the dataset should be verified. This is done for performance reasons. \n",
        "We finish by deactivating the environment.\n"
      ],
      "metadata": {
        "id": "7mM9hCLY-bQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run.sh\n",
        "\n",
        "source activate alpha-beta-crown\n",
        "python robustness_verifier.py --config exp_configs/my_example_config.yaml --start 0 --end 3\n",
        "conda deactivate"
      ],
      "metadata": {
        "id": "1wOCa05uMept"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 run.sh\n",
        "!./run.sh"
      ],
      "metadata": {
        "id": "di2IgirTVxO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try configuration options"
      ],
      "metadata": {
        "id": "-3EjCYapS5Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "α,β-CROWN provides a number of different parameters that can be used for verification. An overview is given here: \n",
        "\n",
        "\n",
        "```\n",
        "general:\n",
        "  device: cuda  # Select device to run verifier, cpu or cuda (GPU).\n",
        "  seed: 100  # Random seed.\n",
        "  conv_mode: patches  # Convolution mode during bound propagation: \"patches\" mode (default) is very efficient, but may not support all architecture; \"matrix\" mode is slow but supports all architectures.\n",
        "  deterministic: false  # Run code in CUDA deterministic mode, which has slower performance but better reproducibility.\n",
        "  double_fp: false  # Use double precision floating point. GPUs with good double precision support are preferable (NVIDIA P100, V100, A100; AMD Radeon Instinc MI50, MI100).\n",
        "  loss_reduction_func: sum  # When batch size is not 1, this reduction function is applied to reduce the bounds into a single number (options are \"sum\" and \"min\").\n",
        "  mode: verified-acc  # Verify against all labels (\"verified-acc\" mode), or just the runnerup labels (\"runnerup\" mode), or using a specified label in dataset (\"speicify-target\" mode, only used for oval20). Mode can also be set as \"crown-only-verified-acc\" or \"alpha-crown-only-verified-acc\", which quickly computes the verified accuracy over the entire dataset via CROWN or alpha-CROWN.\n",
        "  complete_verifier: bab  # Complete verification verifier. \"bab\": branch and bound with beta-CROWN; \"mip\": mixed integer programming (MIP) formulation; \"bab-refine\": branch and bound with intermediate layer bounds computed by MIP.\n",
        "  enable_incomplete_verification: true  # Enable/Disable initial alpha-CROWN incomplete verification (this can save GPU memory when disabled).\n",
        "model:\n",
        "  path: null  # Load pretrained model from this specified path.\n",
        "  name: please_specify_model_name  # Name of model. Model must be defined in the load_verification_dataset() function in utils.py.\n",
        "data:\n",
        "  start: 0  # Start from the i-th property in specified dataset.\n",
        "  end: 10000  # End with the (i-1)-th property in the dataset.\n",
        "  num_outputs: 10  # Number of classes for classification problem.\n",
        "  mean: 0.0  # Mean vector used in data preprocessing.\n",
        "  std: 1.0  # Std vector used in data preprocessing.\n",
        "  pkl_path: null  # Load properties to verify from a .pkl file (only used for oval20 dataset).\n",
        "  dataset: CIFAR  # Dataset name. Dataset must be defined in utils.py.\n",
        "  data_idx_file: null  # A text file with a list of example IDs to run.\n",
        "specification:\n",
        "  type: lp  # Type of verification specification. \"lp\" = L_p norm, \"bounds\" = element-wise lower and upper bound provided by dataloader.\n",
        "  norm: .inf  # Lp-norm for epsilon perturbation in robustness verification (1, 2, inf).\n",
        "  epsilon: null  # Set perturbation size (Lp norm). If not set, a default value may be used based on dataset loader.\n",
        "solver:\n",
        "  alpha-crown:\n",
        "    lr_alpha: 0.1  # Learning rate for the optimizable parameter alpha in alpha-CROWN bound.\n",
        "    iteration: 100  # Number of iterations for alpha-CROWN incomplete verifier.\n",
        "    share_slopes: false  # Share some alpha variables to save memory at the cost of slightly looser bounds.\n",
        "    no_joint_opt: false  # Run alpha-CROWN bounds without joint optimization (only optimize alpha for the last layer bound).\n",
        "  beta-crown:\n",
        "    batch_size: 64  # Batch size in beta-CROWN (number of parallel splits).\n",
        "    lr_alpha: 0.01  # Learning rate for optimizing alpha during branch and bound.\n",
        "    lr_beta: 0.05  # Learning rate for optimizing beta during branch and bound.\n",
        "    lr_decay: 0.98  # Learning rate decay factor during optimization. Need to use a larger value like 0.99 or 0.995 when you increase the number of iterations.\n",
        "    optimizer: adam  # Optimizer used for alpha and beta optimization.\n",
        "    iteration: 50  # Number of iteration for optimizing alpha and beta during branch and bound.\n",
        "  mip:\n",
        "    parallel_solvers: null  # Number of multi-processes for mip solver. Each process computes a mip bound for an intermediate neuron. Default (None) is to auto detect the number of CPU cores (note that each process may use multiple threads, see the next option).\n",
        "    solver_threads: 1  # Number of threads for echo mip solver process (default is to use 1 thread for each solver process).\n",
        "    refine_neuron_timeout: 15  # MIP timeout threshold for improving each intermediate layer bound (in seconds).\n",
        "    refine_neuron_time_percentage: 0.8  # Percentage (x100%) of time used for improving all intermediate layer bounds using mip. Default to be 0.8*timeout.\n",
        "bab:\n",
        "  max_domains: 200000  # Max number of subproblems in branch and bound.\n",
        "  decision_thresh: 0  # Decision threshold of lower bounds. When lower bounds are greater than this value, verification is successful. Set to 0 for robustness verification.\n",
        "  timeout: 360  # Timeout (in second) for verifying one image/property.\n",
        "  branching:\n",
        "    method: kfsb  # Branching heuristic. babsr is fast but less accurate; fsb is slow but most accurate; kfsb is usualy a balance.\n",
        "    candidates: 3  # Number of candidates to consider when using fsb or kfsb. More leads to slower but better branching.\n",
        "    reduceop: min  # Reduction operation to compute branching scores from two sides of a branch (min or max). max can work better on some models.\n",
        "attack:\n",
        "  pgd_order: before  # Run PGD before/after incomplete verification, or skip it.\n",
        "  enable_mip_attack: false  # Use MIP (Gurobi) based attack if PGD cannot find a successful adversarial example.\n",
        "  pgd_steps: 100  # Steps of PGD attack.\n",
        "  pgd_restarts: 30  # Number of random PGD restarts.\n",
        "  pgd_early_stop: true  # Early stop PGD when an adversarial example is found.\n",
        "  pgd_lr_decay: 0.99  # Learning rate decay factor used in PGD attack.\n",
        "  pgd_alpha: auto  # Step size of PGD attack. Default (auto) is epsilon/4.\n",
        "  ```\n",
        "\n"
      ],
      "metadata": {
        "id": "qnX_9C1RS9wD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Try out another epsilon\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Edit_icon_%28the_Noun_Project_30184%29.svg/1024px-Edit_icon_%28the_Noun_Project_30184%29.svg.png\" alt=\"drawing\" width=\"50\"/>\n",
        "\n",
        "Now it's up to you! Try different configuration options and see how it influences the result! \\\\\n",
        "For example: What do you think changes with a changed value for Epsilon? \n",
        "(Expand the cell in order to see the resulting configuration file)"
      ],
      "metadata": {
        "id": "KV5doog1Tpr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exp_configs/my_example_config.yaml\n",
        "general:\n",
        "  # device to run verifier\n",
        "  device: cpu \n",
        "  # Complete verification verifier. \n",
        "  # \"bab\": branch and bound with beta-CROWN; \n",
        "  # \"mip\": mixed integer programming (MIP) formulation; \n",
        "  # \"bab-refine\": branch and bound with intermediate layer bounds computed by MIP.\n",
        "  complete_verifier: bab\n",
        "model:\n",
        "  # name of the model (provided by library, see above)\n",
        "  name: mnist_6_100\n",
        "  # Load pretrained model from this specified path.\n",
        "  path: models/eran/mnist_6_100_nat.pth\n",
        "data:\n",
        "  # Dataset name. Dataset must be defined in utils.py.\n",
        "  dataset: MNIST_ERAN_UN\n",
        "  # Std vector used in data preprocessing.\n",
        "  std: [1.0]\n",
        "  # Mean vector used in data preprocessing.\n",
        "  mean: [0.0]\n",
        "specification:\n",
        "  # Set perturbation size (Lp norm). \n",
        "  # If not set, a default value may be used based on dataset loader.\n",
        "  epsilon: 1\n",
        "solver:\n",
        "  alpha-crown:\n",
        "    # Number of iterations for alpha-CROWN incomplete verifier.\n",
        "    iteration: 10  \n",
        "attack:\n",
        "  # Early stop PGD when an adversarial example is found.\n",
        "  pgd_early_stop: true   "
      ],
      "metadata": {
        "id": "9WLtxtU5Vjtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run your new configuration file\n",
        "So, what do you expect with your new epsilon? "
      ],
      "metadata": {
        "id": "xCvUzLBQxwAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./run.sh"
      ],
      "metadata": {
        "id": "S0kp0BGiVwHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the larger epsilon makes the attack much more likely to succeed. This is because a larger epsilon allows a greater change in the original image."
      ],
      "metadata": {
        "id": "tzSYWTR4x5yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Different norms\n",
        "α,β-CROWN offers the possibility to use different norms for the verification of the networks. \n",
        "###L1 Norm\n",
        "The L1 norm is also known as Manhattan Distance or Taxicab norm. The L1 norm for a vector $x$ is calculated by $||x||_1$=  $\\sum\\nolimits_{i=1}^n |x_i|$. In the case of the plot below it is $||a||_1$=  $|a_1| + |a_2|$ = 3 + 4 = 7. \n",
        "\n"
      ],
      "metadata": {
        "id": "phDUvqNO-Ebl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([3], [4], 'ro')\n",
        "plt.axis([0, 4, 0, 5])\n",
        "plt.plot([3, 0], [4, 4],color=\"red\")\n",
        "plt.plot([3, 3], [0, 4],color=\"red\")\n",
        "plt.annotate('$a$ with $a_1=3$ and $a_2=4$', xy=(3, 4), xytext=(2, 3),\n",
        "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "             )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SNLodaEtGLMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###L2 Norm\n",
        "The L2 norm is also known as the Euclidean norm. It is defined as $||x||_2 = \\sqrt{\\sum\\nolimits_{i=1}^n |x_i|^2}$."
      ],
      "metadata": {
        "id": "cF4O13mvG_Rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([3], [4], 'ro')\n",
        "plt.axis([0, 4, 0, 5])\n",
        "plt.plot([0, 3], [0, 4],color=\"red\")\n",
        "plt.annotate('$a$ with $a_1=3$ and $a_2=4$', xy=(3, 4), xytext=(2, 3),\n",
        "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "             )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b9oA2plLJnO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###L3 Norm\n",
        "The L3 norm oder maximum norm corresponds to the magnitude of the largest component of the vector.\n",
        "It is defined as $||x||_{\\inf} = max_{i=1,..,n}|x_i|$."
      ],
      "metadata": {
        "id": "DQXXfLUiJvXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Different norms in verification\n",
        "\n",
        "As adversarial robustness around a fixed input $\\overrightarrow{y*}$ is defined such that the distance between  $\\overrightarrow{y*}$ and any $\\overrightarrow{x}$ less than or equal to an epsilon always produces the same prediction. The distance here is calculated using one of the vector norms. "
      ],
      "metadata": {
        "id": "IT3-xYpcMv05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Now it's your turn\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Edit_icon_%28the_Noun_Project_30184%29.svg/1024px-Edit_icon_%28the_Noun_Project_30184%29.svg.png\" alt=\"drawing\" width=\"50\"/>\n",
        "\n",
        "Play around with different norms in your configuration file and run the verification."
      ],
      "metadata": {
        "id": "5uYVKGn4OSB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exp_configs/my_example_config.yaml\n",
        "general:\n",
        "  device: cpu \n",
        "  complete_verifier: bab\n",
        "model:\n",
        "  name: mnist_6_100\n",
        "  path: models/eran/mnist_6_100_nat.pth\n",
        "data:\n",
        "  dataset: MNIST_ERAN_UN\n",
        "  std: [1.0]\n",
        "  mean: [0.0]\n",
        "specification:\n",
        "  ###################\n",
        "  # inserted here:\n",
        "  norm: .inf  \n",
        "  # norm: 1\n",
        "  # norm: 2\n",
        "  ###################\n",
        "  epsilon: 0.026\n",
        "solver:\n",
        "  alpha-crown:\n",
        "    iteration: 10\n",
        "attack:\n",
        "  pgd_early_stop: true  "
      ],
      "metadata": {
        "id": "HJZQ6nrjO3HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 run.sh\n",
        "!./run.sh"
      ],
      "metadata": {
        "id": "UYP3_a4vOsWu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}