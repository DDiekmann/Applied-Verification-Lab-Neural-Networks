{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alpha-Beta-Crown.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DDiekmann/Applied-Verification-Lab-Neural-Networks/blob/main/Tutorials/Alpha_Beta_Crown.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial for α,β-CROWN Robustness Verification \n",
        "\n"
      ],
      "metadata": {
        "id": "aYgyJk_R5281"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*This tutorial shows the robustness verification of a neural network trained on \n",
        "the MNIST dataset with use of α,β-CROWN.*\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rz2u1ieA8NBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Why do we need to verify neural networks? \n",
        "Nowadays, neural networks are used in safety-critical areas, among others. As a result, there is a need to formally prove some of the network's behaviours (especially under malicious inputs). \\\\\n",
        "One of those behaviours is the so-called **Robustness** of the network. Robustness means that small perturbations to the inputs should not lead to changes in the output of the neural network. \\\\\n",
        "![img](https://openai.com/content/images/2017/02/adversarial_img_1.png) \\\\\n",
        "The illustration shows, for example, image recognition for an animal. On the left side, the neural network recognises a panda with a probability of 57.7%. By adding noise, a gibbon is recognised in the following with a probability of 99.3%. In this example, this wrong decision is probably not safety-critical, but other use cases (for example in the field of autonomous driving) are conceivable where such a wrong decision could have serious consequences.\n"
      ],
      "metadata": {
        "id": "0MWlEEUHV5YG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Basics on α,β-CROWN"
      ],
      "metadata": {
        "id": "BA4rvbBQSZjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**α,β-CROWN**\n",
        "α,β-CROWN is an open-source neural network verifier based on an efficient bound propagation algorithm and branch and bound. The code can be found on [their website](https://github.com/huanzhang12/alpha-beta-CROWN). \\\\\n",
        "**CROWN** is a general framework to certify robustness of\n",
        "neural networks with general activation functions for given input data points. The algorithm can be used for certifying NNs using linear or quadratic\n",
        "upper and lower bounds for general activation functions that are not necessarily piece-wise linear.[See their paper for more information](https://arxiv.org/pdf/1811.00866.pdf). \\\\\n",
        "**β-CROWN** is a \"new bound propagation based method that can fully encode neuron splits via optimizable parameters\n",
        "β constructed from either primal or dual space\". \\\\\n",
        "**α-CROWN**  \\\\\n",
        "**Complete verification**"
      ],
      "metadata": {
        "id": "sOpfUe1C8xBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial we will first go through the installation of α,β-CROWN and train our own network. This network is to be verified later in the tutorial. Therefore, the verfication must be configured first. "
      ],
      "metadata": {
        "id": "LfF1cvIX70Kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "n0Quth1B5Wbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This installation is based on another [tutorial](https://colab.research.google.com/drive/1mJTOmq2qHxMycHUzBepBN47QWcxda3ov#scrollTo=Y0toepwVIFTG). "
      ],
      "metadata": {
        "id": "Y8neFi6n5da9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will set up our miniconda environment. "
      ],
      "metadata": {
        "id": "A2WnHH7D8QdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "%env PYTHONPATH=\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "metadata": {
        "id": "JV2OjHuS32CK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python in version 3.7 is installed into the environment. "
      ],
      "metadata": {
        "id": "mu9YAbJS8WHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "conda install --channel defaults conda python=3.7 --yes\n",
        "conda update --channel defaults --all --yes"
      ],
      "metadata": {
        "id": "2r20b2Wg36Kd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import sys\n",
        "sys.path\n",
        "!ls /usr/local/lib/python3.7/dist-packages\n",
        "_ = (sys.path\n",
        "        .append(\"/usr/local/lib/python3.7/site-packages\"))"
      ],
      "metadata": {
        "id": "iVia-nO53826"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use the library, we have to clone the corresponding git-repository."
      ],
      "metadata": {
        "id": "clVssA098eAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Uninstall existing Pytorch on Colab, which might be incompatible or buggy.\n",
        "# Note that the alpha beta crown library is tested on Pytorch 1.8.2 LTS, and other versions might be incompatible.(according to reference tutorial)\n",
        "# !pip uninstall --yes torch torchvision torchaudio torchtext\n",
        "!git clone https://github.com/huanzhang12/alpha-beta-CROWN.git"
      ],
      "metadata": {
        "id": "R3m60hqD3_sQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The environment is created. "
      ],
      "metadata": {
        "id": "3mZvUGSX8rAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "# Remove the old environment, if necessary.\n",
        "conda env remove --name alpha-beta-crown\n",
        "conda env create -f alpha-beta-CROWN/complete_verifier/environment.yml  # install all dependents into the alpha-beta-crown environment"
      ],
      "metadata": {
        "id": "UThlPEBk4Ckw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd alpha-beta-CROWN/complete_verifier/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssVpobxp4GQa",
        "outputId": "a1110f86-4e9d-446f-ba46-25c354ba0de4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/alpha-beta-CROWN/complete_verifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As explained on [the website of the α,β-project](https://github.com/huanzhang12/alpha-beta-CROWN), it is nessasary to create a configuration file in order to load the data. "
      ],
      "metadata": {
        "id": "pgl_rkFY8uvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure verification"
      ],
      "metadata": {
        "id": "LK3JuZPt4-mC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to verifiy a model. Therefore, we create a file called my_example_config.yaml which configures parameters for verification. \n",
        "The model is defined as followed: \n",
        "\n",
        "\n",
        "```\n",
        "def mnist_6_100():\n",
        "    model = nn.Sequential(\n",
        "        Flatten(),\n",
        "        nn.Linear(784,100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100,100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100,100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100,100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100,100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100, 10)\n",
        "    )\n",
        "    return model\n",
        "```\n",
        "\n",
        "It contains six linear layers and uses the ReLU activation function."
      ],
      "metadata": {
        "id": "Z8U-YzCS9yAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start with a very basic configuration file, which means that most parameters are set to default."
      ],
      "metadata": {
        "id": "fekbamk9Dm82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exp_configs/my_example_config.yaml\n",
        "general:\n",
        "  # device to run verifier\n",
        "  device: cpu \n",
        "  # Complete verification verifier. \n",
        "  # \"bab\": branch and bound with beta-CROWN; \n",
        "  # \"mip\": mixed integer programming (MIP) formulation; \n",
        "  # \"bab-refine\": branch and bound with intermediate layer bounds computed by MIP.\n",
        "  complete_verifier: bab\n",
        "model:\n",
        "  # name of the model (provided by library, see above)\n",
        "  name: mnist_6_100\n",
        "  # Load pretrained model from this specified path.\n",
        "  path: models/eran/mnist_6_100_nat.pth\n",
        "data:\n",
        "  # Dataset name. Dataset must be defined in utils.py.\n",
        "  dataset: MNIST_ERAN_UN\n",
        "  # Std vector used in data preprocessing.\n",
        "  std: [1.0]\n",
        "  # Mean vector used in data preprocessing.\n",
        "  mean: [0.0]\n",
        "specification:\n",
        "  # Set perturbation size (Lp norm). \n",
        "  # If not set, a default value may be used based on dataset loader.\n",
        "  epsilon: 0.026\n",
        "solver:\n",
        "  alpha-crown:\n",
        "    # Number of iterations for alpha-CROWN incomplete verifier.\n",
        "    iteration: 10\n",
        "attack:\n",
        "  # Early stop PGD when an adversarial example is found.\n",
        "  pgd_early_stop: true  "
      ],
      "metadata": {
        "id": "s1E0j7dbVvGg",
        "outputId": "c2ba127e-8461-4bcc-a719-342e4d2bd8f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing exp_configs/my_example_config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verification of network with α,β-CROWN"
      ],
      "metadata": {
        "id": "ivgGYPZUMShY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we only have to run our verification. "
      ],
      "metadata": {
        "id": "y-uwErjZ-T4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we activate our environment.\n",
        "Then we call the robustness_verifier on our configured yaml-file. The robustness_verifier is a class provided by the α,β-CROWN-library for Lp norm robustness verification and is often used to certify the robustness of a neural network. \n",
        "By setting start to 0 and end to 3, we indicate that only images 0 to 3 from the dataset should be verified. This is done for performance reasons. \n",
        "We finish by deactivating the environment.\n"
      ],
      "metadata": {
        "id": "7mM9hCLY-bQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run.sh\n",
        "\n",
        "source activate alpha-beta-crown\n",
        "python robustness_verifier.py --config exp_configs/my_example_config.yaml --start 0 --end 3\n",
        "conda deactivate"
      ],
      "metadata": {
        "id": "1wOCa05uMept",
        "outputId": "d713b74a-acdb-437e-92b3-09283ef49e08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 run.sh\n",
        "!./run.sh"
      ],
      "metadata": {
        "id": "di2IgirTVxO0",
        "outputId": "8b5aaac2-3632-40b0-c028-755e913eb638",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configurations:\n",
            "\n",
            "general:\n",
            "  device: cpu\n",
            "  seed: 100\n",
            "  conv_mode: patches\n",
            "  deterministic: false\n",
            "  double_fp: false\n",
            "  loss_reduction_func: sum\n",
            "  record_bounds: false\n",
            "  mode: verified-acc\n",
            "  complete_verifier: bab\n",
            "  enable_incomplete_verification: true\n",
            "  get_crown_verified_acc: false\n",
            "model:\n",
            "  path: models/eran/mnist_6_100_nat.pth\n",
            "  name: mnist_6_100\n",
            "data:\n",
            "  start: 0\n",
            "  end: 3\n",
            "  num_outputs: 10\n",
            "  mean: [0.0]\n",
            "  std: [1.0]\n",
            "  pkl_path: null\n",
            "  dataset: MNIST_ERAN_UN\n",
            "  data_filter_path: null\n",
            "  data_idx_file: null\n",
            "specification:\n",
            "  type: lp\n",
            "  norm: .inf\n",
            "  epsilon: 0.026\n",
            "solver:\n",
            "  alpha-crown:\n",
            "    lr_alpha: 0.1\n",
            "    iteration: 10\n",
            "    share_slopes: false\n",
            "    no_joint_opt: false\n",
            "  beta-crown:\n",
            "    batch_size: 64\n",
            "    lr_alpha: 0.01\n",
            "    lr_beta: 0.05\n",
            "    lr_decay: 0.98\n",
            "    optimizer: adam\n",
            "    iteration: 50\n",
            "    beta: true\n",
            "    beta_warmup: true\n",
            "  mip:\n",
            "    parallel_solvers: null\n",
            "    solver_threads: 1\n",
            "    refine_neuron_timeout: 15\n",
            "    refine_neuron_time_percentage: 0.8\n",
            "    early_stop: true\n",
            "bab:\n",
            "  max_domains: 200000\n",
            "  decision_thresh: 0\n",
            "  timeout: 360\n",
            "  get_upper_bound: false\n",
            "  dfs_percent: 0.0\n",
            "  branching:\n",
            "    method: kfsb\n",
            "    candidates: 3\n",
            "    reduceop: min\n",
            "attack:\n",
            "  pgd_order: before\n",
            "  enable_mip_attack: false\n",
            "  pgd_steps: 100\n",
            "  pgd_restarts: 30\n",
            "  pgd_early_stop: true\n",
            "  pgd_lr_decay: 0.99\n",
            "  pgd_alpha: auto\n",
            "debug:\n",
            "  lp_test: null\n",
            "\n",
            "Experiments at Sat Jun 25 18:57:22 2022 on b48537703fe5\n",
            "Sequential(\n",
            "  (0): Flatten()\n",
            "  (1): Linear(in_features=784, out_features=100, bias=True)\n",
            "  (2): ReLU()\n",
            "  (3): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (6): ReLU()\n",
            "  (7): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (8): ReLU()\n",
            "  (9): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (10): ReLU()\n",
            "  (11): Linear(in_features=100, out_features=10, bias=True)\n",
            ")\n",
            "/content/alpha-beta-CROWN/complete_verifier/utils.py:512: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  eps_temp = torch.tensor(eps_temp).reshape(1, -1, 1, 1)\n",
            "############################\n",
            "Sampled data loaded. No normalization used!\n",
            "Shape: torch.Size([1000, 1, 28, 28]) torch.Size([1000]) torch.Size([1000])\n",
            "X range: tensor(1.) tensor(0.) tensor(0.1223)\n",
            "Note runnerup label is empty here!\n",
            "############################\n",
            "epsilon after preprocessing: tensor([[[[0.0260]]]]), data_max = tensor([[[[1.]]]]), data_min = tensor([[[[0.]]]])\n",
            "Task length: 3\n",
            "saving results to Verified_ret_[mnist_6_100]_start=0_end=3_iter=50_b=64_timeout=360_branching=kfsb-min-3_lra-init=0.1_lra=0.01_lrb=0.05_PGD=before.npy\n",
            "\n",
            " %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0 img ID: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "predicted label 7, correct label 7, image norm 72.3686294555664, logits tensor([-2.7419, -1.9715, -0.2067,  1.3752, -0.8952, -3.5481, -7.3186, 13.2074,\n",
            "        -4.4189,  3.8866], grad_fn=<SelectBackward>)\n",
            "##### PGD attack: True label: 7, Tested against: ['all'] ######\n",
            "pgd prediction: tensor([-2.3157, -1.4768, -0.3271,  1.2411, -0.7472, -2.6581, -5.9633, 10.4310,\n",
            "        -3.2570,  3.3437], grad_fn=<SqueezeBackward1>)\n",
            "attack margin tensor([12.7467, 11.9078, 10.7581,  9.1899, 11.1781, 13.0891, 16.3943,     inf,\n",
            "        13.6879,  7.0872], grad_fn=<RsubBackward1>)\n",
            "untargeted pgd failed\n",
            "Model prediction is: tensor([[-2.7419, -1.9715, -0.2067,  1.3752, -0.8952, -3.5481, -7.3186, 13.2074,\n",
            "         -4.4189,  3.8866]], grad_fn=<AddBackward0>)\n",
            "alpha-CROWN optimizable variables initialized.\n",
            "initial CROWN bounds: tensor([[4.7498, 3.7544, 2.8216, 1.2226, 5.1403, 5.2825, 7.9722, 4.6015, 0.9006]]) None\n",
            "verified with init bound!\n",
            "Result: image 0 verification success (with incomplete verifier)!\n",
            "Wall time: 1.0692377090454102\n",
            "\n",
            " %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 1 img ID: 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "predicted label 2, correct label 2, image norm 113.13725280761719, logits tensor([-2.0595,  1.7575, 10.0782,  1.0748, -0.9482, -4.0839, -2.3911,  0.3436,\n",
            "        -0.9526, -4.9323], grad_fn=<SelectBackward>)\n",
            "##### PGD attack: True label: 2, Tested against: ['all'] ######\n",
            "pgd prediction: tensor([-2.2444,  1.8947,  8.5624,  1.2637, -1.0060, -3.4432, -2.2272,  0.5084,\n",
            "        -0.6859, -4.3616], grad_fn=<SqueezeBackward1>)\n",
            "attack margin tensor([10.8067,  6.6677,     inf,  7.2986,  9.5684, 12.0056, 10.7896,  8.0539,\n",
            "         9.2483, 12.9240], grad_fn=<RsubBackward1>)\n",
            "untargeted pgd failed\n",
            "Model prediction is: tensor([[-2.0595,  1.7575, 10.0782,  1.0748, -0.9482, -4.0839, -2.3911,  0.3436,\n",
            "         -0.9526, -4.9323]], grad_fn=<AddBackward0>)\n",
            "alpha-CROWN optimizable variables initialized.\n",
            "initial CROWN bounds: tensor([[0.6301, 0.1344, 0.4976, 2.1355, 2.5526, 2.1028, 1.8815, 1.7729, 3.6696]]) None\n",
            "verified with init bound!\n",
            "Result: image 1 verification success (with incomplete verifier)!\n",
            "Wall time: 1.0497548580169678\n",
            "\n",
            " %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 2 img ID: 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "predicted label 1, correct label 1, image norm 38.709800720214844, logits tensor([-3.7012,  8.8913,  1.1001, -2.5765,  0.6640, -2.1244, -1.6588,  0.6820,\n",
            "         1.0836, -2.1354], grad_fn=<SelectBackward>)\n",
            "##### PGD attack: True label: 1, Tested against: ['all'] ######\n",
            "pgd prediction: tensor([-2.5066,  4.6664,  3.0370, -1.3275,  0.3423, -2.1316, -1.4005,  0.8540,\n",
            "         0.2753, -2.5059], grad_fn=<SqueezeBackward1>)\n",
            "attack margin tensor([7.1731,    inf, 1.6294, 5.9940, 4.3241, 6.7981, 6.0669, 3.8125, 4.3912,\n",
            "        7.1723], grad_fn=<RsubBackward1>)\n",
            "untargeted pgd failed\n",
            "Model prediction is: tensor([[-3.7012,  8.8913,  1.1001, -2.5765,  0.6640, -2.1244, -1.6588,  0.6820,\n",
            "          1.0836, -2.1354]], grad_fn=<AddBackward0>)\n",
            "alpha-CROWN optimizable variables initialized.\n",
            "initial CROWN bounds: tensor([[ -96.2690,  -93.0547,  -94.6280,  -90.1884,  -93.8622,  -79.2704,\n",
            "          -82.7055,  -85.5262, -107.9016]]) None\n",
            "best_l after optimization: 518.3685913085938 with beta sum per layer: []\n",
            "alpha/beta optimization time: 0.6776247024536133\n",
            "initial alpha-CROWN bounds: tensor([[-59.9986, -57.0162, -58.0487, -59.7784, -59.3165, -47.2275, -52.1934,\n",
            "         -56.6780, -68.1114]], grad_fn=<AsStridedBackward>) None\n",
            "Sorted order for labels to attack: [2, 7, 4, 8, 3, 6, 5, 9, 0, 1]\n",
            "##### [2:2] Tested against 2 ######\n",
            "Model prediction is: tensor([[-3.7012,  8.8913,  1.1001, -2.5765,  0.6640, -2.1244, -1.6588,  0.6820,\n",
            "          1.0836, -2.1354]], grad_fn=<AddBackward0>)\n",
            "alpha-CROWN optimizable variables initialized.\n",
            "setting alpha for layer /22 start_node /23\n",
            "setting alpha for layer /22 start_node /25\n",
            "setting alpha for layer /22 start_node /27\n",
            "setting alpha for layer /22 start_node /29\n",
            "not setting layer /22 start_node /31 because shape mismatch (torch.Size([2, 1, 1, 100]) != torch.Size([2, 9, 1, 100]))\n",
            "setting alpha for layer /24 start_node /25\n",
            "setting alpha for layer /24 start_node /27\n",
            "setting alpha for layer /24 start_node /29\n",
            "not setting layer /24 start_node /31 because shape mismatch (torch.Size([2, 1, 1, 100]) != torch.Size([2, 9, 1, 100]))\n",
            "setting alpha for layer /26 start_node /27\n",
            "setting alpha for layer /26 start_node /29\n",
            "not setting layer /26 start_node /31 because shape mismatch (torch.Size([2, 1, 1, 100]) != torch.Size([2, 9, 1, 100]))\n",
            "setting alpha for layer /28 start_node /29\n",
            "not setting layer /28 start_node /31 because shape mismatch (torch.Size([2, 1, 1, 100]) != torch.Size([2, 9, 1, 100]))\n",
            "not setting layer /30 start_node /31 because shape mismatch (torch.Size([2, 1, 1, 100]) != torch.Size([2, 9, 1, 100]))\n",
            "0 /21 torch.Size([1, 100])\n",
            "1 /23 torch.Size([1, 100])\n",
            "2 /25 torch.Size([1, 100])\n",
            "3 /27 torch.Size([1, 100])\n",
            "4 /29 torch.Size([1, 100])\n",
            "best_l after optimization: 56.96556854248047 with beta sum per layer: []\n",
            "alpha/beta optimization time: 0.10374021530151367\n",
            "alpha-CROWN with fixed intermediate bounds: tensor([[-56.9656]], grad_fn=<AsStridedBackward>) None\n",
            "-56.96556854248047\n",
            "layer 0 size torch.Size([100]) unstable 58\n",
            "layer 1 size torch.Size([100]) unstable 90\n",
            "layer 2 size torch.Size([100]) unstable 100\n",
            "layer 3 size torch.Size([100]) unstable 100\n",
            "layer 4 size torch.Size([100]) unstable 100\n",
            "-----------------\n",
            "# of unstable neurons: 448\n",
            "-----------------\n",
            "\n",
            "splitting decisions: [[4, 44]]\n",
            "best_l after optimization: 104.66673278808594 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.18108421564102173]\n",
            "alpha/beta optimization time: 0.4985671043395996\n",
            "This batch time : update_bounds func: 0.5004\t prepare: 0.0012\t bound: 0.4988\t transfer: 0.0001\t finalize: 0.0003\n",
            "Accumulated time: update_bounds func: 0.5004\t prepare: 0.0012\t bound: 0.4988\t transfer: 0.0001\t finalize: 0.0003\n",
            "batch bounding time:  0.5005888938903809\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-53.27995, 42.034431] (1), [-51.38679, 42.034431] (1), \n",
            "length of domains: 2\n",
            "Total time: 0.5193\t pickout: 0.0004\t decision: 0.0181\t get_bound: 0.5006\t add_domain: 0.0002\n",
            "Current lb:-53.279945373535156\n",
            "2 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 0.6439943313598633\n",
            "\n",
            "splitting decisions: [[4, 2], [4, 2]]\n",
            "best_l after optimization: 198.2276153564453 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.4557074308395386]\n",
            "alpha/beta optimization time: 0.49707674980163574\n",
            "This batch time : update_bounds func: 0.4993\t prepare: 0.0014\t bound: 0.4973\t transfer: 0.0001\t finalize: 0.0004\n",
            "Accumulated time: update_bounds func: 0.9997\t prepare: 0.0026\t bound: 0.9960\t transfer: 0.0001\t finalize: 0.0007\n",
            "batch bounding time:  0.4994659423828125\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-51.65765, 42.034431] (2), [-50.07211, 42.034431] (2), [-49.26578, 42.034431] (2), [-47.23207, 42.034431] (2), \n",
            "length of domains: 4\n",
            "Total time: 0.5181\t pickout: 0.0004\t decision: 0.0179\t get_bound: 0.4995\t add_domain: 0.0002\n",
            "Current lb:-51.657649993896484\n",
            "6 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 1.1623191833496094\n",
            "\n",
            "splitting decisions: [[4, 6], [4, 6], [4, 49], [4, 49]]\n",
            "best_l after optimization: 383.441650390625 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 1.101868748664856]\n",
            "alpha/beta optimization time: 0.5138905048370361\n",
            "This batch time : update_bounds func: 0.5167\t prepare: 0.0018\t bound: 0.5141\t transfer: 0.0001\t finalize: 0.0007\n",
            "Accumulated time: update_bounds func: 1.5164\t prepare: 0.0043\t bound: 1.5101\t transfer: 0.0001\t finalize: 0.0014\n",
            "batch bounding time:  0.51692795753479\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-50.19880, 42.034431] (3), [-50.11042, 42.034431] (3), [-48.66611, 42.034431] (3), [-48.63233, 42.034431] (3), [-47.78267, 42.034431] (3), [-47.03459, 42.034431] (3), [-45.94384, 42.034431] (3), [-45.07291, 42.034431] (3), \n",
            "length of domains: 8\n",
            "Total time: 0.5348\t pickout: 0.0006\t decision: 0.0169\t get_bound: 0.5170\t add_domain: 0.0004\n",
            "Current lb:-50.198795318603516\n",
            "14 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 1.6974449157714844\n",
            "\n",
            "splitting decisions: [[4, 11], [4, 49], [4, 11], [4, 49], [4, 6], [4, 11], [4, 6], [4, 30]]\n",
            "best_l after optimization: 733.8565063476562 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 3.3301734924316406]\n",
            "alpha/beta optimization time: 0.5338284969329834\n",
            "This batch time : update_bounds func: 0.5384\t prepare: 0.0027\t bound: 0.5341\t transfer: 0.0002\t finalize: 0.0013\n",
            "Accumulated time: update_bounds func: 2.0548\t prepare: 0.0071\t bound: 2.0442\t transfer: 0.0002\t finalize: 0.0027\n",
            "batch bounding time:  0.5386257171630859\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-48.78342, 42.034431] (4), [-48.75897, 42.034431] (4), [-48.07679, 42.034431] (4), [-47.38496, 42.034431] (4), [-47.22784, 42.034431] (4), [-46.55886, 42.034431] (4), [-46.49231, 42.034431] (4), [-45.91407, 42.034431] (4), [-45.43240, 42.034431] (4), [-45.42344, 42.034431] (4), [-44.80903, 42.034431] (4), [-44.48076, 42.034431] (4), [-43.91847, 42.034431] (4), [-43.88719, 42.034431] (4), [-43.50759, 42.034431] (4), [-43.20039, 42.034431] (4), \n",
            "length of domains: 16\n",
            "Total time: 0.5625\t pickout: 0.0007\t decision: 0.0223\t get_bound: 0.5387\t add_domain: 0.0007\n",
            "Current lb:-48.783416748046875\n",
            "30 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 2.26027250289917\n",
            "\n",
            "splitting decisions: [[4, 11], [4, 49], [4, 11], [4, 11], [4, 40], [4, 11], [4, 40], [4, 40], [4, 6], [4, 49]]\n",
            "best_l after optimization: 1404.825439453125 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 9.288368225097656]\n",
            "alpha/beta optimization time: 0.5933516025543213\n",
            "This batch time : update_bounds func: 0.6002\t prepare: 0.0039\t bound: 0.5936\t transfer: 0.0001\t finalize: 0.0024\n",
            "Accumulated time: update_bounds func: 2.6550\t prepare: 0.0110\t bound: 2.6378\t transfer: 0.0001\t finalize: 0.0051\n",
            "batch bounding time:  0.6004490852355957\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-47.48010, 42.034431] (5), [-47.40058, 42.034431] (5), [-46.72385, 42.034431] (5), [-46.61126, 42.034431] (5), [-46.19817, 42.034431] (5), [-46.04490, 42.034431] (5), [-45.77980, 42.034431] (5), [-45.29888, 42.034431] (5), [-45.08654, 42.034431] (5), [-45.00466, 42.034431] (5), [-44.66739, 42.034431] (5), [-44.45993, 42.034431] (5), [-44.29692, 42.034431] (5), [-44.26187, 42.034431] (5), [-44.02414, 42.034431] (5), [-43.76382, 42.034431] (5), [-43.59287, 42.034431] (5), [-43.44033, 42.034431] (5), [-43.40433, 42.034431] (5), [-43.23991, 42.034431] (5), \n",
            "length of domains: 32\n",
            "Total time: 0.6264\t pickout: 0.0011\t decision: 0.0232\t get_bound: 0.6005\t add_domain: 0.0015\n",
            "Current lb:-47.480098724365234\n",
            "62 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 2.8872268199920654\n",
            "\n",
            "splitting decisions: [[4, 40], [4, 40], [4, 40], [4, 40], [4, 40], [4, 49], [4, 49], [4, 27], [4, 11], [4, 40]]\n",
            "best_l after optimization: 2704.271240234375 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 25.05016326904297]\n",
            "alpha/beta optimization time: 0.7159948348999023\n",
            "This batch time : update_bounds func: 0.7295\t prepare: 0.0079\t bound: 0.7164\t transfer: 0.0002\t finalize: 0.0048\n",
            "Accumulated time: update_bounds func: 3.3845\t prepare: 0.0189\t bound: 3.3542\t transfer: 0.0002\t finalize: 0.0100\n",
            "batch bounding time:  0.7298169136047363\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-46.19017, 42.034431] (6), [-46.16184, 42.034431] (6), [-46.10364, 42.034431] (6), [-45.98270, 42.034431] (6), [-45.48263, 42.034431] (6), [-45.40189, 42.034431] (6), [-45.15732, 42.034431] (6), [-44.99420, 42.034431] (6), [-44.97310, 42.034431] (6), [-44.87853, 42.034431] (6), [-44.75864, 42.034431] (6), [-44.71584, 42.034431] (6), [-44.08307, 42.034431] (6), [-43.95541, 42.034431] (6), [-43.89918, 42.034431] (6), [-43.65172, 42.034431] (6), [-43.56237, 42.034431] (6), [-43.25341, 42.034431] (6), [-43.18885, 42.034431] (6), [-43.16350, 42.034431] (6), \n",
            "length of domains: 64\n",
            "Total time: 0.7693\t pickout: 0.0020\t decision: 0.0344\t get_bound: 0.7300\t add_domain: 0.0030\n",
            "Current lb:-46.1901741027832\n",
            "126 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 3.6573193073272705\n",
            "\n",
            "splitting decisions: [[4, 30], [4, 27], [4, 27], [4, 27], [4, 30], [4, 30], [4, 27], [4, 27], [4, 30], [4, 30]]\n",
            "best_l after optimization: 5223.94873046875 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 60.176170349121094]\n",
            "alpha/beta optimization time: 0.939845085144043\n",
            "This batch time : update_bounds func: 0.9629\t prepare: 0.0129\t bound: 0.9403\t transfer: 0.0002\t finalize: 0.0089\n",
            "Accumulated time: update_bounds func: 4.3474\t prepare: 0.0318\t bound: 4.2946\t transfer: 0.0002\t finalize: 0.0189\n",
            "batch bounding time:  0.9632573127746582\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-45.08402, 42.034431] (7), [-45.06850, 42.034431] (7), [-44.98437, 42.034431] (7), [-44.90120, 42.034431] (7), [-44.62424, 42.034431] (7), [-44.59280, 42.034431] (7), [-44.48050, 42.034431] (7), [-44.30275, 42.034431] (7), [-44.29057, 42.034431] (7), [-44.21556, 42.034431] (7), [-43.94850, 42.034431] (7), [-43.91388, 42.034431] (7), [-43.91365, 42.034431] (7), [-43.82443, 42.034431] (7), [-43.80711, 42.034431] (7), [-43.70984, 42.034431] (7), [-43.70766, 42.034431] (7), [-43.70731, 42.034431] (7), [-43.65223, 42.034431] (7), [-43.37645, 42.034431] (7), \n",
            "length of domains: 128\n",
            "Total time: 1.0263\t pickout: 0.0036\t decision: 0.0519\t get_bound: 0.9635\t add_domain: 0.0072\n",
            "Current lb:-45.084022521972656\n",
            "254 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 4.6849143505096436\n",
            "\n",
            "splitting decisions: [[4, 78], [4, 27], [4, 78], [4, 78], [4, 30], [4, 30], [4, 30], [4, 27], [4, 27], [4, 27]]\n",
            "best_l after optimization: 5266.0947265625 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 62.865699768066406]\n",
            "alpha/beta optimization time: 0.9528584480285645\n",
            "This batch time : update_bounds func: 0.9767\t prepare: 0.0131\t bound: 0.9533\t transfer: 0.0002\t finalize: 0.0097\n",
            "Accumulated time: update_bounds func: 5.3242\t prepare: 0.0450\t bound: 5.2479\t transfer: 0.0002\t finalize: 0.0286\n",
            "batch bounding time:  0.9771735668182373\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-44.03747, 42.034431] (8), [-44.03487, 42.034431] (8), [-43.93186, 42.034431] (8), [-43.83501, 42.034431] (8), [-43.55985, 42.034431] (8), [-43.48882, 42.034431] (8), [-43.47542, 42.034431] (8), [-43.44715, 42.034431] (8), [-43.39219, 42.034431] (8), [-43.38486, 42.034431] (8), [-43.32085, 42.034431] (8), [-43.19854, 42.034431] (8), [-43.08973, 42.034431] (8), [-43.07963, 42.034431] (8), [-42.90274, 42.034431] (8), [-42.86184, 42.034431] (8), [-42.84426, 42.034431] (8), [-42.83836, 42.034431] (8), [-42.83824, 42.034431] (8), [-42.77815, 42.034431] (8), \n",
            "length of domains: 192\n",
            "Total time: 1.0364\t pickout: 0.0035\t decision: 0.0489\t get_bound: 0.9774\t add_domain: 0.0066\n",
            "Current lb:-44.03746795654297\n",
            "382 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 5.722602844238281\n",
            "\n",
            "splitting decisions: [[4, 78], [4, 30], [4, 30], [4, 30], [4, 30], [4, 78], [4, 78], [4, 30], [4, 30], [4, 78]]\n",
            "best_l after optimization: 5237.419921875 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 61.93938064575195]\n",
            "alpha/beta optimization time: 0.9344735145568848\n",
            "This batch time : update_bounds func: 0.9586\t prepare: 0.0140\t bound: 0.9348\t transfer: 0.0002\t finalize: 0.0092\n",
            "Accumulated time: update_bounds func: 6.2828\t prepare: 0.0590\t bound: 6.1827\t transfer: 0.0002\t finalize: 0.0378\n",
            "batch bounding time:  0.9589328765869141\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-42.98080, 42.034431] (9), [-42.91199, 42.034431] (9), [-42.89675, 42.034431] (9), [-42.76473, 42.034431] (9), [-42.57202, 42.034431] (9), [-42.41336, 42.034431] (9), [-42.39521, 42.034431] (9), [-42.39169, 42.034431] (9), [-42.37922, 42.034431] (9), [-42.35997, 42.034431] (9), [-42.35812, 42.034431] (9), [-42.24270, 42.034431] (9), [-42.11121, 42.034431] (9), [-42.10052, 42.034431] (9), [-42.08764, 42.034431] (9), [-42.04029, 42.034431] (9), [-42.01001, 42.034431] (9), [-41.94562, 42.034431] (9), [-41.89270, 42.034431] (9), [-41.86012, 42.034431] (9), \n",
            "length of domains: 256\n",
            "Total time: 1.0151\t pickout: 0.0036\t decision: 0.0453\t get_bound: 0.9592\t add_domain: 0.0071\n",
            "Current lb:-42.980796813964844\n",
            "510 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 6.739153623580933\n",
            "\n",
            "splitting decisions: [[4, 97], [4, 97], [4, 97], [4, 97], [4, 97], [4, 97], [4, 97], [4, 97], [4, 97], [4, 97]]\n",
            "best_l after optimization: 5169.1845703125 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 61.61366653442383]\n",
            "alpha/beta optimization time: 0.9477119445800781\n",
            "This batch time : update_bounds func: 0.9710\t prepare: 0.0130\t bound: 0.9480\t transfer: 0.0002\t finalize: 0.0094\n",
            "Accumulated time: update_bounds func: 7.2538\t prepare: 0.0720\t bound: 7.1307\t transfer: 0.0002\t finalize: 0.0472\n",
            "batch bounding time:  0.9713337421417236\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-41.94626, 42.034431] (10), [-41.94130, 42.034431] (10), [-41.86777, 42.034431] (10), [-41.79655, 42.034431] (10), [-41.58899, 42.034431] (10), [-41.43185, 42.034431] (10), [-41.42847, 42.034431] (10), [-41.41181, 42.034431] (10), [-41.41001, 42.034431] (10), [-41.40841, 42.034431] (10), [-41.40668, 42.034431] (10), [-41.37052, 42.034431] (10), [-41.27841, 42.034431] (10), [-41.20874, 42.034431] (10), [-41.14031, 42.034431] (10), [-41.06649, 42.034431] (10), [-41.02739, 42.034431] (10), [-40.99832, 42.034431] (9), [-40.99130, 42.034431] (10), [-40.96977, 42.034431] (9), \n",
            "length of domains: 320\n",
            "Total time: 1.0304\t pickout: 0.0037\t decision: 0.0472\t get_bound: 0.9716\t add_domain: 0.0080\n",
            "Current lb:-41.94626235961914\n",
            "638 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 7.770789384841919\n",
            "\n",
            "splitting decisions: [[4, 69], [4, 69], [4, 18], [4, 69], [4, 69], [4, 18], [4, 69], [4, 69], [4, 69], [4, 69]]\n",
            "best_l after optimization: 5080.455078125 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 61.80433654785156]\n",
            "alpha/beta optimization time: 0.9436047077178955\n",
            "This batch time : update_bounds func: 0.9681\t prepare: 0.0137\t bound: 0.9439\t transfer: 0.0002\t finalize: 0.0099\n",
            "Accumulated time: update_bounds func: 8.2218\t prepare: 0.0856\t bound: 8.0747\t transfer: 0.0002\t finalize: 0.0571\n",
            "batch bounding time:  0.9683966636657715\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-41.03478, 42.034431] (11), [-41.00084, 42.034431] (11), [-40.93353, 42.034431] (11), [-40.84837, 42.034431] (11), [-40.77231, 42.034431] (11), [-40.67560, 42.034431] (11), [-40.57505, 42.034431] (9), [-40.57356, 42.034431] (10), [-40.57172, 42.034431] (9), [-40.57019, 42.034431] (10), [-40.56758, 42.034431] (10), [-40.55965, 42.034431] (10), [-40.55922, 42.034431] (8), [-40.55542, 42.034431] (10), [-40.54731, 42.034431] (10), [-40.54325, 42.034431] (10), [-40.53960, 42.034431] (10), [-40.53181, 42.034431] (8), [-40.52784, 42.034431] (10), [-40.52184, 42.034431] (10), \n",
            "length of domains: 384\n",
            "Total time: 1.0273\t pickout: 0.0036\t decision: 0.0464\t get_bound: 0.9686\t add_domain: 0.0087\n",
            "Current lb:-41.03478240966797\n",
            "766 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 8.799328327178955\n",
            "\n",
            "splitting decisions: [[4, 18], [4, 18], [4, 69], [4, 18], [4, 69], [4, 18], [4, 69], [4, 69], [4, 69], [4, 97]]\n",
            "best_l after optimization: 5024.88037109375 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 64.88390350341797]\n",
            "alpha/beta optimization time: 0.9505681991577148\n",
            "This batch time : update_bounds func: 0.9747\t prepare: 0.0137\t bound: 0.9509\t transfer: 0.0002\t finalize: 0.0095\n",
            "Accumulated time: update_bounds func: 9.1966\t prepare: 0.0994\t bound: 9.0256\t transfer: 0.0002\t finalize: 0.0666\n",
            "batch bounding time:  0.9751219749450684\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-40.34302, 42.034431] (10), [-40.34241, 42.034431] (10), [-40.34012, 42.034431] (10), [-40.33430, 42.034431] (8), [-40.33421, 42.034431] (7), [-40.33335, 42.034431] (7), [-40.33334, 42.034431] (9), [-40.32901, 42.034431] (10), [-40.32124, 42.034431] (11), [-40.32047, 42.034431] (8), [-40.31371, 42.034431] (11), [-40.31253, 42.034431] (9), [-40.30912, 42.034431] (11), [-40.30758, 42.034431] (10), [-40.30376, 42.034431] (11), [-40.28768, 42.034431] (11), [-40.28714, 42.034431] (10), [-40.28525, 42.034431] (10), [-40.28285, 42.034431] (7), [-40.28153, 42.034431] (8), \n",
            "length of domains: 448\n",
            "Total time: 1.0351\t pickout: 0.0038\t decision: 0.0468\t get_bound: 0.9754\t add_domain: 0.0091\n",
            "Current lb:-40.343017578125\n",
            "894 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 9.835733652114868\n",
            "\n",
            "splitting decisions: [[4, 97], [4, 97], [4, 69], [4, 78], [4, 78], [4, 30], [4, 27], [4, 69], [4, 18], [4, 97]]\n",
            "best_l after optimization: 4986.89892578125 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 68.18719482421875]\n",
            "alpha/beta optimization time: 0.9627397060394287\n",
            "This batch time : update_bounds func: 1.0398\t prepare: 0.0131\t bound: 0.9633\t transfer: 0.0002\t finalize: 0.0628\n",
            "Accumulated time: update_bounds func: 10.2364\t prepare: 0.1124\t bound: 9.9889\t transfer: 0.0002\t finalize: 0.1294\n",
            "batch bounding time:  1.0403306484222412\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-40.06356, 42.034431] (10), [-40.05971, 42.034431] (11), [-40.05830, 42.034431] (10), [-40.05777, 42.034431] (8), [-40.05703, 42.034431] (10), [-40.05593, 42.034431] (9), [-40.04553, 42.034431] (10), [-40.04547, 42.034431] (8), [-40.04169, 42.034431] (10), [-40.03972, 42.034431] (9), [-40.03923, 42.034431] (10), [-40.03909, 42.034431] (7), [-40.03789, 42.034431] (9), [-40.03451, 42.034431] (9), [-40.03395, 42.034431] (10), [-40.02748, 42.034431] (11), [-40.02220, 42.034431] (10), [-40.01919, 42.034431] (9), [-40.01843, 42.034431] (10), [-40.01221, 42.034431] (11), \n",
            "length of domains: 512\n",
            "Total time: 1.1031\t pickout: 0.0038\t decision: 0.0453\t get_bound: 1.0408\t add_domain: 0.0132\n",
            "Current lb:-40.06355667114258\n",
            "1022 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 10.940159559249878\n",
            "\n",
            "splitting decisions: [[4, 27], [4, 18], [4, 69], [4, 78], [4, 18], [4, 69], [4, 27], [4, 78], [4, 69], [4, 69]]\n",
            "best_l after optimization: 4954.45849609375 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 78.72599792480469]\n",
            "alpha/beta optimization time: 0.9605648517608643\n",
            "This batch time : update_bounds func: 0.9865\t prepare: 0.0147\t bound: 0.9609\t transfer: 0.0002\t finalize: 0.0103\n",
            "Accumulated time: update_bounds func: 11.2229\t prepare: 0.1272\t bound: 10.9498\t transfer: 0.0002\t finalize: 0.1397\n",
            "batch bounding time:  0.9868848323822021\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-39.84800, 42.034431] (10), [-39.84651, 42.034431] (10), [-39.84272, 42.034431] (10), [-39.84134, 42.034431] (9), [-39.84070, 42.034431] (10), [-39.84021, 42.034431] (8), [-39.83894, 42.034431] (11), [-39.83827, 42.034431] (10), [-39.83500, 42.034431] (11), [-39.83070, 42.034431] (11), [-39.83058, 42.034431] (11), [-39.82938, 42.034431] (9), [-39.80919, 42.034431] (8), [-39.80838, 42.034431] (9), [-39.80830, 42.034431] (10), [-39.80428, 42.034431] (11), [-39.79976, 42.034431] (10), [-39.79943, 42.034431] (12), [-39.79708, 42.034431] (9), [-39.79174, 42.034431] (11), \n",
            "length of domains: 576\n",
            "Total time: 1.0436\t pickout: 0.0040\t decision: 0.0449\t get_bound: 0.9871\t add_domain: 0.0077\n",
            "Current lb:-39.847999572753906\n",
            "1150 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 11.985134601593018\n",
            "\n",
            "splitting decisions: [[4, 97], [4, 97], [4, 69], [4, 69], [4, 97], [4, 78], [4, 18], [4, 78], [4, 18], [4, 18]]\n",
            "best_l after optimization: 4930.87744140625 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 75.10673522949219]\n",
            "alpha/beta optimization time: 0.9545176029205322\n",
            "This batch time : update_bounds func: 0.9856\t prepare: 0.0196\t bound: 0.9549\t transfer: 0.0002\t finalize: 0.0104\n",
            "Accumulated time: update_bounds func: 12.2085\t prepare: 0.1467\t bound: 11.9046\t transfer: 0.0002\t finalize: 0.1501\n",
            "batch bounding time:  0.985969066619873\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-39.67037, 42.034431] (7), [-39.66978, 42.034431] (11), [-39.66954, 42.034431] (7), [-39.66608, 42.034431] (10), [-39.66258, 42.034431] (11), [-39.65853, 42.034431] (9), [-39.65535, 42.034431] (12), [-39.65360, 42.034431] (10), [-39.64949, 42.034431] (11), [-39.64492, 42.034431] (11), [-39.64454, 42.034431] (10), [-39.64162, 42.034431] (9), [-39.64118, 42.034431] (10), [-39.63747, 42.034431] (11), [-39.63716, 42.034431] (8), [-39.63041, 42.034431] (10), [-39.63020, 42.034431] (8), [-39.62864, 42.034431] (7), [-39.62276, 42.034431] (9), [-39.62146, 42.034431] (8), \n",
            "length of domains: 640\n",
            "Total time: 1.0496\t pickout: 0.0041\t decision: 0.0504\t get_bound: 0.9862\t add_domain: 0.0089\n",
            "Current lb:-39.67036819458008\n",
            "1278 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 13.036065340042114\n",
            "\n",
            "splitting decisions: [[4, 6], [4, 18], [4, 78], [4, 27], [4, 18], [4, 69], [4, 9], [4, 78], [4, 18], [4, 18]]\n",
            "best_l after optimization: 4906.2353515625 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 71.43973541259766]\n",
            "alpha/beta optimization time: 0.9288451671600342\n",
            "This batch time : update_bounds func: 0.9536\t prepare: 0.0133\t bound: 0.9292\t transfer: 0.0002\t finalize: 0.0106\n",
            "Accumulated time: update_bounds func: 13.1621\t prepare: 0.1600\t bound: 12.8338\t transfer: 0.0002\t finalize: 0.1607\n",
            "batch bounding time:  0.9539892673492432\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-39.48734, 42.034431] (11), [-39.48489, 42.034431] (9), [-39.48460, 42.034431] (11), [-39.48409, 42.034431] (12), [-39.48021, 42.034431] (11), [-39.47910, 42.034431] (8), [-39.47841, 42.034431] (11), [-39.47791, 42.034431] (11), [-39.47550, 42.034431] (8), [-39.47466, 42.034431] (8), [-39.47038, 42.034431] (11), [-39.46994, 42.034431] (8), [-39.46659, 42.034431] (9), [-39.46223, 42.034431] (11), [-39.46153, 42.034431] (11), [-39.46118, 42.034431] (10), [-39.46006, 42.034431] (11), [-39.45823, 42.034431] (10), [-39.45782, 42.034431] (9), [-39.45588, 42.034431] (11), \n",
            "length of domains: 704\n",
            "Total time: 1.0179\t pickout: 0.0039\t decision: 0.0521\t get_bound: 0.9542\t add_domain: 0.0078\n",
            "Current lb:-39.48733901977539\n",
            "1406 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 14.055346012115479\n",
            "\n",
            "splitting decisions: [[4, 79], [4, 69], [4, 18], [4, 9], [4, 18], [4, 30], [4, 79], [4, 69], [4, 97], [4, 97]]\n",
            "best_l after optimization: 4891.953125 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 76.43084716796875]\n",
            "alpha/beta optimization time: 0.9781475067138672\n",
            "This batch time : update_bounds func: 1.0081\t prepare: 0.0147\t bound: 0.9785\t transfer: 0.0003\t finalize: 0.0142\n",
            "Accumulated time: update_bounds func: 14.1702\t prepare: 0.1747\t bound: 13.8123\t transfer: 0.0003\t finalize: 0.1749\n",
            "batch bounding time:  1.0084311962127686\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-39.36905, 42.034431] (11), [-39.36715, 42.034431] (10), [-39.36594, 42.034431] (9), [-39.35991, 42.034431] (11), [-39.35978, 42.034431] (8), [-39.35802, 42.034431] (11), [-39.35484, 42.034431] (10), [-39.35398, 42.034431] (9), [-39.35221, 42.034431] (10), [-39.35071, 42.034431] (12), [-39.34938, 42.034431] (10), [-39.34460, 42.034431] (11), [-39.33994, 42.034431] (12), [-39.33398, 42.034431] (8), [-39.33168, 42.034431] (10), [-39.32047, 42.034431] (7), [-39.31953, 42.034431] (11), [-39.31675, 42.034431] (11), [-39.31520, 42.034431] (10), [-39.31497, 42.034431] (11), \n",
            "length of domains: 768\n",
            "Total time: 1.0775\t pickout: 0.0038\t decision: 0.0538\t get_bound: 1.0087\t add_domain: 0.0111\n",
            "Current lb:-39.36905288696289\n",
            "1534 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 15.134084224700928\n",
            "\n",
            "splitting decisions: [[4, 18], [4, 69], [4, 69], [4, 69], [4, 27], [4, 18], [4, 97], [4, 27], [4, 78], [4, 90]]\n",
            "best_l after optimization: 4871.57421875 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 77.7362060546875]\n",
            "alpha/beta optimization time: 0.9531328678131104\n",
            "This batch time : update_bounds func: 0.9801\t prepare: 0.0149\t bound: 0.9535\t transfer: 0.0002\t finalize: 0.0111\n",
            "Accumulated time: update_bounds func: 15.1503\t prepare: 0.1896\t bound: 14.7658\t transfer: 0.0002\t finalize: 0.1860\n",
            "batch bounding time:  0.9804625511169434\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-39.19868, 42.034431] (11), [-39.19863, 42.034431] (11), [-39.19851, 42.034431] (11), [-39.19672, 42.034431] (11), [-39.19583, 42.034431] (12), [-39.19210, 42.034431] (12), [-39.18903, 42.034431] (10), [-39.18832, 42.034431] (10), [-39.18770, 42.034431] (11), [-39.18643, 42.034431] (8), [-39.18568, 42.034431] (10), [-39.18515, 42.034431] (11), [-39.18490, 42.034431] (9), [-39.18120, 42.034431] (11), [-39.17910, 42.034431] (11), [-39.17656, 42.034431] (12), [-39.17643, 42.034431] (11), [-39.17577, 42.034431] (10), [-39.17566, 42.034431] (12), [-39.17397, 42.034431] (12), \n",
            "length of domains: 832\n",
            "Total time: 1.0414\t pickout: 0.0039\t decision: 0.0481\t get_bound: 0.9807\t add_domain: 0.0087\n",
            "Current lb:-39.19867706298828\n",
            "1662 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 16.17738127708435\n",
            "\n",
            "splitting decisions: [[4, 79], [4, 18], [4, 27], [4, 79], [4, 9], [4, 79], [4, 69], [4, 97], [4, 97], [4, 97]]\n",
            "best_l after optimization: 4849.01171875 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 76.33444213867188]\n",
            "alpha/beta optimization time: 0.9336767196655273\n",
            "This batch time : update_bounds func: 0.9588\t prepare: 0.0138\t bound: 0.9341\t transfer: 0.0002\t finalize: 0.0095\n",
            "Accumulated time: update_bounds func: 16.1090\t prepare: 0.2033\t bound: 15.6999\t transfer: 0.0002\t finalize: 0.1955\n",
            "batch bounding time:  0.9591045379638672\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-39.07072, 42.034431] (13), [-39.07050, 42.034431] (8), [-39.06690, 42.034431] (8), [-39.06535, 42.034431] (11), [-39.06389, 42.034431] (12), [-39.06377, 42.034431] (11), [-39.06296, 42.034431] (8), [-39.06118, 42.034431] (12), [-39.06065, 42.034431] (11), [-39.05717, 42.034431] (8), [-39.05700, 42.034431] (13), [-39.04496, 42.034431] (9), [-39.04395, 42.034431] (11), [-39.04291, 42.034431] (7), [-39.04263, 42.034431] (11), [-39.04011, 42.034431] (11), [-39.03913, 42.034431] (11), [-39.02891, 42.034431] (12), [-39.02787, 42.034431] (11), [-39.02261, 42.034431] (10), \n",
            "length of domains: 896\n",
            "Total time: 1.0182\t pickout: 0.0043\t decision: 0.0466\t get_bound: 0.9593\t add_domain: 0.0080\n",
            "Current lb:-39.07072067260742\n",
            "1790 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 17.197132110595703\n",
            "\n",
            "splitting decisions: [[4, 9], [4, 78], [4, 78], [4, 18], [4, 9], [4, 97], [4, 30], [4, 9], [4, 27], [4, 27]]\n",
            "best_l after optimization: 4837.54296875 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 83.63166046142578]\n",
            "alpha/beta optimization time: 0.9342377185821533\n",
            "This batch time : update_bounds func: 0.9593\t prepare: 0.0132\t bound: 0.9347\t transfer: 0.0002\t finalize: 0.0107\n",
            "Accumulated time: update_bounds func: 17.0683\t prepare: 0.2166\t bound: 16.6346\t transfer: 0.0002\t finalize: 0.2063\n",
            "batch bounding time:  0.9596281051635742\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-38.92338, 42.034431] (9), [-38.92273, 42.034431] (9), [-38.92152, 42.034431] (11), [-38.91515, 42.034431] (11), [-38.91469, 42.034431] (10), [-38.91443, 42.034431] (11), [-38.91389, 42.034431] (10), [-38.90985, 42.034431] (11), [-38.90862, 42.034431] (8), [-38.90780, 42.034431] (8), [-38.90437, 42.034431] (9), [-38.90331, 42.034431] (9), [-38.90239, 42.034431] (10), [-38.90046, 42.034431] (8), [-38.89871, 42.034431] (11), [-38.89744, 42.034431] (12), [-38.89736, 42.034431] (10), [-38.89702, 42.034431] (11), [-38.89441, 42.034431] (11), [-38.89307, 42.034431] (11), \n",
            "length of domains: 960\n",
            "Total time: 1.0230\t pickout: 0.0039\t decision: 0.0512\t get_bound: 0.9598\t add_domain: 0.0081\n",
            "Current lb:-38.92338180541992\n",
            "1918 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 18.22177243232727\n",
            "\n",
            "splitting decisions: [[4, 69], [4, 69], [4, 90], [4, 79], [4, 78], [4, 18], [4, 78], [4, 18], [4, 78], [4, 78]]\n",
            "best_l after optimization: 4825.27392578125 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 78.18598175048828]\n",
            "alpha/beta optimization time: 0.9344079494476318\n",
            "This batch time : update_bounds func: 1.0189\t prepare: 0.0133\t bound: 0.9347\t transfer: 0.0002\t finalize: 0.0702\n",
            "Accumulated time: update_bounds func: 18.0872\t prepare: 0.2299\t bound: 17.5693\t transfer: 0.0002\t finalize: 0.2765\n",
            "batch bounding time:  1.019256591796875\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-38.82260, 42.034431] (9), [-38.81944, 42.034431] (9), [-38.81941, 42.034431] (12), [-38.81852, 42.034431] (11), [-38.81818, 42.034431] (10), [-38.81751, 42.034431] (11), [-38.81619, 42.034431] (12), [-38.81513, 42.034431] (8), [-38.81188, 42.034431] (11), [-38.81179, 42.034431] (11), [-38.81075, 42.034431] (12), [-38.80718, 42.034431] (9), [-38.80639, 42.034431] (12), [-38.80426, 42.034431] (10), [-38.80423, 42.034431] (12), [-38.79880, 42.034431] (11), [-38.79679, 42.034431] (9), [-38.79500, 42.034431] (11), [-38.79194, 42.034431] (10), [-38.79174, 42.034431] (10), \n",
            "length of domains: 1024\n",
            "Total time: 1.0797\t pickout: 0.0040\t decision: 0.0480\t get_bound: 1.0195\t add_domain: 0.0082\n",
            "Current lb:-38.82259750366211\n",
            "2046 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 19.303175449371338\n",
            "\n",
            "splitting decisions: [[4, 69], [4, 69], [4, 9], [4, 18], [4, 78], [4, 18], [4, 9], [4, 30], [4, 18], [4, 97]]\n",
            "best_l after optimization: 4811.4140625 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 84.28585815429688]\n",
            "alpha/beta optimization time: 0.9542529582977295\n",
            "This batch time : update_bounds func: 0.9781\t prepare: 0.0132\t bound: 0.9546\t transfer: 0.0002\t finalize: 0.0096\n",
            "Accumulated time: update_bounds func: 19.0652\t prepare: 0.2431\t bound: 18.5239\t transfer: 0.0002\t finalize: 0.2861\n",
            "batch bounding time:  0.978435754776001\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-38.71781, 42.034431] (9), [-38.71664, 42.034431] (9), [-38.71467, 42.034431] (11), [-38.71398, 42.034431] (8), [-38.71329, 42.034431] (12), [-38.71190, 42.034431] (11), [-38.71131, 42.034431] (9), [-38.70763, 42.034431] (12), [-38.70720, 42.034431] (11), [-38.70633, 42.034431] (11), [-38.70548, 42.034431] (12), [-38.70489, 42.034431] (12), [-38.70488, 42.034431] (12), [-38.70364, 42.034431] (9), [-38.70236, 42.034431] (11), [-38.70068, 42.034431] (11), [-38.69985, 42.034431] (13), [-38.69709, 42.034431] (10), [-38.69476, 42.034431] (12), [-38.69363, 42.034431] (11), \n",
            "length of domains: 1088\n",
            "Total time: 1.0412\t pickout: 0.0046\t decision: 0.0479\t get_bound: 0.9787\t add_domain: 0.0100\n",
            "Current lb:-38.717811584472656\n",
            "2174 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 20.34575581550598\n",
            "\n",
            "splitting decisions: [[4, 69], [4, 69], [4, 27], [4, 27], [4, 9], [4, 69], [4, 30], [4, 9], [4, 18], [4, 18]]\n",
            "best_l after optimization: 4798.0556640625 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 81.95298767089844]\n",
            "alpha/beta optimization time: 0.9430389404296875\n",
            "This batch time : update_bounds func: 0.9687\t prepare: 0.0140\t bound: 0.9433\t transfer: 0.0002\t finalize: 0.0107\n",
            "Accumulated time: update_bounds func: 20.0339\t prepare: 0.2572\t bound: 19.4672\t transfer: 0.0002\t finalize: 0.2968\n",
            "batch bounding time:  0.9690291881561279\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-38.63850, 42.034431] (11), [-38.63605, 42.034431] (12), [-38.63058, 42.034431] (13), [-38.63015, 42.034431] (13), [-38.62944, 42.034431] (11), [-38.62753, 42.034431] (12), [-38.62719, 42.034431] (13), [-38.62535, 42.034431] (11), [-38.62534, 42.034431] (12), [-38.62523, 42.034431] (9), [-38.62517, 42.034431] (10), [-38.62358, 42.034431] (12), [-38.62238, 42.034431] (9), [-38.62236, 42.034431] (13), [-38.62176, 42.034431] (11), [-38.61880, 42.034431] (10), [-38.61721, 42.034431] (11), [-38.61670, 42.034431] (10), [-38.61377, 42.034431] (12), [-38.61065, 42.034431] (11), \n",
            "length of domains: 1152\n",
            "Total time: 1.0311\t pickout: 0.0051\t decision: 0.0485\t get_bound: 0.9693\t add_domain: 0.0082\n",
            "Current lb:-38.63849639892578\n",
            "2302 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 21.378618717193604\n",
            "\n",
            "splitting decisions: [[4, 18], [4, 9], [4, 90], [4, 79], [4, 18], [4, 9], [4, 79], [4, 79], [4, 9], [4, 69]]\n",
            "best_l after optimization: 4790.4091796875 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 83.11567687988281]\n",
            "alpha/beta optimization time: 0.9485054016113281\n",
            "This batch time : update_bounds func: 0.9747\t prepare: 0.0139\t bound: 0.9489\t transfer: 0.0002\t finalize: 0.0104\n",
            "Accumulated time: update_bounds func: 21.0086\t prepare: 0.2710\t bound: 20.4161\t transfer: 0.0002\t finalize: 0.3073\n",
            "batch bounding time:  0.9751970767974854\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-38.54921, 42.034431] (13), [-38.54855, 42.034431] (12), [-38.54816, 42.034431] (10), [-38.54748, 42.034431] (12), [-38.54516, 42.034431] (13), [-38.54482, 42.034431] (13), [-38.54324, 42.034431] (12), [-38.54211, 42.034431] (12), [-38.54203, 42.034431] (10), [-38.54177, 42.034431] (12), [-38.54108, 42.034431] (10), [-38.53635, 42.034431] (11), [-38.53531, 42.034431] (11), [-38.53489, 42.034431] (13), [-38.53391, 42.034431] (9), [-38.53311, 42.034431] (13), [-38.53302, 42.034431] (12), [-38.53230, 42.034431] (11), [-38.53018, 42.034431] (11), [-38.52956, 42.034431] (10), \n",
            "length of domains: 1216\n",
            "Total time: 1.0345\t pickout: 0.0041\t decision: 0.0454\t get_bound: 0.9755\t add_domain: 0.0094\n",
            "Current lb:-38.54921340942383\n",
            "2430 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 22.415029525756836\n",
            "\n",
            "splitting decisions: [[4, 90], [4, 9], [4, 27], [4, 9], [4, 79], [4, 9], [4, 9], [4, 9], [4, 69], [4, 9]]\n",
            "best_l after optimization: 4781.40625 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 75.23017120361328]\n",
            "alpha/beta optimization time: 0.9554970264434814\n",
            "This batch time : update_bounds func: 0.9823\t prepare: 0.0148\t bound: 0.9559\t transfer: 0.0002\t finalize: 0.0110\n",
            "Accumulated time: update_bounds func: 21.9909\t prepare: 0.2858\t bound: 21.3720\t transfer: 0.0002\t finalize: 0.3183\n",
            "batch bounding time:  0.982658863067627\n",
            "Current worst splitting domains [lb, ub] (depth):\n",
            "[-38.47185, 42.034431] (12), [-38.47072, 42.034431] (10), [-38.46995, 42.034431] (12), [-38.46933, 42.034431] (10), [-38.46750, 42.034431] (12), [-38.46712, 42.034431] (12), [-38.46668, 42.034431] (10), [-38.46585, 42.034431] (10), [-38.46430, 42.034431] (10), [-38.46154, 42.034431] (12), [-38.45934, 42.034431] (11), [-38.45868, 42.034431] (10), [-38.45846, 42.034431] (12), [-38.45731, 42.034431] (11), [-38.45633, 42.034431] (12), [-38.45608, 42.034431] (9), [-38.45548, 42.034431] (12), [-38.45545, 42.034431] (12), [-38.45223, 42.034431] (10), [-38.45110, 42.034431] (10), \n",
            "length of domains: 1280\n",
            "Total time: 1.0449\t pickout: 0.0041\t decision: 0.0488\t get_bound: 0.9829\t add_domain: 0.0091\n",
            "Current lb:-38.47185134887695\n",
            "2558 neurons visited\n",
            "Global ub: 42.03443145751953, batch ub: inf\n",
            "Cumulative time: 23.46230721473694\n",
            "\n",
            "splitting decisions: [[4, 9], [4, 18], [4, 9], [4, 69], [4, 9], [4, 79], [4, 97], [4, 27], [4, 18], [4, 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try configuration options"
      ],
      "metadata": {
        "id": "-3EjCYapS5Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "α,β-CROWN provides a number of different parameters that can be used for verification. An overview is given here: \n",
        "\n",
        "\n",
        "```\n",
        "general:\n",
        "  device: cuda  # Select device to run verifier, cpu or cuda (GPU).\n",
        "  seed: 100  # Random seed.\n",
        "  conv_mode: patches  # Convolution mode during bound propagation: \"patches\" mode (default) is very efficient, but may not support all architecture; \"matrix\" mode is slow but supports all architectures.\n",
        "  deterministic: false  # Run code in CUDA deterministic mode, which has slower performance but better reproducibility.\n",
        "  double_fp: false  # Use double precision floating point. GPUs with good double precision support are preferable (NVIDIA P100, V100, A100; AMD Radeon Instinc MI50, MI100).\n",
        "  loss_reduction_func: sum  # When batch size is not 1, this reduction function is applied to reduce the bounds into a single number (options are \"sum\" and \"min\").\n",
        "  mode: verified-acc  # Verify against all labels (\"verified-acc\" mode), or just the runnerup labels (\"runnerup\" mode), or using a specified label in dataset (\"speicify-target\" mode, only used for oval20). Mode can also be set as \"crown-only-verified-acc\" or \"alpha-crown-only-verified-acc\", which quickly computes the verified accuracy over the entire dataset via CROWN or alpha-CROWN.\n",
        "  complete_verifier: bab  # Complete verification verifier. \"bab\": branch and bound with beta-CROWN; \"mip\": mixed integer programming (MIP) formulation; \"bab-refine\": branch and bound with intermediate layer bounds computed by MIP.\n",
        "  enable_incomplete_verification: true  # Enable/Disable initial alpha-CROWN incomplete verification (this can save GPU memory when disabled).\n",
        "model:\n",
        "  path: null  # Load pretrained model from this specified path.\n",
        "  name: please_specify_model_name  # Name of model. Model must be defined in the load_verification_dataset() function in utils.py.\n",
        "data:\n",
        "  start: 0  # Start from the i-th property in specified dataset.\n",
        "  end: 10000  # End with the (i-1)-th property in the dataset.\n",
        "  num_outputs: 10  # Number of classes for classification problem.\n",
        "  mean: 0.0  # Mean vector used in data preprocessing.\n",
        "  std: 1.0  # Std vector used in data preprocessing.\n",
        "  pkl_path: null  # Load properties to verify from a .pkl file (only used for oval20 dataset).\n",
        "  dataset: CIFAR  # Dataset name. Dataset must be defined in utils.py.\n",
        "  data_idx_file: null  # A text file with a list of example IDs to run.\n",
        "specification:\n",
        "  type: lp  # Type of verification specification. \"lp\" = L_p norm, \"bounds\" = element-wise lower and upper bound provided by dataloader.\n",
        "  norm: .inf  # Lp-norm for epsilon perturbation in robustness verification (1, 2, inf).\n",
        "  epsilon: null  # Set perturbation size (Lp norm). If not set, a default value may be used based on dataset loader.\n",
        "solver:\n",
        "  alpha-crown:\n",
        "    lr_alpha: 0.1  # Learning rate for the optimizable parameter alpha in alpha-CROWN bound.\n",
        "    iteration: 100  # Number of iterations for alpha-CROWN incomplete verifier.\n",
        "    share_slopes: false  # Share some alpha variables to save memory at the cost of slightly looser bounds.\n",
        "    no_joint_opt: false  # Run alpha-CROWN bounds without joint optimization (only optimize alpha for the last layer bound).\n",
        "  beta-crown:\n",
        "    batch_size: 64  # Batch size in beta-CROWN (number of parallel splits).\n",
        "    lr_alpha: 0.01  # Learning rate for optimizing alpha during branch and bound.\n",
        "    lr_beta: 0.05  # Learning rate for optimizing beta during branch and bound.\n",
        "    lr_decay: 0.98  # Learning rate decay factor during optimization. Need to use a larger value like 0.99 or 0.995 when you increase the number of iterations.\n",
        "    optimizer: adam  # Optimizer used for alpha and beta optimization.\n",
        "    iteration: 50  # Number of iteration for optimizing alpha and beta during branch and bound.\n",
        "  mip:\n",
        "    parallel_solvers: null  # Number of multi-processes for mip solver. Each process computes a mip bound for an intermediate neuron. Default (None) is to auto detect the number of CPU cores (note that each process may use multiple threads, see the next option).\n",
        "    solver_threads: 1  # Number of threads for echo mip solver process (default is to use 1 thread for each solver process).\n",
        "    refine_neuron_timeout: 15  # MIP timeout threshold for improving each intermediate layer bound (in seconds).\n",
        "    refine_neuron_time_percentage: 0.8  # Percentage (x100%) of time used for improving all intermediate layer bounds using mip. Default to be 0.8*timeout.\n",
        "bab:\n",
        "  max_domains: 200000  # Max number of subproblems in branch and bound.\n",
        "  decision_thresh: 0  # Decision threshold of lower bounds. When lower bounds are greater than this value, verification is successful. Set to 0 for robustness verification.\n",
        "  timeout: 360  # Timeout (in second) for verifying one image/property.\n",
        "  branching:\n",
        "    method: kfsb  # Branching heuristic. babsr is fast but less accurate; fsb is slow but most accurate; kfsb is usualy a balance.\n",
        "    candidates: 3  # Number of candidates to consider when using fsb or kfsb. More leads to slower but better branching.\n",
        "    reduceop: min  # Reduction operation to compute branching scores from two sides of a branch (min or max). max can work better on some models.\n",
        "attack:\n",
        "  pgd_order: before  # Run PGD before/after incomplete verification, or skip it.\n",
        "  enable_mip_attack: false  # Use MIP (Gurobi) based attack if PGD cannot find a successful adversarial example.\n",
        "  pgd_steps: 100  # Steps of PGD attack.\n",
        "  pgd_restarts: 30  # Number of random PGD restarts.\n",
        "  pgd_early_stop: true  # Early stop PGD when an adversarial example is found.\n",
        "  pgd_lr_decay: 0.99  # Learning rate decay factor used in PGD attack.\n",
        "  pgd_alpha: auto  # Step size of PGD attack. Default (auto) is epsilon/4.\n",
        "  ```\n",
        "\n"
      ],
      "metadata": {
        "id": "qnX_9C1RS9wD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Try out another epsilon\n",
        "Now it's up to you! Try different configuration options and see how it influences the result! \\\\\n",
        "For example: What do you think changes with a changed value for Epsilon? \n",
        "(Expand the cell in order to see the resulting configuration file)"
      ],
      "metadata": {
        "id": "KV5doog1Tpr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exp_configs/my_example_config.yaml\n",
        "general:\n",
        "  # device to run verifier\n",
        "  device: cpu \n",
        "  # Complete verification verifier. \n",
        "  # \"bab\": branch and bound with beta-CROWN; \n",
        "  # \"mip\": mixed integer programming (MIP) formulation; \n",
        "  # \"bab-refine\": branch and bound with intermediate layer bounds computed by MIP.\n",
        "  complete_verifier: bab\n",
        "model:\n",
        "  # name of the model (provided by library, see above)\n",
        "  name: mnist_6_100\n",
        "  # Load pretrained model from this specified path.\n",
        "  path: models/eran/mnist_6_100_nat.pth\n",
        "data:\n",
        "  # Dataset name. Dataset must be defined in utils.py.\n",
        "  dataset: MNIST_ERAN_UN\n",
        "  # Std vector used in data preprocessing.\n",
        "  std: [1.0]\n",
        "  # Mean vector used in data preprocessing.\n",
        "  mean: [0.0]\n",
        "specification:\n",
        "  # Set perturbation size (Lp norm). \n",
        "  # If not set, a default value may be used based on dataset loader.\n",
        "  epsilon: 1\n",
        "solver:\n",
        "  alpha-crown:\n",
        "    # Number of iterations for alpha-CROWN incomplete verifier.\n",
        "    iteration: 10  \n",
        "attack:\n",
        "  # Early stop PGD when an adversarial example is found.\n",
        "  pgd_early_stop: true   "
      ],
      "metadata": {
        "id": "9WLtxtU5Vjtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run your new configuration file\n",
        "So, what do you expect with your new epsilon? "
      ],
      "metadata": {
        "id": "xCvUzLBQxwAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./run.sh"
      ],
      "metadata": {
        "id": "S0kp0BGiVwHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the larger epsilon makes the attack much more likely to succeed. This is because a larger epsilon allows a greater change in the original image."
      ],
      "metadata": {
        "id": "tzSYWTR4x5yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Different norms\n",
        "α,β-CROWN offers the possibility to use different norms for the verification of the networks. \n",
        "###L1 Norm\n",
        "The L1 norm is also known as Manhattan Distance or Taxicab norm. The L1 norm for a vector $x$ is calculated by $||x||_1$=  $\\sum\\nolimits_{i=1}^n |x_i|$. In the case of the plot below it is $||a||_1$=  $|a_1| + |a_2|$ = 3 + 4 = 7. \n",
        "\n"
      ],
      "metadata": {
        "id": "phDUvqNO-Ebl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([3], [4], 'ro')\n",
        "plt.axis([0, 4, 0, 5])\n",
        "plt.plot([3, 0], [4, 4],color=\"red\")\n",
        "plt.plot([3, 3], [0, 4],color=\"red\")\n",
        "plt.annotate('$a$ with $a_1=3$ and $a_2=4$', xy=(3, 4), xytext=(2, 3),\n",
        "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "             )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SNLodaEtGLMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###L2 Norm\n",
        "The L2 norm is also known as the Euclidean norm. It is defined as $||x||_2 = \\sqrt{\\sum\\nolimits_{i=1}^n |x_i|^2}$."
      ],
      "metadata": {
        "id": "cF4O13mvG_Rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([3], [4], 'ro')\n",
        "plt.axis([0, 4, 0, 5])\n",
        "plt.plot([0, 3], [0, 4],color=\"red\")\n",
        "plt.annotate('$a$ with $a_1=3$ and $a_2=4$', xy=(3, 4), xytext=(2, 3),\n",
        "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "             )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b9oA2plLJnO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###L3 Norm\n",
        "The L3 norm oder maximum norm corresponds to the magnitude of the largest component of the vector.\n",
        "It is defined as $||x||_{\\inf} = max_{i=1,..,n}|x_i|$."
      ],
      "metadata": {
        "id": "DQXXfLUiJvXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Different norms in verification\n",
        "\n",
        "As adversarial robustness around a fixed input $\\overrightarrow{y*}$ is defined such that the distance between  $\\overrightarrow{y*}$ and any $\\overrightarrow{x}$ less than or equal to an epsilon always produces the same prediction. The distance here is calculated using one of the vector norms. "
      ],
      "metadata": {
        "id": "IT3-xYpcMv05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Now it's your turn\n",
        "Play around with different norms in your configuration file and run the verification."
      ],
      "metadata": {
        "id": "5uYVKGn4OSB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exp_configs/my_example_config.yaml\n",
        "general:\n",
        "  device: cpu \n",
        "  complete_verifier: bab\n",
        "model:\n",
        "  name: mnist_6_100\n",
        "  path: models/eran/mnist_6_100_nat.pth\n",
        "data:\n",
        "  dataset: MNIST_ERAN_UN\n",
        "  std: [1.0]\n",
        "  mean: [0.0]\n",
        "specification:\n",
        "  ###################\n",
        "  # inserted here:\n",
        "  norm: .inf  \n",
        "  # norm: 1\n",
        "  # norm: 2\n",
        "  ###################\n",
        "  epsilon: 0.026\n",
        "solver:\n",
        "  alpha-crown:\n",
        "    iteration: 10\n",
        "attack:\n",
        "  pgd_early_stop: true  "
      ],
      "metadata": {
        "id": "HJZQ6nrjO3HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customization on custom model and data\n",
        "Alpha beta crown can be easily be run on customized models and data. \n",
        "We will use it on a custom model and the cifar10 dataset. \n",
        "it can as well support other arbitrary datasets. \n"
      ],
      "metadata": {
        "id": "yN28TqGo66qX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile custom_model_data.py\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import arguments\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def simple_conv_model(in_channel, out_dim):\n",
        "    \"\"\"Simple Convolutional Neural Network model.\"\"\"\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_channel, 16, 4, stride=2, padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 32, 4, stride=2, padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(32*6*6,100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100, out_dim)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def cifar10_dataloader(eps, use_bounds=False):\n",
        "    \"\"\"Example dataloader. For MNIST and CIFAR you can actually use existing ones in utils.py.\"\"\"\n",
        "    assert eps is not None\n",
        "    database_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'datasets')\n",
        "    # You can access the mean and std stored in config file.\n",
        "    mean = torch.tensor(arguments.Config[\"data\"][\"mean\"])\n",
        "    std = torch.tensor(arguments.Config[\"data\"][\"std\"])\n",
        "    normalize = transforms.Normalize(mean=mean, std=std)\n",
        "    test_data = datasets.CIFAR10(database_path, train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), normalize]))\n",
        "    # Load entire dataset.\n",
        "    testloader = torch.utils.data.DataLoader(test_data, batch_size=10000, shuffle=False, num_workers=4)\n",
        "    X, labels = next(iter(testloader))\n",
        "    if use_bounds:\n",
        "        # Option 1: for each example, we return its element-wise lower and upper bounds.\n",
        "        # If you use this option, set --spec_type (\"specifications\"->\"type\" in config) to 'bound'.\n",
        "        absolute_max = torch.reshape((1. - mean) / std, (1, -1, 1, 1))\n",
        "        absolute_min = torch.reshape((0. - mean) / std, (1, -1, 1, 1))\n",
        "        # Be careful with normalization.\n",
        "        new_eps = torch.reshape(eps / std, (1, -1, 1, 1))\n",
        "        data_max = torch.min(X + new_eps, absolute_max)\n",
        "        data_min = torch.max(X - new_eps, absolute_min)\n",
        "        # In this case, the epsilon does not matter here.\n",
        "        ret_eps = None\n",
        "    else:\n",
        "        # Option 2: return a single epsilon for all data examples, as well as clipping lower and upper bounds.\n",
        "        # Set data_max and data_min to be None if no clip. For CIFAR-10 we clip to [0,1].\n",
        "        data_max = torch.reshape((1. - mean) / std, (1, -1, 1, 1))\n",
        "        data_min = torch.reshape((0. - mean) / std, (1, -1, 1, 1))\n",
        "        if eps is None:\n",
        "            raise ValueError('You must specify an epsilon')\n",
        "        # Rescale epsilon.\n",
        "        ret_eps = torch.reshape(eps / std, (1, -1, 1, 1))\n",
        "    return X, labels, data_max, data_min, ret_eps"
      ],
      "metadata": {
        "id": "VoHaCyxm9XL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using the configuration for the custom model and data\n"
      ],
      "metadata": {
        "id": "BqJ1lKnT-aJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exp_configs/tutorial_cifar_example.yaml\n",
        "general:\n",
        "  mode: verified-acc\n",
        "model:\n",
        "  # Use the simple_conv_model() model in \"your_model_data.py\".\n",
        "  name: Customized(\"custom_model_data\", \"simple_conv_model\", in_channel=3, out_dim=10)\n",
        "  path: models/eran/cifar_conv_small_pgd.pth\n",
        "data:\n",
        "  # Use the cifar10_dataloader()  in \"custom_model_data.py\".\n",
        "  dataset: Customized(\"custom_model_data\", \"cifar10_dataloader\")\n",
        "  mean: [0.4914, 0.4822, 0.4465]\n",
        "  std: [0.2023, 0.1994, 0.201]\n",
        "specification:\n",
        "  epsilon: 0.00784313725  # 2./255.\n",
        "attack:\n",
        "  pgd_restarts: 100\n",
        "solver:\n",
        "  beta-crown:\n",
        "    batch_size: 2048\n",
        "    iteration: 20\n",
        "bab:\n",
        "  max_domains: 5000000\n",
        "  timeout: 300"
      ],
      "metadata": {
        "id": "YX8HNd2k-Ybv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# checking robustness on custom data and model\n"
      ],
      "metadata": {
        "id": "1cIoiuiyBa5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "source activate alpha-beta-crown\n",
        "python robustness_verifier.py --config exp_configs/tutorial_cifar_example.yaml --start 3 --end 4\n",
        "conda deactivate"
      ],
      "metadata": {
        "id": "_ft54GlDBjDb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}